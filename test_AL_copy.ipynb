{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zhangxl2002/ORL/blob/main/T5_Ner_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:46:03.615350Z",
     "iopub.status.busy": "2024-02-28T11:46:03.615160Z",
     "iopub.status.idle": "2024-02-28T11:46:03.618572Z",
     "shell.execute_reply": "2024-02-28T11:46:03.617968Z",
     "shell.execute_reply.started": "2024-02-28T11:46:03.615332Z"
    },
    "id": "4rfSWZLR6E7k",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr  9 20:34:56 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          Off  | 00000000:00:08.0 Off |                    0 |\n",
      "|  0%   27C    P8    16W / 150W |      2MiB / 22731MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BlIYFZKgmOU"
   },
   "source": [
    "# Named Entity Recognition with T5\n",
    "\n",
    "This notebook shows how to finetune [T5 Model](https://https://huggingface.co/docs/transformers/model_doc/t5) for token classification or named entity recognition with pytorch lighning. In this demo, I used the T5-Small and cast the entities as a text using the text to text framework used in the t5 paper. During Eval the generated tokens are then split and classifies into their specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:23.741652Z",
     "iopub.status.busy": "2024-02-28T14:14:23.741327Z",
     "iopub.status.idle": "2024-02-28T14:14:27.411717Z",
     "shell.execute_reply": "2024-02-28T14:14:27.411232Z",
     "shell.execute_reply.started": "2024-02-28T14:14:23.741632Z"
    },
    "id": "cBQiMj-p5lfz",
    "outputId": "7802de39-2c8b-4e20-c061-164dcbb6af9e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    MT5ForConditionalGeneration,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:29.473867Z",
     "iopub.status.busy": "2024-02-28T14:14:29.473454Z",
     "iopub.status.idle": "2024-02-28T14:14:29.527177Z",
     "shell.execute_reply": "2024-02-28T14:14:29.526681Z",
     "shell.execute_reply.started": "2024-02-28T14:14:29.473846Z"
    },
    "id": "0WqcwP916Dwq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:36.446536Z",
     "iopub.status.busy": "2024-02-28T14:14:36.446076Z",
     "iopub.status.idle": "2024-02-28T14:15:04.104154Z",
     "shell.execute_reply": "2024-02-28T14:15:04.103481Z",
     "shell.execute_reply.started": "2024-02-28T14:14:36.446513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"zhangxl2002/mpqa_ORL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:15:30.477538Z",
     "iopub.status.busy": "2024-02-28T14:15:30.476989Z",
     "iopub.status.idle": "2024-02-28T14:15:30.751473Z",
     "shell.execute_reply": "2024-02-28T14:15:30.750974Z",
     "shell.execute_reply.started": "2024-02-28T14:15:30.477514Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['words', 'label_ids', 'labels', 'spans', 'dse'],\n",
      "        num_rows: 3549\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['words', 'label_ids', 'labels', 'spans', 'dse'],\n",
      "        num_rows: 893\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['words', 'label_ids', 'labels', 'spans', 'dse'],\n",
      "        num_rows: 1509\n",
      "    })\n",
      "})\n",
      "{'words': ['He', 'argued', 'that', 'had', 'the', 'West', 'not', 'continued', 'to', 'keep', 'alive', 'during', 'the', 'past', 'several', 'years', 'the', 'Cold', 'War', 'stereotype', 'of', 'a', 'threat', 'from', 'the', 'East', ',', 'but', 'would', 'have', 'concentrated', 'instead', 'on', 'terrorism', ',', 'the', 'common', 'enemy', ',', 'the', 'twin', 'towers', 'of', 'New', 'York', 'may', 'not', 'have', 'collapsed', '.'], 'label_ids': [1, 3, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 0], 'labels': ['B-AGENT', 'B-DSE', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O'], 'spans': ['AGENT: He', 'TARGET: the West', 'TARGET: terrorism', 'TARGET: the twin towers of New York may not have collapsed'], 'dse': 'argued'}\n",
      "{'words': ['They', 'think', 'she', 'is', 'traveling', 'too', 'much', ',', 'while', 'internal', 'problems', 'sap', 'the', 'country', \"'s\", 'energy', ',', 'and', 'that', 'she', 'has', 'misplaced', 'strategic', 'priorities', 'in', 'her', 'overseas', 'visits', '.'], 'label_ids': [1, 3, 5, 0, 0, 0, 0, 0, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': ['B-AGENT', 'B-DSE', 'B-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': ['AGENT: They', 'TARGET: she', 'TARGET: internal problems', 'TARGET: she'], 'dse': 'think'}\n",
      "{'words': ['He', 'claims', 'that', 'he', 'belongs', 'to', 'the', 'good', 'side', 'and', 'whoever', 'is', 'on', 'the', 'other', 'side', 'is', 'the', 'evil', 'that', 'must', 'be', 'destroyed', 'or', 'at', 'least', 'deserves', 'condemnation', '.'], 'label_ids': [1, 3, 0, 5, 0, 0, 0, 0, 0, 0, 5, 6, 6, 6, 6, 6, 0, 5, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': ['B-AGENT', 'B-DSE', 'O', 'B-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': ['AGENT: He', 'TARGET: he', 'TARGET: whoever is on the other side', 'TARGET: the evil'], 'dse': 'claims'}\n",
      "{'words': ['``', 'The', 'situation', 'we', 'see', 'now', 'is', 'what', 'I', 'call', 'the', 'Last', 'Supper', '--', 'it', 'is', 'ZANU-PF', \"'s\", 'final', 'feast', ',', \"''\", 'Mr.', 'Tsvangirai', 'said', '.'], 'label_ids': [0, 5, 6, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 5, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': ['O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'B-DSE', 'O', 'O', 'O', 'O', 'B-TARGET', 'O', 'B-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': ['TARGET: The situation', 'AGENT: I', 'TARGET: it', 'TARGET: ZANU-PF'], 'dse': 'call'}\n",
      "{'words': ['Protesters', 'carried', 'banners', 'reading', ',', '``', 'Ratify', 'the', 'Kyoto', 'Protocol', ',', \"''\", 'and', '``', 'Koizumi', 'say', '`', 'No', \"'\", 'to', 'Bush', ':', 'Stick', 'to', 'Kyoto', 'Protocol', '.', \"''\"], 'label_ids': [1, 3, 4, 4, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 5, 6, 0, 0], 'labels': ['B-AGENT', 'B-DSE', 'I-DSE', 'I-DSE', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O'], 'spans': ['AGENT: Protesters', 'TARGET: the Kyoto Protocol', 'TARGET: Bush', 'TARGET: Kyoto Protocol'], 'dse': 'carried banners reading'}\n",
      "{'words': ['Carl', 'Pope', ',', 'the', 'director', 'of', 'the', 'Sierra', 'Club', ',', 'considers', 'for', 'his', 'part', 'that', 'the', 'Bush', 'Administration', '``', 'is', 'sticking', 'to', 'the', 'polluting', 'policies', 'that', 'the', 'energy', 'industry', 'asked', 'for', 'rather', 'than', 'taking', 'the', 'sensible', 'steps', 'that', 'can', 'protect', 'our', 'health', '.', \"''\"], 'label_ids': [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6, 0, 0, 0, 0, 0, 0, 0], 'labels': ['B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-DSE', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'spans': ['AGENT: Carl Pope', 'TARGET: the Bush Administration', 'TARGET: the polluting policies', 'TARGET: the sensible steps'], 'dse': 'considers'}\n",
      "The Kimberley Provincial Hospital said it would probably know by Tuesday whether one of its patients had Congo Fever .\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "cnt = 0\n",
    "for i in range(len(dataset['train'])):\n",
    "    if len(dataset['train'][i]['spans']) > 3:\n",
    "        print(dataset['train'][i])\n",
    "        cnt+=1\n",
    "        if cnt>5: break\n",
    "\n",
    "print(\" \".join(dataset['train'][0]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:03.808350Z",
     "iopub.status.busy": "2024-02-28T14:16:03.808004Z",
     "iopub.status.idle": "2024-02-28T14:16:03.814544Z",
     "shell.execute_reply": "2024-02-28T14:16:03.814024Z",
     "shell.execute_reply.started": "2024-02-28T14:16:03.808329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MPQADataset(Dataset):\n",
    "  def __init__(self, tokenizer, dataset, type_path, max_len=512):\n",
    "\n",
    "    self.data = dataset[type_path]\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.tokenizer.max_length = max_len\n",
    "    self.tokenizer.model_max_length = max_len\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "  def _build(self):\n",
    "    for idx in range(len(self.data)):\n",
    "      input_, target = \" \".join(self.data[idx][\"words\"]), \"; \".join(self.data[idx][\"spans\"])\n",
    "      input_ = input_ + \" DSE:\" + self.data[idx][\"dse\"]\n",
    "\n",
    "      input_ = input_.lower() + ' </s>'\n",
    "      target = target.lower() + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target],max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:07.466521Z",
     "iopub.status.busy": "2024-02-28T14:16:07.466195Z",
     "iopub.status.idle": "2024-02-28T14:16:10.213091Z",
     "shell.execute_reply": "2024-02-28T14:16:10.212619Z",
     "shell.execute_reply.started": "2024-02-28T14:16:07.466502Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5TokenizerFast(name_or_path='../T5-base', vocab_size=32100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../T5-base\")\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "input_dataset = MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:13.859761Z",
     "iopub.status.busy": "2024-02-28T14:16:13.859213Z",
     "iopub.status.idle": "2024-02-28T14:16:13.865043Z",
     "shell.execute_reply": "2024-02-28T14:16:13.864610Z",
     "shell.execute_reply.started": "2024-02-28T14:16:13.859741Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "the kimberley\n",
      "[2, 1]\n",
      "tensor([8, 3])\n",
      "0\n",
      "the kimberley provincial hospital said it would probably know by tuesday whether one of its patients had congo fever. dse:would probably know</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "agent: the kimberley provincial hospital; target: whether one of its patients had congo fever</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data = input_dataset[0]\n",
    "# print(data)\n",
    "print(tokenizer.decode(data[\"source_ids\"][400]))\n",
    "print((tokenizer.decode(data[\"source_ids\"][0:5])))\n",
    "print(tokenizer.encode(\"<unk>\"))\n",
    "print(data[\"source_ids\"][0:2])\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.decode(data[\"source_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(data[\"target_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:33.181671Z",
     "iopub.status.busy": "2024-02-28T14:16:33.181353Z",
     "iopub.status.idle": "2024-02-28T14:16:33.185008Z",
     "shell.execute_reply": "2024-02-28T14:16:33.184575Z",
     "shell.execute_reply.started": "2024-02-28T14:16:33.181650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"zhangxl2002/mpqa_ORL\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:34.903982Z",
     "iopub.status.busy": "2024-02-28T14:16:34.903434Z",
     "iopub.status.idle": "2024-02-28T14:16:34.906468Z",
     "shell.execute_reply": "2024-02-28T14:16:34.905943Z",
     "shell.execute_reply.started": "2024-02-28T14:16:34.903962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner():\n",
    "    def __init__(self, hparam):\n",
    "        self.hparam = hparam\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     hparam.model_name_or_path)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"saved_models/model_epoch_3\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            hparam.model_name_or_path\n",
    "        )\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        # tensorboard_logs = {\"train_loss\": loss}\n",
    "        # return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return loss\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparam.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparam.learning_rate, eps=self.hparam.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self,\n",
    "                       epoch=None,\n",
    "                       batch_idx=None,\n",
    "                       optimizer=None,\n",
    "                       optimizer_idx=None,\n",
    "                       optimizer_closure=None,\n",
    "                       on_tpu=None,\n",
    "                       using_native_amp=None,\n",
    "                       using_lbfgs=None\n",
    "                       ):\n",
    "\n",
    "        # optimizer.step(closure=optimizer_closure)\n",
    "        # optimizer.zero_grad()\n",
    "        self.opt.step(closure=optimizer_closure)\n",
    "        self.opt.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(\n",
    "            self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparam.train_batch_size,\n",
    "                                drop_last=True, shuffle=True, num_workers=2)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) //\n",
    "             (self.hparam.train_batch_size * max(1, self.hparam.n_gpu)))\n",
    "            // self.hparam.gradient_accumulation_steps\n",
    "            * float(self.hparam.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparam.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:39.522728Z",
     "iopub.status.busy": "2024-02-28T14:16:39.522408Z",
     "iopub.status.idle": "2024-02-28T14:16:46.303899Z",
     "shell.execute_reply": "2024-02-28T14:16:46.303415Z",
     "shell.execute_reply.started": "2024-02-28T14:16:39.522708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Config\n",
    "# config = T5Config.from_pretrained(\"t5-base\")\n",
    "# # 创建一个与原始模型相同架构的模型实例\n",
    "# model = T5ForConditionalGeneration(config=config)\n",
    "\n",
    "# # 加载之前保存的PyTorch模型的参数\n",
    "# model.load_state_dict(torch.load(\"saved_models/model_epoch_3.pth\"))\n",
    "\n",
    "# # 将模型保存为Hugging Face的格式\n",
    "# model.save_pretrained(\"./test_saved_models\")\n",
    "\n",
    "# # 模型现在已经转换为Hugging Face的形式并保存到指定路径下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = \"saveCheckpointPath/checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    tokenizer.max_length = args.max_seq_length\n",
    "    tokenizer.model_max_length = args.max_seq_length\n",
    "    # dataset = load_dataset(args.data_dir)\n",
    "    dataset = load_dataset(\"zhangxl2002/mpqa_ORL\")\n",
    "    return MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path=type_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Average Loss: 0.1086\n",
      "Model saved at ./saved_models/model_epoch_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2199\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [1/443], Loss: 0.0727\n",
      "Epoch [1/10], Batch [2/443], Loss: 0.0844\n",
      "Epoch [1/10], Batch [3/443], Loss: 0.2332\n",
      "Epoch [1/10], Batch [4/443], Loss: 0.1178\n",
      "Epoch [1/10], Batch [5/443], Loss: 0.1194\n",
      "Epoch [1/10], Batch [6/443], Loss: 0.0579\n",
      "Epoch [1/10], Batch [7/443], Loss: 0.1523\n",
      "Epoch [1/10], Batch [8/443], Loss: 0.1300\n",
      "Epoch [1/10], Batch [9/443], Loss: 0.1476\n",
      "Epoch [1/10], Batch [10/443], Loss: 0.1538\n",
      "Epoch [1/10], Batch [11/443], Loss: 0.0868\n",
      "Epoch [1/10], Batch [12/443], Loss: 0.1182\n",
      "Epoch [1/10], Batch [13/443], Loss: 0.3243\n",
      "Epoch [1/10], Batch [14/443], Loss: 0.1552\n",
      "Epoch [1/10], Batch [15/443], Loss: 0.1220\n",
      "Epoch [1/10], Batch [16/443], Loss: 0.1050\n",
      "Epoch [1/10], Batch [17/443], Loss: 0.0642\n",
      "Epoch [1/10], Batch [18/443], Loss: 0.1202\n",
      "Epoch [1/10], Batch [19/443], Loss: 0.2135\n",
      "Epoch [1/10], Batch [20/443], Loss: 0.2059\n",
      "Epoch [1/10], Batch [21/443], Loss: 0.0734\n",
      "Epoch [1/10], Batch [22/443], Loss: 0.1277\n",
      "Epoch [1/10], Batch [23/443], Loss: 0.1724\n",
      "Epoch [1/10], Batch [24/443], Loss: 0.2066\n",
      "Epoch [1/10], Batch [25/443], Loss: 0.1193\n",
      "Epoch [1/10], Batch [26/443], Loss: 0.1496\n",
      "Epoch [1/10], Batch [27/443], Loss: 0.1015\n",
      "Epoch [1/10], Batch [28/443], Loss: 0.1213\n",
      "Epoch [1/10], Batch [29/443], Loss: 0.0732\n",
      "Epoch [1/10], Batch [30/443], Loss: 0.2065\n",
      "Epoch [1/10], Batch [31/443], Loss: 0.1803\n",
      "Epoch [1/10], Batch [32/443], Loss: 0.0749\n",
      "Epoch [1/10], Batch [33/443], Loss: 0.0925\n",
      "Epoch [1/10], Batch [34/443], Loss: 0.1874\n",
      "Epoch [1/10], Batch [35/443], Loss: 0.0894\n",
      "Epoch [1/10], Batch [36/443], Loss: 0.1313\n",
      "Epoch [1/10], Batch [37/443], Loss: 0.0436\n",
      "Epoch [1/10], Batch [38/443], Loss: 0.1322\n",
      "Epoch [1/10], Batch [39/443], Loss: 0.1475\n",
      "Epoch [1/10], Batch [40/443], Loss: 0.1256\n",
      "Epoch [1/10], Batch [41/443], Loss: 0.2497\n",
      "Epoch [1/10], Batch [42/443], Loss: 0.1718\n",
      "Epoch [1/10], Batch [43/443], Loss: 0.1614\n",
      "Epoch [1/10], Batch [44/443], Loss: 0.1482\n",
      "Epoch [1/10], Batch [45/443], Loss: 0.2071\n",
      "Epoch [1/10], Batch [46/443], Loss: 0.2968\n",
      "Epoch [1/10], Batch [47/443], Loss: 0.1002\n",
      "Epoch [1/10], Batch [48/443], Loss: 0.1313\n",
      "Epoch [1/10], Batch [49/443], Loss: 0.1787\n",
      "Epoch [1/10], Batch [50/443], Loss: 0.1284\n",
      "Epoch [1/10], Batch [51/443], Loss: 0.0583\n",
      "Epoch [1/10], Batch [52/443], Loss: 0.2213\n",
      "Epoch [1/10], Batch [53/443], Loss: 0.3284\n",
      "Epoch [1/10], Batch [54/443], Loss: 0.2301\n",
      "Epoch [1/10], Batch [55/443], Loss: 0.1263\n",
      "Epoch [1/10], Batch [56/443], Loss: 0.1101\n",
      "Epoch [1/10], Batch [57/443], Loss: 0.1618\n",
      "Epoch [1/10], Batch [58/443], Loss: 0.1829\n",
      "Epoch [1/10], Batch [59/443], Loss: 0.1232\n",
      "Epoch [1/10], Batch [60/443], Loss: 0.0972\n",
      "Epoch [1/10], Batch [61/443], Loss: 0.1942\n",
      "Epoch [1/10], Batch [62/443], Loss: 0.3547\n",
      "Epoch [1/10], Batch [63/443], Loss: 0.0534\n",
      "Epoch [1/10], Batch [64/443], Loss: 0.1337\n",
      "Epoch [1/10], Batch [65/443], Loss: 0.1105\n",
      "Epoch [1/10], Batch [66/443], Loss: 0.0619\n",
      "Epoch [1/10], Batch [67/443], Loss: 0.1342\n",
      "Epoch [1/10], Batch [68/443], Loss: 0.1416\n",
      "Epoch [1/10], Batch [69/443], Loss: 0.0927\n",
      "Epoch [1/10], Batch [70/443], Loss: 0.0830\n",
      "Epoch [1/10], Batch [71/443], Loss: 0.1546\n",
      "Epoch [1/10], Batch [72/443], Loss: 0.0774\n",
      "Epoch [1/10], Batch [73/443], Loss: 0.1877\n",
      "Epoch [1/10], Batch [74/443], Loss: 0.2511\n",
      "Epoch [1/10], Batch [75/443], Loss: 0.1370\n",
      "Epoch [1/10], Batch [76/443], Loss: 0.2115\n",
      "Epoch [1/10], Batch [77/443], Loss: 0.1918\n",
      "Epoch [1/10], Batch [78/443], Loss: 0.0902\n",
      "Epoch [1/10], Batch [79/443], Loss: 0.1121\n",
      "Epoch [1/10], Batch [80/443], Loss: 0.1266\n",
      "Epoch [1/10], Batch [81/443], Loss: 0.0957\n",
      "Epoch [1/10], Batch [82/443], Loss: 0.0658\n",
      "Epoch [1/10], Batch [83/443], Loss: 0.1776\n",
      "Epoch [1/10], Batch [84/443], Loss: 0.1593\n",
      "Epoch [1/10], Batch [85/443], Loss: 0.0946\n",
      "Epoch [1/10], Batch [86/443], Loss: 0.1753\n",
      "Epoch [1/10], Batch [87/443], Loss: 0.1482\n",
      "Epoch [1/10], Batch [88/443], Loss: 0.1492\n",
      "Epoch [1/10], Batch [89/443], Loss: 0.0990\n",
      "Epoch [1/10], Batch [90/443], Loss: 0.2041\n",
      "Epoch [1/10], Batch [91/443], Loss: 0.0558\n",
      "Epoch [1/10], Batch [92/443], Loss: 0.1203\n",
      "Epoch [1/10], Batch [93/443], Loss: 0.1427\n",
      "Epoch [1/10], Batch [94/443], Loss: 0.0799\n",
      "Epoch [1/10], Batch [95/443], Loss: 0.1624\n",
      "Epoch [1/10], Batch [96/443], Loss: 0.1285\n",
      "Epoch [1/10], Batch [97/443], Loss: 0.2325\n",
      "Epoch [1/10], Batch [98/443], Loss: 0.3698\n",
      "Epoch [1/10], Batch [99/443], Loss: 0.0521\n",
      "Epoch [1/10], Batch [100/443], Loss: 0.1592\n",
      "Epoch [1/10], Batch [101/443], Loss: 0.1300\n",
      "Epoch [1/10], Batch [102/443], Loss: 0.0604\n",
      "Epoch [1/10], Batch [103/443], Loss: 0.0940\n",
      "Epoch [1/10], Batch [104/443], Loss: 0.0607\n",
      "Epoch [1/10], Batch [105/443], Loss: 0.1126\n",
      "Epoch [1/10], Batch [106/443], Loss: 0.0701\n",
      "Epoch [1/10], Batch [107/443], Loss: 0.0829\n",
      "Epoch [1/10], Batch [108/443], Loss: 0.1691\n",
      "Epoch [1/10], Batch [109/443], Loss: 0.2122\n",
      "Epoch [1/10], Batch [110/443], Loss: 0.1776\n",
      "Epoch [1/10], Batch [111/443], Loss: 0.2239\n",
      "Epoch [1/10], Batch [112/443], Loss: 0.1357\n",
      "Epoch [1/10], Batch [113/443], Loss: 0.2792\n",
      "Epoch [1/10], Batch [114/443], Loss: 0.2349\n",
      "Epoch [1/10], Batch [115/443], Loss: 0.1126\n",
      "Epoch [1/10], Batch [116/443], Loss: 0.0948\n",
      "Epoch [1/10], Batch [117/443], Loss: 0.1904\n",
      "Epoch [1/10], Batch [118/443], Loss: 0.1700\n",
      "Epoch [1/10], Batch [119/443], Loss: 0.2072\n",
      "Epoch [1/10], Batch [120/443], Loss: 0.1395\n",
      "Epoch [1/10], Batch [121/443], Loss: 0.1010\n",
      "Epoch [1/10], Batch [122/443], Loss: 0.1288\n",
      "Epoch [1/10], Batch [123/443], Loss: 0.1805\n",
      "Epoch [1/10], Batch [124/443], Loss: 0.1079\n",
      "Epoch [1/10], Batch [125/443], Loss: 0.1574\n",
      "Epoch [1/10], Batch [126/443], Loss: 0.1496\n",
      "Epoch [1/10], Batch [127/443], Loss: 0.2441\n",
      "Epoch [1/10], Batch [128/443], Loss: 0.1589\n",
      "Epoch [1/10], Batch [129/443], Loss: 0.1203\n",
      "Epoch [1/10], Batch [130/443], Loss: 0.1324\n",
      "Epoch [1/10], Batch [131/443], Loss: 0.2094\n",
      "Epoch [1/10], Batch [132/443], Loss: 0.0804\n",
      "Epoch [1/10], Batch [133/443], Loss: 0.1995\n",
      "Epoch [1/10], Batch [134/443], Loss: 0.2294\n",
      "Epoch [1/10], Batch [135/443], Loss: 0.1759\n",
      "Epoch [1/10], Batch [136/443], Loss: 0.3188\n",
      "Epoch [1/10], Batch [137/443], Loss: 0.0776\n",
      "Epoch [1/10], Batch [138/443], Loss: 0.1638\n",
      "Epoch [1/10], Batch [139/443], Loss: 0.1079\n",
      "Epoch [1/10], Batch [140/443], Loss: 0.1346\n",
      "Epoch [1/10], Batch [141/443], Loss: 0.1895\n",
      "Epoch [1/10], Batch [142/443], Loss: 0.1860\n",
      "Epoch [1/10], Batch [143/443], Loss: 0.0855\n",
      "Epoch [1/10], Batch [144/443], Loss: 0.1406\n",
      "Epoch [1/10], Batch [145/443], Loss: 0.0883\n",
      "Epoch [1/10], Batch [146/443], Loss: 0.1843\n",
      "Epoch [1/10], Batch [147/443], Loss: 0.1099\n",
      "Epoch [1/10], Batch [148/443], Loss: 0.1923\n",
      "Epoch [1/10], Batch [149/443], Loss: 0.1703\n",
      "Epoch [1/10], Batch [150/443], Loss: 0.1429\n",
      "Epoch [1/10], Batch [151/443], Loss: 0.0808\n",
      "Epoch [1/10], Batch [152/443], Loss: 0.2025\n",
      "Epoch [1/10], Batch [153/443], Loss: 0.1086\n",
      "Epoch [1/10], Batch [154/443], Loss: 0.0915\n",
      "Epoch [1/10], Batch [155/443], Loss: 0.1069\n",
      "Epoch [1/10], Batch [156/443], Loss: 0.1454\n",
      "Epoch [1/10], Batch [157/443], Loss: 0.4356\n",
      "Epoch [1/10], Batch [158/443], Loss: 0.0643\n",
      "Epoch [1/10], Batch [159/443], Loss: 0.1577\n",
      "Epoch [1/10], Batch [160/443], Loss: 0.1529\n",
      "Epoch [1/10], Batch [161/443], Loss: 0.1205\n",
      "Epoch [1/10], Batch [162/443], Loss: 0.0834\n",
      "Epoch [1/10], Batch [163/443], Loss: 0.2221\n",
      "Epoch [1/10], Batch [164/443], Loss: 0.1757\n",
      "Epoch [1/10], Batch [165/443], Loss: 0.0954\n",
      "Epoch [1/10], Batch [166/443], Loss: 0.0721\n",
      "Epoch [1/10], Batch [167/443], Loss: 0.1167\n",
      "Epoch [1/10], Batch [168/443], Loss: 0.1832\n",
      "Epoch [1/10], Batch [169/443], Loss: 0.1452\n",
      "Epoch [1/10], Batch [170/443], Loss: 0.0687\n",
      "Epoch [1/10], Batch [171/443], Loss: 0.1441\n",
      "Epoch [1/10], Batch [172/443], Loss: 0.0880\n",
      "Epoch [1/10], Batch [173/443], Loss: 0.1621\n",
      "Epoch [1/10], Batch [174/443], Loss: 0.0567\n",
      "Epoch [1/10], Batch [175/443], Loss: 0.1603\n",
      "Epoch [1/10], Batch [176/443], Loss: 0.1996\n",
      "Epoch [1/10], Batch [177/443], Loss: 0.1818\n",
      "Epoch [1/10], Batch [178/443], Loss: 0.1487\n",
      "Epoch [1/10], Batch [179/443], Loss: 0.1844\n",
      "Epoch [1/10], Batch [180/443], Loss: 0.0985\n",
      "Epoch [1/10], Batch [181/443], Loss: 0.0698\n",
      "Epoch [1/10], Batch [182/443], Loss: 0.1797\n",
      "Epoch [1/10], Batch [183/443], Loss: 0.1586\n",
      "Epoch [1/10], Batch [184/443], Loss: 0.0560\n",
      "Epoch [1/10], Batch [185/443], Loss: 0.0786\n",
      "Epoch [1/10], Batch [186/443], Loss: 0.1028\n",
      "Epoch [1/10], Batch [187/443], Loss: 0.0851\n",
      "Epoch [1/10], Batch [188/443], Loss: 0.1394\n",
      "Epoch [1/10], Batch [189/443], Loss: 0.1629\n",
      "Epoch [1/10], Batch [190/443], Loss: 0.2014\n",
      "Epoch [1/10], Batch [191/443], Loss: 0.1717\n",
      "Epoch [1/10], Batch [192/443], Loss: 0.2449\n",
      "Epoch [1/10], Batch [193/443], Loss: 0.1779\n",
      "Epoch [1/10], Batch [194/443], Loss: 0.1132\n",
      "Epoch [1/10], Batch [195/443], Loss: 0.1028\n",
      "Epoch [1/10], Batch [196/443], Loss: 0.1278\n",
      "Epoch [1/10], Batch [197/443], Loss: 0.0968\n",
      "Epoch [1/10], Batch [198/443], Loss: 0.0810\n",
      "Epoch [1/10], Batch [199/443], Loss: 0.1205\n",
      "Epoch [1/10], Batch [200/443], Loss: 0.0835\n",
      "Epoch [1/10], Batch [201/443], Loss: 0.1047\n",
      "Epoch [1/10], Batch [202/443], Loss: 0.0858\n",
      "Epoch [1/10], Batch [203/443], Loss: 0.0900\n",
      "Epoch [1/10], Batch [204/443], Loss: 0.1287\n",
      "Epoch [1/10], Batch [205/443], Loss: 0.1920\n",
      "Epoch [1/10], Batch [206/443], Loss: 0.1802\n",
      "Epoch [1/10], Batch [207/443], Loss: 0.0948\n",
      "Epoch [1/10], Batch [208/443], Loss: 0.1951\n",
      "Epoch [1/10], Batch [209/443], Loss: 0.1041\n",
      "Epoch [1/10], Batch [210/443], Loss: 0.0569\n",
      "Epoch [1/10], Batch [211/443], Loss: 0.0811\n",
      "Epoch [1/10], Batch [212/443], Loss: 0.0739\n",
      "Epoch [1/10], Batch [213/443], Loss: 0.1870\n",
      "Epoch [1/10], Batch [214/443], Loss: 0.1037\n",
      "Epoch [1/10], Batch [215/443], Loss: 0.1952\n",
      "Epoch [1/10], Batch [216/443], Loss: 0.1964\n",
      "Epoch [1/10], Batch [217/443], Loss: 0.1230\n",
      "Epoch [1/10], Batch [218/443], Loss: 0.3139\n",
      "Epoch [1/10], Batch [219/443], Loss: 0.0797\n",
      "Epoch [1/10], Batch [220/443], Loss: 0.1515\n",
      "Epoch [1/10], Batch [221/443], Loss: 0.1506\n",
      "Epoch [1/10], Batch [222/443], Loss: 0.1680\n",
      "Epoch [1/10], Batch [223/443], Loss: 0.0819\n",
      "Epoch [1/10], Batch [224/443], Loss: 0.0903\n",
      "Epoch [1/10], Batch [225/443], Loss: 0.1171\n",
      "Epoch [1/10], Batch [226/443], Loss: 0.3134\n",
      "Epoch [1/10], Batch [227/443], Loss: 0.1780\n",
      "Epoch [1/10], Batch [228/443], Loss: 0.1469\n",
      "Epoch [1/10], Batch [229/443], Loss: 0.0373\n",
      "Epoch [1/10], Batch [230/443], Loss: 0.2212\n",
      "Epoch [1/10], Batch [231/443], Loss: 0.1099\n",
      "Epoch [1/10], Batch [232/443], Loss: 0.3165\n",
      "Epoch [1/10], Batch [233/443], Loss: 0.1092\n",
      "Epoch [1/10], Batch [234/443], Loss: 0.0861\n",
      "Epoch [1/10], Batch [235/443], Loss: 0.1610\n",
      "Epoch [1/10], Batch [236/443], Loss: 0.1415\n",
      "Epoch [1/10], Batch [237/443], Loss: 0.1127\n",
      "Epoch [1/10], Batch [238/443], Loss: 0.1448\n",
      "Epoch [1/10], Batch [239/443], Loss: 0.0989\n",
      "Epoch [1/10], Batch [240/443], Loss: 0.1230\n",
      "Epoch [1/10], Batch [241/443], Loss: 0.1426\n",
      "Epoch [1/10], Batch [242/443], Loss: 0.1319\n",
      "Epoch [1/10], Batch [243/443], Loss: 0.0772\n",
      "Epoch [1/10], Batch [244/443], Loss: 0.1062\n",
      "Epoch [1/10], Batch [245/443], Loss: 0.2087\n",
      "Epoch [1/10], Batch [246/443], Loss: 0.2145\n",
      "Epoch [1/10], Batch [247/443], Loss: 0.1500\n",
      "Epoch [1/10], Batch [248/443], Loss: 0.2426\n",
      "Epoch [1/10], Batch [249/443], Loss: 0.1672\n",
      "Epoch [1/10], Batch [250/443], Loss: 0.0890\n",
      "Epoch [1/10], Batch [251/443], Loss: 0.1188\n",
      "Epoch [1/10], Batch [252/443], Loss: 0.1445\n",
      "Epoch [1/10], Batch [253/443], Loss: 0.0831\n",
      "Epoch [1/10], Batch [254/443], Loss: 0.0454\n",
      "Epoch [1/10], Batch [255/443], Loss: 0.4711\n",
      "Epoch [1/10], Batch [256/443], Loss: 0.1058\n",
      "Epoch [1/10], Batch [257/443], Loss: 0.1080\n",
      "Epoch [1/10], Batch [258/443], Loss: 0.1032\n",
      "Epoch [1/10], Batch [259/443], Loss: 0.2441\n",
      "Epoch [1/10], Batch [260/443], Loss: 0.1830\n",
      "Epoch [1/10], Batch [261/443], Loss: 0.1712\n",
      "Epoch [1/10], Batch [262/443], Loss: 0.0392\n",
      "Epoch [1/10], Batch [263/443], Loss: 0.1093\n",
      "Epoch [1/10], Batch [264/443], Loss: 0.0480\n",
      "Epoch [1/10], Batch [265/443], Loss: 0.0518\n",
      "Epoch [1/10], Batch [266/443], Loss: 0.1649\n",
      "Epoch [1/10], Batch [267/443], Loss: 0.0800\n",
      "Epoch [1/10], Batch [268/443], Loss: 0.1141\n",
      "Epoch [1/10], Batch [269/443], Loss: 0.1039\n",
      "Epoch [1/10], Batch [270/443], Loss: 0.0590\n",
      "Epoch [1/10], Batch [271/443], Loss: 0.0905\n",
      "Epoch [1/10], Batch [272/443], Loss: 0.0492\n",
      "Epoch [1/10], Batch [273/443], Loss: 0.0967\n",
      "Epoch [1/10], Batch [274/443], Loss: 0.0560\n",
      "Epoch [1/10], Batch [275/443], Loss: 0.1089\n",
      "Epoch [1/10], Batch [276/443], Loss: 0.1582\n",
      "Epoch [1/10], Batch [277/443], Loss: 0.2334\n",
      "Epoch [1/10], Batch [278/443], Loss: 0.0860\n",
      "Epoch [1/10], Batch [279/443], Loss: 0.1022\n",
      "Epoch [1/10], Batch [280/443], Loss: 0.1482\n",
      "Epoch [1/10], Batch [281/443], Loss: 0.0966\n",
      "Epoch [1/10], Batch [282/443], Loss: 0.1435\n",
      "Epoch [1/10], Batch [283/443], Loss: 0.1100\n",
      "Epoch [1/10], Batch [284/443], Loss: 0.0727\n",
      "Epoch [1/10], Batch [285/443], Loss: 0.0557\n",
      "Epoch [1/10], Batch [286/443], Loss: 0.1754\n",
      "Epoch [1/10], Batch [287/443], Loss: 0.1806\n",
      "Epoch [1/10], Batch [288/443], Loss: 0.2000\n",
      "Epoch [1/10], Batch [289/443], Loss: 0.1882\n",
      "Epoch [1/10], Batch [290/443], Loss: 0.1392\n",
      "Epoch [1/10], Batch [291/443], Loss: 0.0395\n",
      "Epoch [1/10], Batch [292/443], Loss: 0.1958\n",
      "Epoch [1/10], Batch [293/443], Loss: 0.0859\n",
      "Epoch [1/10], Batch [294/443], Loss: 0.1061\n",
      "Epoch [1/10], Batch [295/443], Loss: 0.1504\n",
      "Epoch [1/10], Batch [296/443], Loss: 0.0755\n",
      "Epoch [1/10], Batch [297/443], Loss: 0.0995\n",
      "Epoch [1/10], Batch [298/443], Loss: 0.1903\n",
      "Epoch [1/10], Batch [299/443], Loss: 0.0867\n",
      "Epoch [1/10], Batch [300/443], Loss: 0.0673\n",
      "Epoch [1/10], Batch [301/443], Loss: 0.1071\n",
      "Epoch [1/10], Batch [302/443], Loss: 0.1533\n",
      "Epoch [1/10], Batch [303/443], Loss: 0.0707\n",
      "Epoch [1/10], Batch [304/443], Loss: 0.0674\n",
      "Epoch [1/10], Batch [305/443], Loss: 0.0661\n",
      "Epoch [1/10], Batch [306/443], Loss: 0.2377\n",
      "Epoch [1/10], Batch [307/443], Loss: 0.1090\n",
      "Epoch [1/10], Batch [308/443], Loss: 0.1387\n",
      "Epoch [1/10], Batch [309/443], Loss: 0.0581\n",
      "Epoch [1/10], Batch [310/443], Loss: 0.1786\n",
      "Epoch [1/10], Batch [311/443], Loss: 0.1203\n",
      "Epoch [1/10], Batch [312/443], Loss: 0.0848\n",
      "Epoch [1/10], Batch [313/443], Loss: 0.0560\n",
      "Epoch [1/10], Batch [314/443], Loss: 0.1363\n",
      "Epoch [1/10], Batch [315/443], Loss: 0.0908\n",
      "Epoch [1/10], Batch [316/443], Loss: 0.1918\n",
      "Epoch [1/10], Batch [317/443], Loss: 0.0853\n",
      "Epoch [1/10], Batch [318/443], Loss: 0.1873\n",
      "Epoch [1/10], Batch [319/443], Loss: 0.0598\n",
      "Epoch [1/10], Batch [320/443], Loss: 0.0505\n",
      "Epoch [1/10], Batch [321/443], Loss: 0.1107\n",
      "Epoch [1/10], Batch [322/443], Loss: 0.1753\n",
      "Epoch [1/10], Batch [323/443], Loss: 0.1260\n",
      "Epoch [1/10], Batch [324/443], Loss: 0.0825\n",
      "Epoch [1/10], Batch [325/443], Loss: 0.1142\n",
      "Epoch [1/10], Batch [326/443], Loss: 0.0707\n",
      "Epoch [1/10], Batch [327/443], Loss: 0.1555\n",
      "Epoch [1/10], Batch [328/443], Loss: 0.1569\n",
      "Epoch [1/10], Batch [329/443], Loss: 0.0804\n",
      "Epoch [1/10], Batch [330/443], Loss: 0.1532\n",
      "Epoch [1/10], Batch [331/443], Loss: 0.1533\n",
      "Epoch [1/10], Batch [332/443], Loss: 0.1133\n",
      "Epoch [1/10], Batch [333/443], Loss: 0.1200\n",
      "Epoch [1/10], Batch [334/443], Loss: 0.0526\n",
      "Epoch [1/10], Batch [335/443], Loss: 0.1024\n",
      "Epoch [1/10], Batch [336/443], Loss: 0.1255\n",
      "Epoch [1/10], Batch [337/443], Loss: 0.2484\n",
      "Epoch [1/10], Batch [338/443], Loss: 0.0540\n",
      "Epoch [1/10], Batch [339/443], Loss: 0.0801\n",
      "Epoch [1/10], Batch [340/443], Loss: 0.0454\n",
      "Epoch [1/10], Batch [341/443], Loss: 0.0981\n",
      "Epoch [1/10], Batch [342/443], Loss: 0.1085\n",
      "Epoch [1/10], Batch [343/443], Loss: 0.0708\n",
      "Epoch [1/10], Batch [344/443], Loss: 0.0903\n",
      "Epoch [1/10], Batch [345/443], Loss: 0.2404\n",
      "Epoch [1/10], Batch [346/443], Loss: 0.0757\n",
      "Epoch [1/10], Batch [347/443], Loss: 0.1265\n",
      "Epoch [1/10], Batch [348/443], Loss: 0.1260\n",
      "Epoch [1/10], Batch [349/443], Loss: 0.1797\n",
      "Epoch [1/10], Batch [350/443], Loss: 0.1299\n",
      "Epoch [1/10], Batch [351/443], Loss: 0.1262\n",
      "Epoch [1/10], Batch [352/443], Loss: 0.1858\n",
      "Epoch [1/10], Batch [353/443], Loss: 0.1336\n",
      "Epoch [1/10], Batch [354/443], Loss: 0.0650\n",
      "Epoch [1/10], Batch [355/443], Loss: 0.2316\n",
      "Epoch [1/10], Batch [356/443], Loss: 0.2202\n",
      "Epoch [1/10], Batch [357/443], Loss: 0.1626\n",
      "Epoch [1/10], Batch [358/443], Loss: 0.1269\n",
      "Epoch [1/10], Batch [359/443], Loss: 0.1753\n",
      "Epoch [1/10], Batch [360/443], Loss: 0.0588\n",
      "Epoch [1/10], Batch [361/443], Loss: 0.2412\n",
      "Epoch [1/10], Batch [362/443], Loss: 0.2109\n",
      "Epoch [1/10], Batch [363/443], Loss: 0.0618\n",
      "Epoch [1/10], Batch [364/443], Loss: 0.2766\n",
      "Epoch [1/10], Batch [365/443], Loss: 0.1737\n",
      "Epoch [1/10], Batch [366/443], Loss: 0.1670\n",
      "Epoch [1/10], Batch [367/443], Loss: 0.0904\n",
      "Epoch [1/10], Batch [368/443], Loss: 0.1441\n",
      "Epoch [1/10], Batch [369/443], Loss: 0.2172\n",
      "Epoch [1/10], Batch [370/443], Loss: 0.3326\n",
      "Epoch [1/10], Batch [371/443], Loss: 0.1349\n",
      "Epoch [1/10], Batch [372/443], Loss: 0.2902\n",
      "Epoch [1/10], Batch [373/443], Loss: 0.0642\n",
      "Epoch [1/10], Batch [374/443], Loss: 0.1281\n",
      "Epoch [1/10], Batch [375/443], Loss: 0.1721\n",
      "Epoch [1/10], Batch [376/443], Loss: 0.1142\n",
      "Epoch [1/10], Batch [377/443], Loss: 0.1704\n",
      "Epoch [1/10], Batch [378/443], Loss: 0.1342\n",
      "Epoch [1/10], Batch [379/443], Loss: 0.1799\n",
      "Epoch [1/10], Batch [380/443], Loss: 0.1104\n",
      "Epoch [1/10], Batch [381/443], Loss: 0.0658\n",
      "Epoch [1/10], Batch [382/443], Loss: 0.0932\n",
      "Epoch [1/10], Batch [383/443], Loss: 0.1109\n",
      "Epoch [1/10], Batch [384/443], Loss: 0.1130\n",
      "Epoch [1/10], Batch [385/443], Loss: 0.0996\n",
      "Epoch [1/10], Batch [386/443], Loss: 0.0516\n",
      "Epoch [1/10], Batch [387/443], Loss: 0.1534\n",
      "Epoch [1/10], Batch [388/443], Loss: 0.1601\n",
      "Epoch [1/10], Batch [389/443], Loss: 0.0666\n",
      "Epoch [1/10], Batch [390/443], Loss: 0.0993\n",
      "Epoch [1/10], Batch [391/443], Loss: 0.1036\n",
      "Epoch [1/10], Batch [392/443], Loss: 0.0975\n",
      "Epoch [1/10], Batch [393/443], Loss: 0.0991\n",
      "Epoch [1/10], Batch [394/443], Loss: 0.0532\n",
      "Epoch [1/10], Batch [395/443], Loss: 0.0587\n",
      "Epoch [1/10], Batch [396/443], Loss: 0.1766\n",
      "Epoch [1/10], Batch [397/443], Loss: 0.1648\n",
      "Epoch [1/10], Batch [398/443], Loss: 0.0757\n",
      "Epoch [1/10], Batch [399/443], Loss: 0.1051\n",
      "Epoch [1/10], Batch [400/443], Loss: 0.1929\n",
      "Epoch [1/10], Batch [401/443], Loss: 0.0467\n",
      "Epoch [1/10], Batch [402/443], Loss: 0.0810\n",
      "Epoch [1/10], Batch [403/443], Loss: 0.2271\n",
      "Epoch [1/10], Batch [404/443], Loss: 0.0608\n",
      "Epoch [1/10], Batch [405/443], Loss: 0.0859\n",
      "Epoch [1/10], Batch [406/443], Loss: 0.0869\n",
      "Epoch [1/10], Batch [407/443], Loss: 0.5519\n",
      "Epoch [1/10], Batch [408/443], Loss: 0.0722\n",
      "Epoch [1/10], Batch [409/443], Loss: 0.0709\n",
      "Epoch [1/10], Batch [410/443], Loss: 0.1094\n",
      "Epoch [1/10], Batch [411/443], Loss: 0.1180\n",
      "Epoch [1/10], Batch [412/443], Loss: 0.2165\n",
      "Epoch [1/10], Batch [413/443], Loss: 0.2615\n",
      "Epoch [1/10], Batch [414/443], Loss: 0.0790\n",
      "Epoch [1/10], Batch [415/443], Loss: 0.0306\n",
      "Epoch [1/10], Batch [416/443], Loss: 0.1434\n",
      "Epoch [1/10], Batch [417/443], Loss: 0.0603\n",
      "Epoch [1/10], Batch [418/443], Loss: 0.0968\n",
      "Epoch [1/10], Batch [419/443], Loss: 0.1041\n",
      "Epoch [1/10], Batch [420/443], Loss: 0.0822\n",
      "Epoch [1/10], Batch [421/443], Loss: 0.0487\n",
      "Epoch [1/10], Batch [422/443], Loss: 0.0751\n",
      "Epoch [1/10], Batch [423/443], Loss: 0.1469\n",
      "Epoch [1/10], Batch [424/443], Loss: 0.1747\n",
      "Epoch [1/10], Batch [425/443], Loss: 0.0958\n",
      "Epoch [1/10], Batch [426/443], Loss: 0.0639\n",
      "Epoch [1/10], Batch [427/443], Loss: 0.2154\n",
      "Epoch [1/10], Batch [428/443], Loss: 0.0830\n",
      "Epoch [1/10], Batch [429/443], Loss: 0.1151\n",
      "Epoch [1/10], Batch [430/443], Loss: 0.1480\n",
      "Epoch [1/10], Batch [431/443], Loss: 0.0970\n",
      "Epoch [1/10], Batch [432/443], Loss: 0.0686\n",
      "Epoch [1/10], Batch [433/443], Loss: 0.0733\n",
      "Epoch [1/10], Batch [434/443], Loss: 0.0953\n",
      "Epoch [1/10], Batch [435/443], Loss: 0.1159\n",
      "Epoch [1/10], Batch [436/443], Loss: 0.2377\n",
      "Epoch [1/10], Batch [437/443], Loss: 0.1329\n",
      "Epoch [1/10], Batch [438/443], Loss: 0.2355\n",
      "Epoch [1/10], Batch [439/443], Loss: 0.0666\n",
      "Epoch [1/10], Batch [440/443], Loss: 0.0415\n",
      "Epoch [1/10], Batch [441/443], Loss: 0.1238\n",
      "Epoch [1/10], Batch [442/443], Loss: 0.0997\n",
      "Epoch [1/10], Batch [443/443], Loss: 0.0726\n",
      "Epoch [2/10], Average Loss: 0.1362\n",
      "Model saved at ./saved_models/model_epoch_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2228\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Batch [1/443], Loss: 0.0426\n",
      "Epoch [2/10], Batch [2/443], Loss: 0.0777\n",
      "Epoch [2/10], Batch [3/443], Loss: 0.1016\n",
      "Epoch [2/10], Batch [4/443], Loss: 0.1017\n",
      "Epoch [2/10], Batch [5/443], Loss: 0.0489\n",
      "Epoch [2/10], Batch [6/443], Loss: 0.1583\n",
      "Epoch [2/10], Batch [7/443], Loss: 0.1039\n",
      "Epoch [2/10], Batch [8/443], Loss: 0.0566\n",
      "Epoch [2/10], Batch [9/443], Loss: 0.1440\n",
      "Epoch [2/10], Batch [10/443], Loss: 0.0530\n",
      "Epoch [2/10], Batch [11/443], Loss: 0.0409\n",
      "Epoch [2/10], Batch [12/443], Loss: 0.1178\n",
      "Epoch [2/10], Batch [13/443], Loss: 0.0655\n",
      "Epoch [2/10], Batch [14/443], Loss: 0.3897\n",
      "Epoch [2/10], Batch [15/443], Loss: 0.0758\n",
      "Epoch [2/10], Batch [16/443], Loss: 0.0773\n",
      "Epoch [2/10], Batch [17/443], Loss: 0.0426\n",
      "Epoch [2/10], Batch [18/443], Loss: 0.0425\n",
      "Epoch [2/10], Batch [19/443], Loss: 0.0402\n",
      "Epoch [2/10], Batch [20/443], Loss: 0.1425\n",
      "Epoch [2/10], Batch [21/443], Loss: 0.0690\n",
      "Epoch [2/10], Batch [22/443], Loss: 0.0878\n",
      "Epoch [2/10], Batch [23/443], Loss: 0.1552\n",
      "Epoch [2/10], Batch [24/443], Loss: 0.0734\n",
      "Epoch [2/10], Batch [25/443], Loss: 0.1037\n",
      "Epoch [2/10], Batch [26/443], Loss: 0.0687\n",
      "Epoch [2/10], Batch [27/443], Loss: 0.0871\n",
      "Epoch [2/10], Batch [28/443], Loss: 0.0757\n",
      "Epoch [2/10], Batch [29/443], Loss: 0.0722\n",
      "Epoch [2/10], Batch [30/443], Loss: 0.0789\n",
      "Epoch [2/10], Batch [31/443], Loss: 0.1715\n",
      "Epoch [2/10], Batch [32/443], Loss: 0.0492\n",
      "Epoch [2/10], Batch [33/443], Loss: 0.0447\n",
      "Epoch [2/10], Batch [34/443], Loss: 0.0809\n",
      "Epoch [2/10], Batch [35/443], Loss: 0.0616\n",
      "Epoch [2/10], Batch [36/443], Loss: 0.1470\n",
      "Epoch [2/10], Batch [37/443], Loss: 0.0364\n",
      "Epoch [2/10], Batch [38/443], Loss: 0.0792\n",
      "Epoch [2/10], Batch [39/443], Loss: 0.1134\n",
      "Epoch [2/10], Batch [40/443], Loss: 0.0374\n",
      "Epoch [2/10], Batch [41/443], Loss: 0.0492\n",
      "Epoch [2/10], Batch [42/443], Loss: 0.0270\n",
      "Epoch [2/10], Batch [43/443], Loss: 0.0364\n",
      "Epoch [2/10], Batch [44/443], Loss: 0.1206\n",
      "Epoch [2/10], Batch [45/443], Loss: 0.0417\n",
      "Epoch [2/10], Batch [46/443], Loss: 0.0387\n",
      "Epoch [2/10], Batch [47/443], Loss: 0.0846\n",
      "Epoch [2/10], Batch [48/443], Loss: 0.0522\n",
      "Epoch [2/10], Batch [49/443], Loss: 0.0376\n",
      "Epoch [2/10], Batch [50/443], Loss: 0.1505\n",
      "Epoch [2/10], Batch [51/443], Loss: 0.0321\n",
      "Epoch [2/10], Batch [52/443], Loss: 0.0990\n",
      "Epoch [2/10], Batch [53/443], Loss: 0.0211\n",
      "Epoch [2/10], Batch [54/443], Loss: 0.1687\n",
      "Epoch [2/10], Batch [55/443], Loss: 0.0224\n",
      "Epoch [2/10], Batch [56/443], Loss: 0.1564\n",
      "Epoch [2/10], Batch [57/443], Loss: 0.0464\n",
      "Epoch [2/10], Batch [58/443], Loss: 0.0310\n",
      "Epoch [2/10], Batch [59/443], Loss: 0.0781\n",
      "Epoch [2/10], Batch [60/443], Loss: 0.0578\n",
      "Epoch [2/10], Batch [61/443], Loss: 0.0628\n",
      "Epoch [2/10], Batch [62/443], Loss: 0.1007\n",
      "Epoch [2/10], Batch [63/443], Loss: 0.0512\n",
      "Epoch [2/10], Batch [64/443], Loss: 0.0342\n",
      "Epoch [2/10], Batch [65/443], Loss: 0.0487\n",
      "Epoch [2/10], Batch [66/443], Loss: 0.1024\n",
      "Epoch [2/10], Batch [67/443], Loss: 0.0701\n",
      "Epoch [2/10], Batch [68/443], Loss: 0.1074\n",
      "Epoch [2/10], Batch [69/443], Loss: 0.1041\n",
      "Epoch [2/10], Batch [70/443], Loss: 0.0372\n",
      "Epoch [2/10], Batch [71/443], Loss: 0.0834\n",
      "Epoch [2/10], Batch [72/443], Loss: 0.0991\n",
      "Epoch [2/10], Batch [73/443], Loss: 0.0430\n",
      "Epoch [2/10], Batch [74/443], Loss: 0.1288\n",
      "Epoch [2/10], Batch [75/443], Loss: 0.0925\n",
      "Epoch [2/10], Batch [76/443], Loss: 0.0232\n",
      "Epoch [2/10], Batch [77/443], Loss: 0.0240\n",
      "Epoch [2/10], Batch [78/443], Loss: 0.1499\n",
      "Epoch [2/10], Batch [79/443], Loss: 0.1005\n",
      "Epoch [2/10], Batch [80/443], Loss: 0.2909\n",
      "Epoch [2/10], Batch [81/443], Loss: 0.0276\n",
      "Epoch [2/10], Batch [82/443], Loss: 0.1094\n",
      "Epoch [2/10], Batch [83/443], Loss: 0.0456\n",
      "Epoch [2/10], Batch [84/443], Loss: 0.1396\n",
      "Epoch [2/10], Batch [85/443], Loss: 0.0597\n",
      "Epoch [2/10], Batch [86/443], Loss: 0.1098\n",
      "Epoch [2/10], Batch [87/443], Loss: 0.0644\n",
      "Epoch [2/10], Batch [88/443], Loss: 0.0435\n",
      "Epoch [2/10], Batch [89/443], Loss: 0.0364\n",
      "Epoch [2/10], Batch [90/443], Loss: 0.1190\n",
      "Epoch [2/10], Batch [91/443], Loss: 0.0678\n",
      "Epoch [2/10], Batch [92/443], Loss: 0.0573\n",
      "Epoch [2/10], Batch [93/443], Loss: 0.0275\n",
      "Epoch [2/10], Batch [94/443], Loss: 0.0386\n",
      "Epoch [2/10], Batch [95/443], Loss: 0.0299\n",
      "Epoch [2/10], Batch [96/443], Loss: 0.0756\n",
      "Epoch [2/10], Batch [97/443], Loss: 0.1070\n",
      "Epoch [2/10], Batch [98/443], Loss: 0.1851\n",
      "Epoch [2/10], Batch [99/443], Loss: 0.0915\n",
      "Epoch [2/10], Batch [100/443], Loss: 0.0262\n",
      "Epoch [2/10], Batch [101/443], Loss: 0.0486\n",
      "Epoch [2/10], Batch [102/443], Loss: 0.0573\n",
      "Epoch [2/10], Batch [103/443], Loss: 0.1277\n",
      "Epoch [2/10], Batch [104/443], Loss: 0.0529\n",
      "Epoch [2/10], Batch [105/443], Loss: 0.1733\n",
      "Epoch [2/10], Batch [106/443], Loss: 0.1721\n",
      "Epoch [2/10], Batch [107/443], Loss: 0.0551\n",
      "Epoch [2/10], Batch [108/443], Loss: 0.0679\n",
      "Epoch [2/10], Batch [109/443], Loss: 0.0291\n",
      "Epoch [2/10], Batch [110/443], Loss: 0.0771\n",
      "Epoch [2/10], Batch [111/443], Loss: 0.0631\n",
      "Epoch [2/10], Batch [112/443], Loss: 0.0499\n",
      "Epoch [2/10], Batch [113/443], Loss: 0.0378\n",
      "Epoch [2/10], Batch [114/443], Loss: 0.0959\n",
      "Epoch [2/10], Batch [115/443], Loss: 0.0531\n",
      "Epoch [2/10], Batch [116/443], Loss: 0.1942\n",
      "Epoch [2/10], Batch [117/443], Loss: 0.0925\n",
      "Epoch [2/10], Batch [118/443], Loss: 0.0309\n",
      "Epoch [2/10], Batch [119/443], Loss: 0.0610\n",
      "Epoch [2/10], Batch [120/443], Loss: 0.0514\n",
      "Epoch [2/10], Batch [121/443], Loss: 0.0751\n",
      "Epoch [2/10], Batch [122/443], Loss: 0.0336\n",
      "Epoch [2/10], Batch [123/443], Loss: 0.0879\n",
      "Epoch [2/10], Batch [124/443], Loss: 0.0453\n",
      "Epoch [2/10], Batch [125/443], Loss: 0.0327\n",
      "Epoch [2/10], Batch [126/443], Loss: 0.0681\n",
      "Epoch [2/10], Batch [127/443], Loss: 0.1869\n",
      "Epoch [2/10], Batch [128/443], Loss: 0.0656\n",
      "Epoch [2/10], Batch [129/443], Loss: 0.0479\n",
      "Epoch [2/10], Batch [130/443], Loss: 0.0900\n",
      "Epoch [2/10], Batch [131/443], Loss: 0.0723\n",
      "Epoch [2/10], Batch [132/443], Loss: 0.0519\n",
      "Epoch [2/10], Batch [133/443], Loss: 0.0637\n",
      "Epoch [2/10], Batch [134/443], Loss: 0.1723\n",
      "Epoch [2/10], Batch [135/443], Loss: 0.0848\n",
      "Epoch [2/10], Batch [136/443], Loss: 0.0784\n",
      "Epoch [2/10], Batch [137/443], Loss: 0.0433\n",
      "Epoch [2/10], Batch [138/443], Loss: 0.0400\n",
      "Epoch [2/10], Batch [139/443], Loss: 0.0510\n",
      "Epoch [2/10], Batch [140/443], Loss: 0.0309\n",
      "Epoch [2/10], Batch [141/443], Loss: 0.0825\n",
      "Epoch [2/10], Batch [142/443], Loss: 0.1171\n",
      "Epoch [2/10], Batch [143/443], Loss: 0.0378\n",
      "Epoch [2/10], Batch [144/443], Loss: 0.0298\n",
      "Epoch [2/10], Batch [145/443], Loss: 0.0487\n",
      "Epoch [2/10], Batch [146/443], Loss: 0.0995\n",
      "Epoch [2/10], Batch [147/443], Loss: 0.1070\n",
      "Epoch [2/10], Batch [148/443], Loss: 0.0379\n",
      "Epoch [2/10], Batch [149/443], Loss: 0.0514\n",
      "Epoch [2/10], Batch [150/443], Loss: 0.0347\n",
      "Epoch [2/10], Batch [151/443], Loss: 0.0821\n",
      "Epoch [2/10], Batch [152/443], Loss: 0.2028\n",
      "Epoch [2/10], Batch [153/443], Loss: 0.0222\n",
      "Epoch [2/10], Batch [154/443], Loss: 0.0940\n",
      "Epoch [2/10], Batch [155/443], Loss: 0.0588\n",
      "Epoch [2/10], Batch [156/443], Loss: 0.0950\n",
      "Epoch [2/10], Batch [157/443], Loss: 0.0817\n",
      "Epoch [2/10], Batch [158/443], Loss: 0.0829\n",
      "Epoch [2/10], Batch [159/443], Loss: 0.0635\n",
      "Epoch [2/10], Batch [160/443], Loss: 0.0732\n",
      "Epoch [2/10], Batch [161/443], Loss: 0.0523\n",
      "Epoch [2/10], Batch [162/443], Loss: 0.0548\n",
      "Epoch [2/10], Batch [163/443], Loss: 0.1580\n",
      "Epoch [2/10], Batch [164/443], Loss: 0.0539\n",
      "Epoch [2/10], Batch [165/443], Loss: 0.0498\n",
      "Epoch [2/10], Batch [166/443], Loss: 0.1038\n",
      "Epoch [2/10], Batch [167/443], Loss: 0.0744\n",
      "Epoch [2/10], Batch [168/443], Loss: 0.1016\n",
      "Epoch [2/10], Batch [169/443], Loss: 0.0536\n",
      "Epoch [2/10], Batch [170/443], Loss: 0.0760\n",
      "Epoch [2/10], Batch [171/443], Loss: 0.0356\n",
      "Epoch [2/10], Batch [172/443], Loss: 0.0456\n",
      "Epoch [2/10], Batch [173/443], Loss: 0.0734\n",
      "Epoch [2/10], Batch [174/443], Loss: 0.0379\n",
      "Epoch [2/10], Batch [175/443], Loss: 0.0807\n",
      "Epoch [2/10], Batch [176/443], Loss: 0.0721\n",
      "Epoch [2/10], Batch [177/443], Loss: 0.0425\n",
      "Epoch [2/10], Batch [178/443], Loss: 0.0568\n",
      "Epoch [2/10], Batch [179/443], Loss: 0.0335\n",
      "Epoch [2/10], Batch [180/443], Loss: 0.0646\n",
      "Epoch [2/10], Batch [181/443], Loss: 0.0888\n",
      "Epoch [2/10], Batch [182/443], Loss: 0.0459\n",
      "Epoch [2/10], Batch [183/443], Loss: 0.0619\n",
      "Epoch [2/10], Batch [184/443], Loss: 0.1909\n",
      "Epoch [2/10], Batch [185/443], Loss: 0.0791\n",
      "Epoch [2/10], Batch [186/443], Loss: 0.1727\n",
      "Epoch [2/10], Batch [187/443], Loss: 0.1193\n",
      "Epoch [2/10], Batch [188/443], Loss: 0.0349\n",
      "Epoch [2/10], Batch [189/443], Loss: 0.1130\n",
      "Epoch [2/10], Batch [190/443], Loss: 0.0621\n",
      "Epoch [2/10], Batch [191/443], Loss: 0.1105\n",
      "Epoch [2/10], Batch [192/443], Loss: 0.0346\n",
      "Epoch [2/10], Batch [193/443], Loss: 0.0502\n",
      "Epoch [2/10], Batch [194/443], Loss: 0.0437\n",
      "Epoch [2/10], Batch [195/443], Loss: 0.1120\n",
      "Epoch [2/10], Batch [196/443], Loss: 0.0615\n",
      "Epoch [2/10], Batch [197/443], Loss: 0.0468\n",
      "Epoch [2/10], Batch [198/443], Loss: 0.1196\n",
      "Epoch [2/10], Batch [199/443], Loss: 0.1437\n",
      "Epoch [2/10], Batch [200/443], Loss: 0.0559\n",
      "Epoch [2/10], Batch [201/443], Loss: 0.0446\n",
      "Epoch [2/10], Batch [202/443], Loss: 0.0370\n",
      "Epoch [2/10], Batch [203/443], Loss: 0.1150\n",
      "Epoch [2/10], Batch [204/443], Loss: 0.0776\n",
      "Epoch [2/10], Batch [205/443], Loss: 0.1425\n",
      "Epoch [2/10], Batch [206/443], Loss: 0.1336\n",
      "Epoch [2/10], Batch [207/443], Loss: 0.0614\n",
      "Epoch [2/10], Batch [208/443], Loss: 0.0549\n",
      "Epoch [2/10], Batch [209/443], Loss: 0.0173\n",
      "Epoch [2/10], Batch [210/443], Loss: 0.0642\n",
      "Epoch [2/10], Batch [211/443], Loss: 0.1137\n",
      "Epoch [2/10], Batch [212/443], Loss: 0.0676\n",
      "Epoch [2/10], Batch [213/443], Loss: 0.0647\n",
      "Epoch [2/10], Batch [214/443], Loss: 0.0740\n",
      "Epoch [2/10], Batch [215/443], Loss: 0.1027\n",
      "Epoch [2/10], Batch [216/443], Loss: 0.0815\n",
      "Epoch [2/10], Batch [217/443], Loss: 0.0935\n",
      "Epoch [2/10], Batch [218/443], Loss: 0.1408\n",
      "Epoch [2/10], Batch [219/443], Loss: 0.1323\n",
      "Epoch [2/10], Batch [220/443], Loss: 0.0159\n",
      "Epoch [2/10], Batch [221/443], Loss: 0.0323\n",
      "Epoch [2/10], Batch [222/443], Loss: 0.0836\n",
      "Epoch [2/10], Batch [223/443], Loss: 0.0735\n",
      "Epoch [2/10], Batch [224/443], Loss: 0.0771\n",
      "Epoch [2/10], Batch [225/443], Loss: 0.1063\n",
      "Epoch [2/10], Batch [226/443], Loss: 0.0697\n",
      "Epoch [2/10], Batch [227/443], Loss: 0.0538\n",
      "Epoch [2/10], Batch [228/443], Loss: 0.0693\n",
      "Epoch [2/10], Batch [229/443], Loss: 0.1497\n",
      "Epoch [2/10], Batch [230/443], Loss: 0.0745\n",
      "Epoch [2/10], Batch [231/443], Loss: 0.0809\n",
      "Epoch [2/10], Batch [232/443], Loss: 0.0714\n",
      "Epoch [2/10], Batch [233/443], Loss: 0.0603\n",
      "Epoch [2/10], Batch [234/443], Loss: 0.0708\n",
      "Epoch [2/10], Batch [235/443], Loss: 0.2152\n",
      "Epoch [2/10], Batch [236/443], Loss: 0.1501\n",
      "Epoch [2/10], Batch [237/443], Loss: 0.0595\n",
      "Epoch [2/10], Batch [238/443], Loss: 0.0414\n",
      "Epoch [2/10], Batch [239/443], Loss: 0.0554\n",
      "Epoch [2/10], Batch [240/443], Loss: 0.0799\n",
      "Epoch [2/10], Batch [241/443], Loss: 0.0402\n",
      "Epoch [2/10], Batch [242/443], Loss: 0.1342\n",
      "Epoch [2/10], Batch [243/443], Loss: 0.0388\n",
      "Epoch [2/10], Batch [244/443], Loss: 0.0670\n",
      "Epoch [2/10], Batch [245/443], Loss: 0.2191\n",
      "Epoch [2/10], Batch [246/443], Loss: 0.1081\n",
      "Epoch [2/10], Batch [247/443], Loss: 0.1249\n",
      "Epoch [2/10], Batch [248/443], Loss: 0.0570\n",
      "Epoch [2/10], Batch [249/443], Loss: 0.0650\n",
      "Epoch [2/10], Batch [250/443], Loss: 0.0435\n",
      "Epoch [2/10], Batch [251/443], Loss: 0.0997\n",
      "Epoch [2/10], Batch [252/443], Loss: 0.0769\n",
      "Epoch [2/10], Batch [253/443], Loss: 0.0467\n",
      "Epoch [2/10], Batch [254/443], Loss: 0.0484\n",
      "Epoch [2/10], Batch [255/443], Loss: 0.0545\n",
      "Epoch [2/10], Batch [256/443], Loss: 0.0288\n",
      "Epoch [2/10], Batch [257/443], Loss: 0.0550\n",
      "Epoch [2/10], Batch [258/443], Loss: 0.0359\n",
      "Epoch [2/10], Batch [259/443], Loss: 0.0951\n",
      "Epoch [2/10], Batch [260/443], Loss: 0.0499\n",
      "Epoch [2/10], Batch [261/443], Loss: 0.0452\n",
      "Epoch [2/10], Batch [262/443], Loss: 0.0200\n",
      "Epoch [2/10], Batch [263/443], Loss: 0.0835\n",
      "Epoch [2/10], Batch [264/443], Loss: 0.1347\n",
      "Epoch [2/10], Batch [265/443], Loss: 0.0656\n",
      "Epoch [2/10], Batch [266/443], Loss: 0.0577\n",
      "Epoch [2/10], Batch [267/443], Loss: 0.0609\n",
      "Epoch [2/10], Batch [268/443], Loss: 0.1320\n",
      "Epoch [2/10], Batch [269/443], Loss: 0.1294\n",
      "Epoch [2/10], Batch [270/443], Loss: 0.0381\n",
      "Epoch [2/10], Batch [271/443], Loss: 0.0312\n",
      "Epoch [2/10], Batch [272/443], Loss: 0.0729\n",
      "Epoch [2/10], Batch [273/443], Loss: 0.0288\n",
      "Epoch [2/10], Batch [274/443], Loss: 0.1122\n",
      "Epoch [2/10], Batch [275/443], Loss: 0.1528\n",
      "Epoch [2/10], Batch [276/443], Loss: 0.0762\n",
      "Epoch [2/10], Batch [277/443], Loss: 0.0738\n",
      "Epoch [2/10], Batch [278/443], Loss: 0.1553\n",
      "Epoch [2/10], Batch [279/443], Loss: 0.0926\n",
      "Epoch [2/10], Batch [280/443], Loss: 0.0213\n",
      "Epoch [2/10], Batch [281/443], Loss: 0.0755\n",
      "Epoch [2/10], Batch [282/443], Loss: 0.0670\n",
      "Epoch [2/10], Batch [283/443], Loss: 0.0313\n",
      "Epoch [2/10], Batch [284/443], Loss: 0.0389\n",
      "Epoch [2/10], Batch [285/443], Loss: 0.1205\n",
      "Epoch [2/10], Batch [286/443], Loss: 0.0635\n",
      "Epoch [2/10], Batch [287/443], Loss: 0.1549\n",
      "Epoch [2/10], Batch [288/443], Loss: 0.0493\n",
      "Epoch [2/10], Batch [289/443], Loss: 0.1165\n",
      "Epoch [2/10], Batch [290/443], Loss: 0.0940\n",
      "Epoch [2/10], Batch [291/443], Loss: 0.0991\n",
      "Epoch [2/10], Batch [292/443], Loss: 0.0405\n",
      "Epoch [2/10], Batch [293/443], Loss: 0.0448\n",
      "Epoch [2/10], Batch [294/443], Loss: 0.0806\n",
      "Epoch [2/10], Batch [295/443], Loss: 0.0642\n",
      "Epoch [2/10], Batch [296/443], Loss: 0.0510\n",
      "Epoch [2/10], Batch [297/443], Loss: 0.0769\n",
      "Epoch [2/10], Batch [298/443], Loss: 0.1591\n",
      "Epoch [2/10], Batch [299/443], Loss: 0.0459\n",
      "Epoch [2/10], Batch [300/443], Loss: 0.1742\n",
      "Epoch [2/10], Batch [301/443], Loss: 0.0307\n",
      "Epoch [2/10], Batch [302/443], Loss: 0.0901\n",
      "Epoch [2/10], Batch [303/443], Loss: 0.0679\n",
      "Epoch [2/10], Batch [304/443], Loss: 0.1261\n",
      "Epoch [2/10], Batch [305/443], Loss: 0.0720\n",
      "Epoch [2/10], Batch [306/443], Loss: 0.0350\n",
      "Epoch [2/10], Batch [307/443], Loss: 0.0189\n",
      "Epoch [2/10], Batch [308/443], Loss: 0.0869\n",
      "Epoch [2/10], Batch [309/443], Loss: 0.0609\n",
      "Epoch [2/10], Batch [310/443], Loss: 0.1016\n",
      "Epoch [2/10], Batch [311/443], Loss: 0.0922\n",
      "Epoch [2/10], Batch [312/443], Loss: 0.0505\n",
      "Epoch [2/10], Batch [313/443], Loss: 0.0256\n",
      "Epoch [2/10], Batch [314/443], Loss: 0.0809\n",
      "Epoch [2/10], Batch [315/443], Loss: 0.0985\n",
      "Epoch [2/10], Batch [316/443], Loss: 0.0619\n",
      "Epoch [2/10], Batch [317/443], Loss: 0.0451\n",
      "Epoch [2/10], Batch [318/443], Loss: 0.0506\n",
      "Epoch [2/10], Batch [319/443], Loss: 0.0639\n",
      "Epoch [2/10], Batch [320/443], Loss: 0.0578\n",
      "Epoch [2/10], Batch [321/443], Loss: 0.0931\n",
      "Epoch [2/10], Batch [322/443], Loss: 0.3678\n",
      "Epoch [2/10], Batch [323/443], Loss: 0.1479\n",
      "Epoch [2/10], Batch [324/443], Loss: 0.1279\n",
      "Epoch [2/10], Batch [325/443], Loss: 0.0496\n",
      "Epoch [2/10], Batch [326/443], Loss: 0.0896\n",
      "Epoch [2/10], Batch [327/443], Loss: 0.1716\n",
      "Epoch [2/10], Batch [328/443], Loss: 0.0708\n",
      "Epoch [2/10], Batch [329/443], Loss: 0.0217\n",
      "Epoch [2/10], Batch [330/443], Loss: 0.1361\n",
      "Epoch [2/10], Batch [331/443], Loss: 0.0909\n",
      "Epoch [2/10], Batch [332/443], Loss: 0.0409\n",
      "Epoch [2/10], Batch [333/443], Loss: 0.1242\n",
      "Epoch [2/10], Batch [334/443], Loss: 0.1310\n",
      "Epoch [2/10], Batch [335/443], Loss: 0.0755\n",
      "Epoch [2/10], Batch [336/443], Loss: 0.0465\n",
      "Epoch [2/10], Batch [337/443], Loss: 0.0318\n",
      "Epoch [2/10], Batch [338/443], Loss: 0.0555\n",
      "Epoch [2/10], Batch [339/443], Loss: 0.1207\n",
      "Epoch [2/10], Batch [340/443], Loss: 0.1098\n",
      "Epoch [2/10], Batch [341/443], Loss: 0.0764\n",
      "Epoch [2/10], Batch [342/443], Loss: 0.0577\n",
      "Epoch [2/10], Batch [343/443], Loss: 0.0842\n",
      "Epoch [2/10], Batch [344/443], Loss: 0.0836\n",
      "Epoch [2/10], Batch [345/443], Loss: 0.2503\n",
      "Epoch [2/10], Batch [346/443], Loss: 0.0908\n",
      "Epoch [2/10], Batch [347/443], Loss: 0.0568\n",
      "Epoch [2/10], Batch [348/443], Loss: 0.0580\n",
      "Epoch [2/10], Batch [349/443], Loss: 0.1412\n",
      "Epoch [2/10], Batch [350/443], Loss: 0.0493\n",
      "Epoch [2/10], Batch [351/443], Loss: 0.0449\n",
      "Epoch [2/10], Batch [352/443], Loss: 0.0822\n",
      "Epoch [2/10], Batch [353/443], Loss: 0.0803\n",
      "Epoch [2/10], Batch [354/443], Loss: 0.1276\n",
      "Epoch [2/10], Batch [355/443], Loss: 0.2108\n",
      "Epoch [2/10], Batch [356/443], Loss: 0.1085\n",
      "Epoch [2/10], Batch [357/443], Loss: 0.0552\n",
      "Epoch [2/10], Batch [358/443], Loss: 0.0338\n",
      "Epoch [2/10], Batch [359/443], Loss: 0.1629\n",
      "Epoch [2/10], Batch [360/443], Loss: 0.1456\n",
      "Epoch [2/10], Batch [361/443], Loss: 0.1145\n",
      "Epoch [2/10], Batch [362/443], Loss: 0.0874\n",
      "Epoch [2/10], Batch [363/443], Loss: 0.1838\n",
      "Epoch [2/10], Batch [364/443], Loss: 0.0856\n",
      "Epoch [2/10], Batch [365/443], Loss: 0.0407\n",
      "Epoch [2/10], Batch [366/443], Loss: 0.0868\n",
      "Epoch [2/10], Batch [367/443], Loss: 0.1516\n",
      "Epoch [2/10], Batch [368/443], Loss: 0.0661\n",
      "Epoch [2/10], Batch [369/443], Loss: 0.0952\n",
      "Epoch [2/10], Batch [370/443], Loss: 0.1440\n",
      "Epoch [2/10], Batch [371/443], Loss: 0.0465\n",
      "Epoch [2/10], Batch [372/443], Loss: 0.0679\n",
      "Epoch [2/10], Batch [373/443], Loss: 0.0321\n",
      "Epoch [2/10], Batch [374/443], Loss: 0.1160\n",
      "Epoch [2/10], Batch [375/443], Loss: 0.0989\n",
      "Epoch [2/10], Batch [376/443], Loss: 0.0696\n",
      "Epoch [2/10], Batch [377/443], Loss: 0.0560\n",
      "Epoch [2/10], Batch [378/443], Loss: 0.0866\n",
      "Epoch [2/10], Batch [379/443], Loss: 0.0673\n",
      "Epoch [2/10], Batch [380/443], Loss: 0.0646\n",
      "Epoch [2/10], Batch [381/443], Loss: 0.0407\n",
      "Epoch [2/10], Batch [382/443], Loss: 0.1083\n",
      "Epoch [2/10], Batch [383/443], Loss: 0.2170\n",
      "Epoch [2/10], Batch [384/443], Loss: 0.0409\n",
      "Epoch [2/10], Batch [385/443], Loss: 0.0270\n",
      "Epoch [2/10], Batch [386/443], Loss: 0.0693\n",
      "Epoch [2/10], Batch [387/443], Loss: 0.1547\n",
      "Epoch [2/10], Batch [388/443], Loss: 0.1163\n",
      "Epoch [2/10], Batch [389/443], Loss: 0.0755\n",
      "Epoch [2/10], Batch [390/443], Loss: 0.0334\n",
      "Epoch [2/10], Batch [391/443], Loss: 0.1293\n",
      "Epoch [2/10], Batch [392/443], Loss: 0.0942\n",
      "Epoch [2/10], Batch [393/443], Loss: 0.1220\n",
      "Epoch [2/10], Batch [394/443], Loss: 0.0601\n",
      "Epoch [2/10], Batch [395/443], Loss: 0.0482\n",
      "Epoch [2/10], Batch [396/443], Loss: 0.0505\n",
      "Epoch [2/10], Batch [397/443], Loss: 0.0453\n",
      "Epoch [2/10], Batch [398/443], Loss: 0.0814\n",
      "Epoch [2/10], Batch [399/443], Loss: 0.0716\n",
      "Epoch [2/10], Batch [400/443], Loss: 0.0883\n",
      "Epoch [2/10], Batch [401/443], Loss: 0.1070\n",
      "Epoch [2/10], Batch [402/443], Loss: 0.0596\n",
      "Epoch [2/10], Batch [403/443], Loss: 0.0575\n",
      "Epoch [2/10], Batch [404/443], Loss: 0.0160\n",
      "Epoch [2/10], Batch [405/443], Loss: 0.1863\n",
      "Epoch [2/10], Batch [406/443], Loss: 0.1347\n",
      "Epoch [2/10], Batch [407/443], Loss: 0.0903\n",
      "Epoch [2/10], Batch [408/443], Loss: 0.0447\n",
      "Epoch [2/10], Batch [409/443], Loss: 0.0870\n",
      "Epoch [2/10], Batch [410/443], Loss: 0.0786\n",
      "Epoch [2/10], Batch [411/443], Loss: 0.0530\n",
      "Epoch [2/10], Batch [412/443], Loss: 0.0914\n",
      "Epoch [2/10], Batch [413/443], Loss: 0.1715\n",
      "Epoch [2/10], Batch [414/443], Loss: 0.0794\n",
      "Epoch [2/10], Batch [415/443], Loss: 0.0633\n",
      "Epoch [2/10], Batch [416/443], Loss: 0.0585\n",
      "Epoch [2/10], Batch [417/443], Loss: 0.0475\n",
      "Epoch [2/10], Batch [418/443], Loss: 0.0689\n",
      "Epoch [2/10], Batch [419/443], Loss: 0.2256\n",
      "Epoch [2/10], Batch [420/443], Loss: 0.0444\n",
      "Epoch [2/10], Batch [421/443], Loss: 0.0239\n",
      "Epoch [2/10], Batch [422/443], Loss: 0.0256\n",
      "Epoch [2/10], Batch [423/443], Loss: 0.0922\n",
      "Epoch [2/10], Batch [424/443], Loss: 0.0546\n",
      "Epoch [2/10], Batch [425/443], Loss: 0.0707\n",
      "Epoch [2/10], Batch [426/443], Loss: 0.0694\n",
      "Epoch [2/10], Batch [427/443], Loss: 0.0611\n",
      "Epoch [2/10], Batch [428/443], Loss: 0.0274\n",
      "Epoch [2/10], Batch [429/443], Loss: 0.0466\n",
      "Epoch [2/10], Batch [430/443], Loss: 0.1963\n",
      "Epoch [2/10], Batch [431/443], Loss: 0.0766\n",
      "Epoch [2/10], Batch [432/443], Loss: 0.0623\n",
      "Epoch [2/10], Batch [433/443], Loss: 0.0887\n",
      "Epoch [2/10], Batch [434/443], Loss: 0.0829\n",
      "Epoch [2/10], Batch [435/443], Loss: 0.0618\n",
      "Epoch [2/10], Batch [436/443], Loss: 0.1151\n",
      "Epoch [2/10], Batch [437/443], Loss: 0.0478\n",
      "Epoch [2/10], Batch [438/443], Loss: 0.1896\n",
      "Epoch [2/10], Batch [439/443], Loss: 0.0639\n",
      "Epoch [2/10], Batch [440/443], Loss: 0.0997\n",
      "Epoch [2/10], Batch [441/443], Loss: 0.1508\n",
      "Epoch [2/10], Batch [442/443], Loss: 0.0747\n",
      "Epoch [2/10], Batch [443/443], Loss: 0.1028\n",
      "Epoch [3/10], Average Loss: 0.0819\n",
      "Model saved at ./saved_models/model_epoch_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2228\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Batch [1/443], Loss: 0.1169\n",
      "Epoch [3/10], Batch [2/443], Loss: 0.0507\n",
      "Epoch [3/10], Batch [3/443], Loss: 0.0585\n",
      "Epoch [3/10], Batch [4/443], Loss: 0.1776\n",
      "Epoch [3/10], Batch [5/443], Loss: 0.1381\n",
      "Epoch [3/10], Batch [6/443], Loss: 0.0974\n",
      "Epoch [3/10], Batch [7/443], Loss: 0.0473\n",
      "Epoch [3/10], Batch [8/443], Loss: 0.0812\n",
      "Epoch [3/10], Batch [9/443], Loss: 0.1133\n",
      "Epoch [3/10], Batch [10/443], Loss: 0.0286\n",
      "Epoch [3/10], Batch [11/443], Loss: 0.0697\n",
      "Epoch [3/10], Batch [12/443], Loss: 0.0671\n",
      "Epoch [3/10], Batch [13/443], Loss: 0.0735\n",
      "Epoch [3/10], Batch [14/443], Loss: 0.0676\n",
      "Epoch [3/10], Batch [15/443], Loss: 0.1246\n",
      "Epoch [3/10], Batch [16/443], Loss: 0.0962\n",
      "Epoch [3/10], Batch [17/443], Loss: 0.0623\n",
      "Epoch [3/10], Batch [18/443], Loss: 0.0888\n",
      "Epoch [3/10], Batch [19/443], Loss: 0.0958\n",
      "Epoch [3/10], Batch [20/443], Loss: 0.0974\n",
      "Epoch [3/10], Batch [21/443], Loss: 0.0387\n",
      "Epoch [3/10], Batch [22/443], Loss: 0.1758\n",
      "Epoch [3/10], Batch [23/443], Loss: 0.0204\n",
      "Epoch [3/10], Batch [24/443], Loss: 0.0504\n",
      "Epoch [3/10], Batch [25/443], Loss: 0.1016\n",
      "Epoch [3/10], Batch [26/443], Loss: 0.0684\n",
      "Epoch [3/10], Batch [27/443], Loss: 0.0383\n",
      "Epoch [3/10], Batch [28/443], Loss: 0.0707\n",
      "Epoch [3/10], Batch [29/443], Loss: 0.0599\n",
      "Epoch [3/10], Batch [30/443], Loss: 0.1743\n",
      "Epoch [3/10], Batch [31/443], Loss: 0.1317\n",
      "Epoch [3/10], Batch [32/443], Loss: 0.0824\n",
      "Epoch [3/10], Batch [33/443], Loss: 0.0654\n",
      "Epoch [3/10], Batch [34/443], Loss: 0.0737\n",
      "Epoch [3/10], Batch [35/443], Loss: 0.0679\n",
      "Epoch [3/10], Batch [36/443], Loss: 0.0410\n",
      "Epoch [3/10], Batch [37/443], Loss: 0.0749\n",
      "Epoch [3/10], Batch [38/443], Loss: 0.0948\n",
      "Epoch [3/10], Batch [39/443], Loss: 0.0579\n",
      "Epoch [3/10], Batch [40/443], Loss: 0.0526\n",
      "Epoch [3/10], Batch [41/443], Loss: 0.1467\n",
      "Epoch [3/10], Batch [42/443], Loss: 0.1189\n",
      "Epoch [3/10], Batch [43/443], Loss: 0.0534\n",
      "Epoch [3/10], Batch [44/443], Loss: 0.1281\n",
      "Epoch [3/10], Batch [45/443], Loss: 0.0176\n",
      "Epoch [3/10], Batch [46/443], Loss: 0.1020\n",
      "Epoch [3/10], Batch [47/443], Loss: 0.0500\n",
      "Epoch [3/10], Batch [48/443], Loss: 0.0811\n",
      "Epoch [3/10], Batch [49/443], Loss: 0.0720\n",
      "Epoch [3/10], Batch [50/443], Loss: 0.0406\n",
      "Epoch [3/10], Batch [51/443], Loss: 0.0593\n",
      "Epoch [3/10], Batch [52/443], Loss: 0.0838\n",
      "Epoch [3/10], Batch [53/443], Loss: 0.1562\n",
      "Epoch [3/10], Batch [54/443], Loss: 0.0910\n",
      "Epoch [3/10], Batch [55/443], Loss: 0.0847\n",
      "Epoch [3/10], Batch [56/443], Loss: 0.0426\n",
      "Epoch [3/10], Batch [57/443], Loss: 0.0635\n",
      "Epoch [3/10], Batch [58/443], Loss: 0.0280\n",
      "Epoch [3/10], Batch [59/443], Loss: 0.0651\n",
      "Epoch [3/10], Batch [60/443], Loss: 0.0800\n",
      "Epoch [3/10], Batch [61/443], Loss: 0.0422\n",
      "Epoch [3/10], Batch [62/443], Loss: 0.0421\n",
      "Epoch [3/10], Batch [63/443], Loss: 0.1101\n",
      "Epoch [3/10], Batch [64/443], Loss: 0.1921\n",
      "Epoch [3/10], Batch [65/443], Loss: 0.0570\n",
      "Epoch [3/10], Batch [66/443], Loss: 0.0508\n",
      "Epoch [3/10], Batch [67/443], Loss: 0.0596\n",
      "Epoch [3/10], Batch [68/443], Loss: 0.1439\n",
      "Epoch [3/10], Batch [69/443], Loss: 0.0288\n",
      "Epoch [3/10], Batch [70/443], Loss: 0.1224\n",
      "Epoch [3/10], Batch [71/443], Loss: 0.0606\n",
      "Epoch [3/10], Batch [72/443], Loss: 0.1055\n",
      "Epoch [3/10], Batch [73/443], Loss: 0.0613\n",
      "Epoch [3/10], Batch [74/443], Loss: 0.0414\n",
      "Epoch [3/10], Batch [75/443], Loss: 0.0374\n",
      "Epoch [3/10], Batch [76/443], Loss: 0.0327\n",
      "Epoch [3/10], Batch [77/443], Loss: 0.1065\n",
      "Epoch [3/10], Batch [78/443], Loss: 0.0447\n",
      "Epoch [3/10], Batch [79/443], Loss: 0.0584\n",
      "Epoch [3/10], Batch [80/443], Loss: 0.0608\n",
      "Epoch [3/10], Batch [81/443], Loss: 0.0771\n",
      "Epoch [3/10], Batch [82/443], Loss: 0.0717\n",
      "Epoch [3/10], Batch [83/443], Loss: 0.0198\n",
      "Epoch [3/10], Batch [84/443], Loss: 0.1508\n",
      "Epoch [3/10], Batch [85/443], Loss: 0.0856\n",
      "Epoch [3/10], Batch [86/443], Loss: 0.0372\n",
      "Epoch [3/10], Batch [87/443], Loss: 0.1538\n",
      "Epoch [3/10], Batch [88/443], Loss: 0.0894\n",
      "Epoch [3/10], Batch [89/443], Loss: 0.0973\n",
      "Epoch [3/10], Batch [90/443], Loss: 0.1082\n",
      "Epoch [3/10], Batch [91/443], Loss: 0.1940\n",
      "Epoch [3/10], Batch [92/443], Loss: 0.1001\n",
      "Epoch [3/10], Batch [93/443], Loss: 0.0453\n",
      "Epoch [3/10], Batch [94/443], Loss: 0.1828\n",
      "Epoch [3/10], Batch [95/443], Loss: 0.0520\n",
      "Epoch [3/10], Batch [96/443], Loss: 0.0507\n",
      "Epoch [3/10], Batch [97/443], Loss: 0.0732\n",
      "Epoch [3/10], Batch [98/443], Loss: 0.0677\n",
      "Epoch [3/10], Batch [99/443], Loss: 0.0208\n",
      "Epoch [3/10], Batch [100/443], Loss: 0.0504\n",
      "Epoch [3/10], Batch [101/443], Loss: 0.1035\n",
      "Epoch [3/10], Batch [102/443], Loss: 0.0798\n",
      "Epoch [3/10], Batch [103/443], Loss: 0.0799\n",
      "Epoch [3/10], Batch [104/443], Loss: 0.0651\n",
      "Epoch [3/10], Batch [105/443], Loss: 0.1250\n",
      "Epoch [3/10], Batch [106/443], Loss: 0.0496\n",
      "Epoch [3/10], Batch [107/443], Loss: 0.0753\n",
      "Epoch [3/10], Batch [108/443], Loss: 0.0511\n",
      "Epoch [3/10], Batch [109/443], Loss: 0.0986\n",
      "Epoch [3/10], Batch [110/443], Loss: 0.0476\n",
      "Epoch [3/10], Batch [111/443], Loss: 0.1366\n",
      "Epoch [3/10], Batch [112/443], Loss: 0.0227\n",
      "Epoch [3/10], Batch [113/443], Loss: 0.0811\n",
      "Epoch [3/10], Batch [114/443], Loss: 0.1430\n",
      "Epoch [3/10], Batch [115/443], Loss: 0.1250\n",
      "Epoch [3/10], Batch [116/443], Loss: 0.1107\n",
      "Epoch [3/10], Batch [117/443], Loss: 0.0359\n",
      "Epoch [3/10], Batch [118/443], Loss: 0.0636\n",
      "Epoch [3/10], Batch [119/443], Loss: 0.0357\n",
      "Epoch [3/10], Batch [120/443], Loss: 0.0747\n",
      "Epoch [3/10], Batch [121/443], Loss: 0.0292\n",
      "Epoch [3/10], Batch [122/443], Loss: 0.0737\n",
      "Epoch [3/10], Batch [123/443], Loss: 0.0273\n",
      "Epoch [3/10], Batch [124/443], Loss: 0.0612\n",
      "Epoch [3/10], Batch [125/443], Loss: 0.1339\n",
      "Epoch [3/10], Batch [126/443], Loss: 0.1267\n",
      "Epoch [3/10], Batch [127/443], Loss: 0.0563\n",
      "Epoch [3/10], Batch [128/443], Loss: 0.1017\n",
      "Epoch [3/10], Batch [129/443], Loss: 0.1086\n",
      "Epoch [3/10], Batch [130/443], Loss: 0.1061\n",
      "Epoch [3/10], Batch [131/443], Loss: 0.0513\n",
      "Epoch [3/10], Batch [132/443], Loss: 0.0444\n",
      "Epoch [3/10], Batch [133/443], Loss: 0.0579\n",
      "Epoch [3/10], Batch [134/443], Loss: 0.1582\n",
      "Epoch [3/10], Batch [135/443], Loss: 0.1497\n",
      "Epoch [3/10], Batch [136/443], Loss: 0.1862\n",
      "Epoch [3/10], Batch [137/443], Loss: 0.0665\n",
      "Epoch [3/10], Batch [138/443], Loss: 0.0437\n",
      "Epoch [3/10], Batch [139/443], Loss: 0.0643\n",
      "Epoch [3/10], Batch [140/443], Loss: 0.0656\n",
      "Epoch [3/10], Batch [141/443], Loss: 0.0954\n",
      "Epoch [3/10], Batch [142/443], Loss: 0.0497\n",
      "Epoch [3/10], Batch [143/443], Loss: 0.0326\n",
      "Epoch [3/10], Batch [144/443], Loss: 0.1213\n",
      "Epoch [3/10], Batch [145/443], Loss: 0.1177\n",
      "Epoch [3/10], Batch [146/443], Loss: 0.1128\n",
      "Epoch [3/10], Batch [147/443], Loss: 0.1165\n",
      "Epoch [3/10], Batch [148/443], Loss: 0.0645\n",
      "Epoch [3/10], Batch [149/443], Loss: 0.1473\n",
      "Epoch [3/10], Batch [150/443], Loss: 0.0239\n",
      "Epoch [3/10], Batch [151/443], Loss: 0.0848\n",
      "Epoch [3/10], Batch [152/443], Loss: 0.0936\n",
      "Epoch [3/10], Batch [153/443], Loss: 0.0521\n",
      "Epoch [3/10], Batch [154/443], Loss: 0.0317\n",
      "Epoch [3/10], Batch [155/443], Loss: 0.1118\n",
      "Epoch [3/10], Batch [156/443], Loss: 0.0934\n",
      "Epoch [3/10], Batch [157/443], Loss: 0.0359\n",
      "Epoch [3/10], Batch [158/443], Loss: 0.3154\n",
      "Epoch [3/10], Batch [159/443], Loss: 0.0661\n",
      "Epoch [3/10], Batch [160/443], Loss: 0.1153\n",
      "Epoch [3/10], Batch [161/443], Loss: 0.0459\n",
      "Epoch [3/10], Batch [162/443], Loss: 0.0821\n",
      "Epoch [3/10], Batch [163/443], Loss: 0.1072\n",
      "Epoch [3/10], Batch [164/443], Loss: 0.0416\n",
      "Epoch [3/10], Batch [165/443], Loss: 0.0997\n",
      "Epoch [3/10], Batch [166/443], Loss: 0.0319\n",
      "Epoch [3/10], Batch [167/443], Loss: 0.0549\n",
      "Epoch [3/10], Batch [168/443], Loss: 0.1186\n",
      "Epoch [3/10], Batch [169/443], Loss: 0.0530\n",
      "Epoch [3/10], Batch [170/443], Loss: 0.1508\n",
      "Epoch [3/10], Batch [171/443], Loss: 0.0778\n",
      "Epoch [3/10], Batch [172/443], Loss: 0.0859\n",
      "Epoch [3/10], Batch [173/443], Loss: 0.0686\n",
      "Epoch [3/10], Batch [174/443], Loss: 0.0686\n",
      "Epoch [3/10], Batch [175/443], Loss: 0.0654\n",
      "Epoch [3/10], Batch [176/443], Loss: 0.0829\n",
      "Epoch [3/10], Batch [177/443], Loss: 0.0505\n",
      "Epoch [3/10], Batch [178/443], Loss: 0.0761\n",
      "Epoch [3/10], Batch [179/443], Loss: 0.1060\n",
      "Epoch [3/10], Batch [180/443], Loss: 0.0768\n",
      "Epoch [3/10], Batch [181/443], Loss: 0.0460\n",
      "Epoch [3/10], Batch [182/443], Loss: 0.0377\n",
      "Epoch [3/10], Batch [183/443], Loss: 0.1055\n",
      "Epoch [3/10], Batch [184/443], Loss: 0.0981\n",
      "Epoch [3/10], Batch [185/443], Loss: 0.1051\n",
      "Epoch [3/10], Batch [186/443], Loss: 0.0266\n",
      "Epoch [3/10], Batch [187/443], Loss: 0.2093\n",
      "Epoch [3/10], Batch [188/443], Loss: 0.0885\n",
      "Epoch [3/10], Batch [189/443], Loss: 0.0772\n",
      "Epoch [3/10], Batch [190/443], Loss: 0.0230\n",
      "Epoch [3/10], Batch [191/443], Loss: 0.0778\n",
      "Epoch [3/10], Batch [192/443], Loss: 0.1199\n",
      "Epoch [3/10], Batch [193/443], Loss: 0.1314\n",
      "Epoch [3/10], Batch [194/443], Loss: 0.0653\n",
      "Epoch [3/10], Batch [195/443], Loss: 0.0635\n",
      "Epoch [3/10], Batch [196/443], Loss: 0.0680\n",
      "Epoch [3/10], Batch [197/443], Loss: 0.0736\n",
      "Epoch [3/10], Batch [198/443], Loss: 0.0729\n",
      "Epoch [3/10], Batch [199/443], Loss: 0.0309\n",
      "Epoch [3/10], Batch [200/443], Loss: 0.0302\n",
      "Epoch [3/10], Batch [201/443], Loss: 0.0524\n",
      "Epoch [3/10], Batch [202/443], Loss: 0.0345\n",
      "Epoch [3/10], Batch [203/443], Loss: 0.0319\n",
      "Epoch [3/10], Batch [204/443], Loss: 0.0818\n",
      "Epoch [3/10], Batch [205/443], Loss: 0.0766\n",
      "Epoch [3/10], Batch [206/443], Loss: 0.0397\n",
      "Epoch [3/10], Batch [207/443], Loss: 0.1907\n",
      "Epoch [3/10], Batch [208/443], Loss: 0.0760\n",
      "Epoch [3/10], Batch [209/443], Loss: 0.0673\n",
      "Epoch [3/10], Batch [210/443], Loss: 0.0901\n",
      "Epoch [3/10], Batch [211/443], Loss: 0.0335\n",
      "Epoch [3/10], Batch [212/443], Loss: 0.0931\n",
      "Epoch [3/10], Batch [213/443], Loss: 0.0690\n",
      "Epoch [3/10], Batch [214/443], Loss: 0.0340\n",
      "Epoch [3/10], Batch [215/443], Loss: 0.0717\n",
      "Epoch [3/10], Batch [216/443], Loss: 0.1370\n",
      "Epoch [3/10], Batch [217/443], Loss: 0.1032\n",
      "Epoch [3/10], Batch [218/443], Loss: 0.0960\n",
      "Epoch [3/10], Batch [219/443], Loss: 0.1848\n",
      "Epoch [3/10], Batch [220/443], Loss: 0.0692\n",
      "Epoch [3/10], Batch [221/443], Loss: 0.1547\n",
      "Epoch [3/10], Batch [222/443], Loss: 0.0505\n",
      "Epoch [3/10], Batch [223/443], Loss: 0.0913\n",
      "Epoch [3/10], Batch [224/443], Loss: 0.0639\n",
      "Epoch [3/10], Batch [225/443], Loss: 0.0749\n",
      "Epoch [3/10], Batch [226/443], Loss: 0.0534\n",
      "Epoch [3/10], Batch [227/443], Loss: 0.0456\n",
      "Epoch [3/10], Batch [228/443], Loss: 0.0450\n",
      "Epoch [3/10], Batch [229/443], Loss: 0.0280\n",
      "Epoch [3/10], Batch [230/443], Loss: 0.0296\n",
      "Epoch [3/10], Batch [231/443], Loss: 0.1025\n",
      "Epoch [3/10], Batch [232/443], Loss: 0.0276\n",
      "Epoch [3/10], Batch [233/443], Loss: 0.0512\n",
      "Epoch [3/10], Batch [234/443], Loss: 0.0609\n",
      "Epoch [3/10], Batch [235/443], Loss: 0.0685\n",
      "Epoch [3/10], Batch [236/443], Loss: 0.0745\n",
      "Epoch [3/10], Batch [237/443], Loss: 0.0546\n",
      "Epoch [3/10], Batch [238/443], Loss: 0.0360\n",
      "Epoch [3/10], Batch [239/443], Loss: 0.0722\n",
      "Epoch [3/10], Batch [240/443], Loss: 0.1862\n",
      "Epoch [3/10], Batch [241/443], Loss: 0.0666\n",
      "Epoch [3/10], Batch [242/443], Loss: 0.0528\n",
      "Epoch [3/10], Batch [243/443], Loss: 0.0769\n",
      "Epoch [3/10], Batch [244/443], Loss: 0.0286\n",
      "Epoch [3/10], Batch [245/443], Loss: 0.0291\n",
      "Epoch [3/10], Batch [246/443], Loss: 0.0656\n",
      "Epoch [3/10], Batch [247/443], Loss: 0.1349\n",
      "Epoch [3/10], Batch [248/443], Loss: 0.0446\n",
      "Epoch [3/10], Batch [249/443], Loss: 0.1309\n",
      "Epoch [3/10], Batch [250/443], Loss: 0.0827\n",
      "Epoch [3/10], Batch [251/443], Loss: 0.0924\n",
      "Epoch [3/10], Batch [252/443], Loss: 0.1142\n",
      "Epoch [3/10], Batch [253/443], Loss: 0.0600\n",
      "Epoch [3/10], Batch [254/443], Loss: 0.0755\n",
      "Epoch [3/10], Batch [255/443], Loss: 0.0553\n",
      "Epoch [3/10], Batch [256/443], Loss: 0.0706\n",
      "Epoch [3/10], Batch [257/443], Loss: 0.0703\n",
      "Epoch [3/10], Batch [258/443], Loss: 0.0312\n",
      "Epoch [3/10], Batch [259/443], Loss: 0.0488\n",
      "Epoch [3/10], Batch [260/443], Loss: 0.1725\n",
      "Epoch [3/10], Batch [261/443], Loss: 0.1302\n",
      "Epoch [3/10], Batch [262/443], Loss: 0.0390\n",
      "Epoch [3/10], Batch [263/443], Loss: 0.0613\n",
      "Epoch [3/10], Batch [264/443], Loss: 0.1169\n",
      "Epoch [3/10], Batch [265/443], Loss: 0.0808\n",
      "Epoch [3/10], Batch [266/443], Loss: 0.1123\n",
      "Epoch [3/10], Batch [267/443], Loss: 0.0349\n",
      "Epoch [3/10], Batch [268/443], Loss: 0.0266\n",
      "Epoch [3/10], Batch [269/443], Loss: 0.3792\n",
      "Epoch [3/10], Batch [270/443], Loss: 0.0402\n",
      "Epoch [3/10], Batch [271/443], Loss: 0.1544\n",
      "Epoch [3/10], Batch [272/443], Loss: 0.0474\n",
      "Epoch [3/10], Batch [273/443], Loss: 0.2025\n",
      "Epoch [3/10], Batch [274/443], Loss: 0.0945\n",
      "Epoch [3/10], Batch [275/443], Loss: 0.0601\n",
      "Epoch [3/10], Batch [276/443], Loss: 0.0721\n",
      "Epoch [3/10], Batch [277/443], Loss: 0.0543\n",
      "Epoch [3/10], Batch [278/443], Loss: 0.0359\n",
      "Epoch [3/10], Batch [279/443], Loss: 0.1173\n",
      "Epoch [3/10], Batch [280/443], Loss: 0.0527\n",
      "Epoch [3/10], Batch [281/443], Loss: 0.0245\n",
      "Epoch [3/10], Batch [282/443], Loss: 0.1173\n",
      "Epoch [3/10], Batch [283/443], Loss: 0.0621\n",
      "Epoch [3/10], Batch [284/443], Loss: 0.1529\n",
      "Epoch [3/10], Batch [285/443], Loss: 0.0485\n",
      "Epoch [3/10], Batch [286/443], Loss: 0.0494\n",
      "Epoch [3/10], Batch [287/443], Loss: 0.0291\n",
      "Epoch [3/10], Batch [288/443], Loss: 0.0668\n",
      "Epoch [3/10], Batch [289/443], Loss: 0.0808\n",
      "Epoch [3/10], Batch [290/443], Loss: 0.1877\n",
      "Epoch [3/10], Batch [291/443], Loss: 0.1511\n",
      "Epoch [3/10], Batch [292/443], Loss: 0.0405\n",
      "Epoch [3/10], Batch [293/443], Loss: 0.0332\n",
      "Epoch [3/10], Batch [294/443], Loss: 0.0506\n",
      "Epoch [3/10], Batch [295/443], Loss: 0.0581\n",
      "Epoch [3/10], Batch [296/443], Loss: 0.1460\n",
      "Epoch [3/10], Batch [297/443], Loss: 0.0471\n",
      "Epoch [3/10], Batch [298/443], Loss: 0.1025\n",
      "Epoch [3/10], Batch [299/443], Loss: 0.0271\n",
      "Epoch [3/10], Batch [300/443], Loss: 0.0834\n",
      "Epoch [3/10], Batch [301/443], Loss: 0.0923\n",
      "Epoch [3/10], Batch [302/443], Loss: 0.1915\n",
      "Epoch [3/10], Batch [303/443], Loss: 0.0558\n",
      "Epoch [3/10], Batch [304/443], Loss: 0.0726\n",
      "Epoch [3/10], Batch [305/443], Loss: 0.0484\n",
      "Epoch [3/10], Batch [306/443], Loss: 0.0962\n",
      "Epoch [3/10], Batch [307/443], Loss: 0.2398\n",
      "Epoch [3/10], Batch [308/443], Loss: 0.0264\n",
      "Epoch [3/10], Batch [309/443], Loss: 0.0962\n",
      "Epoch [3/10], Batch [310/443], Loss: 0.0585\n",
      "Epoch [3/10], Batch [311/443], Loss: 0.1076\n",
      "Epoch [3/10], Batch [312/443], Loss: 0.0292\n",
      "Epoch [3/10], Batch [313/443], Loss: 0.0418\n",
      "Epoch [3/10], Batch [314/443], Loss: 0.0782\n",
      "Epoch [3/10], Batch [315/443], Loss: 0.1006\n",
      "Epoch [3/10], Batch [316/443], Loss: 0.0685\n",
      "Epoch [3/10], Batch [317/443], Loss: 0.1118\n",
      "Epoch [3/10], Batch [318/443], Loss: 0.0472\n",
      "Epoch [3/10], Batch [319/443], Loss: 0.1329\n",
      "Epoch [3/10], Batch [320/443], Loss: 0.0480\n",
      "Epoch [3/10], Batch [321/443], Loss: 0.1195\n",
      "Epoch [3/10], Batch [322/443], Loss: 0.0949\n",
      "Epoch [3/10], Batch [323/443], Loss: 0.1701\n",
      "Epoch [3/10], Batch [324/443], Loss: 0.1068\n",
      "Epoch [3/10], Batch [325/443], Loss: 0.0416\n",
      "Epoch [3/10], Batch [326/443], Loss: 0.0951\n",
      "Epoch [3/10], Batch [327/443], Loss: 0.0605\n",
      "Epoch [3/10], Batch [328/443], Loss: 0.0734\n",
      "Epoch [3/10], Batch [329/443], Loss: 0.0588\n",
      "Epoch [3/10], Batch [330/443], Loss: 0.2732\n",
      "Epoch [3/10], Batch [331/443], Loss: 0.0389\n",
      "Epoch [3/10], Batch [332/443], Loss: 0.0579\n",
      "Epoch [3/10], Batch [333/443], Loss: 0.0875\n",
      "Epoch [3/10], Batch [334/443], Loss: 0.0633\n",
      "Epoch [3/10], Batch [335/443], Loss: 0.0356\n",
      "Epoch [3/10], Batch [336/443], Loss: 0.1131\n",
      "Epoch [3/10], Batch [337/443], Loss: 0.0689\n",
      "Epoch [3/10], Batch [338/443], Loss: 0.0419\n",
      "Epoch [3/10], Batch [339/443], Loss: 0.0250\n",
      "Epoch [3/10], Batch [340/443], Loss: 0.1173\n",
      "Epoch [3/10], Batch [341/443], Loss: 0.0897\n",
      "Epoch [3/10], Batch [342/443], Loss: 0.0390\n",
      "Epoch [3/10], Batch [343/443], Loss: 0.1500\n",
      "Epoch [3/10], Batch [344/443], Loss: 0.1347\n",
      "Epoch [3/10], Batch [345/443], Loss: 0.0703\n",
      "Epoch [3/10], Batch [346/443], Loss: 0.0582\n",
      "Epoch [3/10], Batch [347/443], Loss: 0.0515\n",
      "Epoch [3/10], Batch [348/443], Loss: 0.0527\n",
      "Epoch [3/10], Batch [349/443], Loss: 0.1205\n",
      "Epoch [3/10], Batch [350/443], Loss: 0.0372\n",
      "Epoch [3/10], Batch [351/443], Loss: 0.0415\n",
      "Epoch [3/10], Batch [352/443], Loss: 0.0814\n",
      "Epoch [3/10], Batch [353/443], Loss: 0.1009\n",
      "Epoch [3/10], Batch [354/443], Loss: 0.0799\n",
      "Epoch [3/10], Batch [355/443], Loss: 0.0967\n",
      "Epoch [3/10], Batch [356/443], Loss: 0.0632\n",
      "Epoch [3/10], Batch [357/443], Loss: 0.0628\n",
      "Epoch [3/10], Batch [358/443], Loss: 0.0297\n",
      "Epoch [3/10], Batch [359/443], Loss: 0.0907\n",
      "Epoch [3/10], Batch [360/443], Loss: 0.0606\n",
      "Epoch [3/10], Batch [361/443], Loss: 0.1172\n",
      "Epoch [3/10], Batch [362/443], Loss: 0.1064\n",
      "Epoch [3/10], Batch [363/443], Loss: 0.0320\n",
      "Epoch [3/10], Batch [364/443], Loss: 0.0754\n",
      "Epoch [3/10], Batch [365/443], Loss: 0.0263\n",
      "Epoch [3/10], Batch [366/443], Loss: 0.0976\n",
      "Epoch [3/10], Batch [367/443], Loss: 0.0701\n",
      "Epoch [3/10], Batch [368/443], Loss: 0.0559\n",
      "Epoch [3/10], Batch [369/443], Loss: 0.0803\n",
      "Epoch [3/10], Batch [370/443], Loss: 0.0698\n",
      "Epoch [3/10], Batch [371/443], Loss: 0.0666\n",
      "Epoch [3/10], Batch [372/443], Loss: 0.0265\n",
      "Epoch [3/10], Batch [373/443], Loss: 0.0937\n",
      "Epoch [3/10], Batch [374/443], Loss: 0.1383\n",
      "Epoch [3/10], Batch [375/443], Loss: 0.1914\n",
      "Epoch [3/10], Batch [376/443], Loss: 0.0822\n",
      "Epoch [3/10], Batch [377/443], Loss: 0.0519\n",
      "Epoch [3/10], Batch [378/443], Loss: 0.0689\n",
      "Epoch [3/10], Batch [379/443], Loss: 0.0922\n",
      "Epoch [3/10], Batch [380/443], Loss: 0.0870\n",
      "Epoch [3/10], Batch [381/443], Loss: 0.0333\n",
      "Epoch [3/10], Batch [382/443], Loss: 0.0578\n",
      "Epoch [3/10], Batch [383/443], Loss: 0.0340\n",
      "Epoch [3/10], Batch [384/443], Loss: 0.0171\n",
      "Epoch [3/10], Batch [385/443], Loss: 0.0604\n",
      "Epoch [3/10], Batch [386/443], Loss: 0.0587\n",
      "Epoch [3/10], Batch [387/443], Loss: 0.0556\n",
      "Epoch [3/10], Batch [388/443], Loss: 0.0401\n",
      "Epoch [3/10], Batch [389/443], Loss: 0.0381\n",
      "Epoch [3/10], Batch [390/443], Loss: 0.0364\n",
      "Epoch [3/10], Batch [391/443], Loss: 0.0655\n",
      "Epoch [3/10], Batch [392/443], Loss: 0.1367\n",
      "Epoch [3/10], Batch [393/443], Loss: 0.1468\n",
      "Epoch [3/10], Batch [394/443], Loss: 0.0642\n",
      "Epoch [3/10], Batch [395/443], Loss: 0.0574\n",
      "Epoch [3/10], Batch [396/443], Loss: 0.0518\n",
      "Epoch [3/10], Batch [397/443], Loss: 0.0582\n",
      "Epoch [3/10], Batch [398/443], Loss: 0.1913\n",
      "Epoch [3/10], Batch [399/443], Loss: 0.0366\n",
      "Epoch [3/10], Batch [400/443], Loss: 0.0685\n",
      "Epoch [3/10], Batch [401/443], Loss: 0.1342\n",
      "Epoch [3/10], Batch [402/443], Loss: 0.0476\n",
      "Epoch [3/10], Batch [403/443], Loss: 0.1242\n",
      "Epoch [3/10], Batch [404/443], Loss: 0.1381\n",
      "Epoch [3/10], Batch [405/443], Loss: 0.1799\n",
      "Epoch [3/10], Batch [406/443], Loss: 0.0282\n",
      "Epoch [3/10], Batch [407/443], Loss: 0.0815\n",
      "Epoch [3/10], Batch [408/443], Loss: 0.2666\n",
      "Epoch [3/10], Batch [409/443], Loss: 0.1789\n",
      "Epoch [3/10], Batch [410/443], Loss: 0.0618\n",
      "Epoch [3/10], Batch [411/443], Loss: 0.0909\n",
      "Epoch [3/10], Batch [412/443], Loss: 0.1335\n",
      "Epoch [3/10], Batch [413/443], Loss: 0.0637\n",
      "Epoch [3/10], Batch [414/443], Loss: 0.0507\n",
      "Epoch [3/10], Batch [415/443], Loss: 0.0921\n",
      "Epoch [3/10], Batch [416/443], Loss: 0.0524\n",
      "Epoch [3/10], Batch [417/443], Loss: 0.1344\n",
      "Epoch [3/10], Batch [418/443], Loss: 0.0497\n",
      "Epoch [3/10], Batch [419/443], Loss: 0.0458\n",
      "Epoch [3/10], Batch [420/443], Loss: 0.1106\n",
      "Epoch [3/10], Batch [421/443], Loss: 0.0562\n",
      "Epoch [3/10], Batch [422/443], Loss: 0.0602\n",
      "Epoch [3/10], Batch [423/443], Loss: 0.1876\n",
      "Epoch [3/10], Batch [424/443], Loss: 0.0595\n",
      "Epoch [3/10], Batch [425/443], Loss: 0.1063\n",
      "Epoch [3/10], Batch [426/443], Loss: 0.0650\n",
      "Epoch [3/10], Batch [427/443], Loss: 0.1390\n",
      "Epoch [3/10], Batch [428/443], Loss: 0.0885\n",
      "Epoch [3/10], Batch [429/443], Loss: 0.0545\n",
      "Epoch [3/10], Batch [430/443], Loss: 0.0635\n",
      "Epoch [3/10], Batch [431/443], Loss: 0.0890\n",
      "Epoch [3/10], Batch [432/443], Loss: 0.0374\n",
      "Epoch [3/10], Batch [433/443], Loss: 0.0934\n",
      "Epoch [3/10], Batch [434/443], Loss: 0.1080\n",
      "Epoch [3/10], Batch [435/443], Loss: 0.0359\n",
      "Epoch [3/10], Batch [436/443], Loss: 0.1288\n",
      "Epoch [3/10], Batch [437/443], Loss: 0.0550\n",
      "Epoch [3/10], Batch [438/443], Loss: 0.0476\n",
      "Epoch [3/10], Batch [439/443], Loss: 0.0186\n",
      "Epoch [3/10], Batch [440/443], Loss: 0.1888\n",
      "Epoch [3/10], Batch [441/443], Loss: 0.0635\n",
      "Epoch [3/10], Batch [442/443], Loss: 0.0359\n",
      "Epoch [3/10], Batch [443/443], Loss: 0.0879\n",
      "Epoch [4/10], Average Loss: 0.0817\n",
      "Model saved at ./saved_models/model_epoch_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2228\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Batch [1/443], Loss: 0.0501\n",
      "Epoch [4/10], Batch [2/443], Loss: 0.0530\n",
      "Epoch [4/10], Batch [3/443], Loss: 0.0407\n",
      "Epoch [4/10], Batch [4/443], Loss: 0.1006\n",
      "Epoch [4/10], Batch [5/443], Loss: 0.0271\n",
      "Epoch [4/10], Batch [6/443], Loss: 0.1185\n",
      "Epoch [4/10], Batch [7/443], Loss: 0.0636\n",
      "Epoch [4/10], Batch [8/443], Loss: 0.0375\n",
      "Epoch [4/10], Batch [9/443], Loss: 0.0634\n",
      "Epoch [4/10], Batch [10/443], Loss: 0.1434\n",
      "Epoch [4/10], Batch [11/443], Loss: 0.0770\n",
      "Epoch [4/10], Batch [12/443], Loss: 0.0413\n",
      "Epoch [4/10], Batch [13/443], Loss: 0.0367\n",
      "Epoch [4/10], Batch [14/443], Loss: 0.0391\n",
      "Epoch [4/10], Batch [15/443], Loss: 0.1073\n",
      "Epoch [4/10], Batch [16/443], Loss: 0.0799\n",
      "Epoch [4/10], Batch [17/443], Loss: 0.1280\n",
      "Epoch [4/10], Batch [18/443], Loss: 0.0864\n",
      "Epoch [4/10], Batch [19/443], Loss: 0.0791\n",
      "Epoch [4/10], Batch [20/443], Loss: 0.1235\n",
      "Epoch [4/10], Batch [21/443], Loss: 0.0864\n",
      "Epoch [4/10], Batch [22/443], Loss: 0.0457\n",
      "Epoch [4/10], Batch [23/443], Loss: 0.0653\n",
      "Epoch [4/10], Batch [24/443], Loss: 0.0608\n",
      "Epoch [4/10], Batch [25/443], Loss: 0.0295\n",
      "Epoch [4/10], Batch [26/443], Loss: 0.0492\n",
      "Epoch [4/10], Batch [27/443], Loss: 0.0390\n",
      "Epoch [4/10], Batch [28/443], Loss: 0.0856\n",
      "Epoch [4/10], Batch [29/443], Loss: 0.1425\n",
      "Epoch [4/10], Batch [30/443], Loss: 0.1002\n",
      "Epoch [4/10], Batch [31/443], Loss: 0.1038\n",
      "Epoch [4/10], Batch [32/443], Loss: 0.1588\n",
      "Epoch [4/10], Batch [33/443], Loss: 0.0849\n",
      "Epoch [4/10], Batch [34/443], Loss: 0.0508\n",
      "Epoch [4/10], Batch [35/443], Loss: 0.0446\n",
      "Epoch [4/10], Batch [36/443], Loss: 0.0354\n",
      "Epoch [4/10], Batch [37/443], Loss: 0.1066\n",
      "Epoch [4/10], Batch [38/443], Loss: 0.0528\n",
      "Epoch [4/10], Batch [39/443], Loss: 0.0428\n",
      "Epoch [4/10], Batch [40/443], Loss: 0.2360\n",
      "Epoch [4/10], Batch [41/443], Loss: 0.1833\n",
      "Epoch [4/10], Batch [42/443], Loss: 0.0211\n",
      "Epoch [4/10], Batch [43/443], Loss: 0.1224\n",
      "Epoch [4/10], Batch [44/443], Loss: 0.1808\n",
      "Epoch [4/10], Batch [45/443], Loss: 0.0966\n",
      "Epoch [4/10], Batch [46/443], Loss: 0.1642\n",
      "Epoch [4/10], Batch [47/443], Loss: 0.0745\n",
      "Epoch [4/10], Batch [48/443], Loss: 0.0942\n",
      "Epoch [4/10], Batch [49/443], Loss: 0.1366\n",
      "Epoch [4/10], Batch [50/443], Loss: 0.0461\n",
      "Epoch [4/10], Batch [51/443], Loss: 0.1657\n",
      "Epoch [4/10], Batch [52/443], Loss: 0.1656\n",
      "Epoch [4/10], Batch [53/443], Loss: 0.1334\n",
      "Epoch [4/10], Batch [54/443], Loss: 0.0857\n",
      "Epoch [4/10], Batch [55/443], Loss: 0.1083\n",
      "Epoch [4/10], Batch [56/443], Loss: 0.0237\n",
      "Epoch [4/10], Batch [57/443], Loss: 0.1239\n",
      "Epoch [4/10], Batch [58/443], Loss: 0.0950\n",
      "Epoch [4/10], Batch [59/443], Loss: 0.0595\n",
      "Epoch [4/10], Batch [60/443], Loss: 0.0984\n",
      "Epoch [4/10], Batch [61/443], Loss: 0.0295\n",
      "Epoch [4/10], Batch [62/443], Loss: 0.2216\n",
      "Epoch [4/10], Batch [63/443], Loss: 0.0458\n",
      "Epoch [4/10], Batch [64/443], Loss: 0.1334\n",
      "Epoch [4/10], Batch [65/443], Loss: 0.0968\n",
      "Epoch [4/10], Batch [66/443], Loss: 0.0755\n",
      "Epoch [4/10], Batch [67/443], Loss: 0.0784\n",
      "Epoch [4/10], Batch [68/443], Loss: 0.1937\n",
      "Epoch [4/10], Batch [69/443], Loss: 0.0554\n",
      "Epoch [4/10], Batch [70/443], Loss: 0.0857\n",
      "Epoch [4/10], Batch [71/443], Loss: 0.1117\n",
      "Epoch [4/10], Batch [72/443], Loss: 0.0282\n",
      "Epoch [4/10], Batch [73/443], Loss: 0.1033\n",
      "Epoch [4/10], Batch [74/443], Loss: 0.1370\n",
      "Epoch [4/10], Batch [75/443], Loss: 0.1242\n",
      "Epoch [4/10], Batch [76/443], Loss: 0.0816\n",
      "Epoch [4/10], Batch [77/443], Loss: 0.0728\n",
      "Epoch [4/10], Batch [78/443], Loss: 0.0766\n",
      "Epoch [4/10], Batch [79/443], Loss: 0.0815\n",
      "Epoch [4/10], Batch [80/443], Loss: 0.0388\n",
      "Epoch [4/10], Batch [81/443], Loss: 0.0632\n",
      "Epoch [4/10], Batch [82/443], Loss: 0.0740\n",
      "Epoch [4/10], Batch [83/443], Loss: 0.0231\n",
      "Epoch [4/10], Batch [84/443], Loss: 0.0676\n",
      "Epoch [4/10], Batch [85/443], Loss: 0.1224\n",
      "Epoch [4/10], Batch [86/443], Loss: 0.0630\n",
      "Epoch [4/10], Batch [87/443], Loss: 0.0714\n",
      "Epoch [4/10], Batch [88/443], Loss: 0.2473\n",
      "Epoch [4/10], Batch [89/443], Loss: 0.2396\n",
      "Epoch [4/10], Batch [90/443], Loss: 0.1146\n",
      "Epoch [4/10], Batch [91/443], Loss: 0.1176\n",
      "Epoch [4/10], Batch [92/443], Loss: 0.0364\n",
      "Epoch [4/10], Batch [93/443], Loss: 0.0379\n",
      "Epoch [4/10], Batch [94/443], Loss: 0.0445\n",
      "Epoch [4/10], Batch [95/443], Loss: 0.0541\n",
      "Epoch [4/10], Batch [96/443], Loss: 0.0629\n",
      "Epoch [4/10], Batch [97/443], Loss: 0.1938\n",
      "Epoch [4/10], Batch [98/443], Loss: 0.0725\n",
      "Epoch [4/10], Batch [99/443], Loss: 0.0237\n",
      "Epoch [4/10], Batch [100/443], Loss: 0.0517\n",
      "Epoch [4/10], Batch [101/443], Loss: 0.0874\n",
      "Epoch [4/10], Batch [102/443], Loss: 0.1628\n",
      "Epoch [4/10], Batch [103/443], Loss: 0.0712\n",
      "Epoch [4/10], Batch [104/443], Loss: 0.0305\n",
      "Epoch [4/10], Batch [105/443], Loss: 0.0655\n",
      "Epoch [4/10], Batch [106/443], Loss: 0.0611\n",
      "Epoch [4/10], Batch [107/443], Loss: 0.1202\n",
      "Epoch [4/10], Batch [108/443], Loss: 0.0642\n",
      "Epoch [4/10], Batch [109/443], Loss: 0.1768\n",
      "Epoch [4/10], Batch [110/443], Loss: 0.0355\n",
      "Epoch [4/10], Batch [111/443], Loss: 0.0437\n",
      "Epoch [4/10], Batch [112/443], Loss: 0.0860\n",
      "Epoch [4/10], Batch [113/443], Loss: 0.0415\n",
      "Epoch [4/10], Batch [114/443], Loss: 0.0480\n",
      "Epoch [4/10], Batch [115/443], Loss: 0.0657\n",
      "Epoch [4/10], Batch [116/443], Loss: 0.1686\n",
      "Epoch [4/10], Batch [117/443], Loss: 0.1965\n",
      "Epoch [4/10], Batch [118/443], Loss: 0.1042\n",
      "Epoch [4/10], Batch [119/443], Loss: 0.0717\n",
      "Epoch [4/10], Batch [120/443], Loss: 0.0491\n",
      "Epoch [4/10], Batch [121/443], Loss: 0.1020\n",
      "Epoch [4/10], Batch [122/443], Loss: 0.0573\n",
      "Epoch [4/10], Batch [123/443], Loss: 0.0531\n",
      "Epoch [4/10], Batch [124/443], Loss: 0.0616\n",
      "Epoch [4/10], Batch [125/443], Loss: 0.1582\n",
      "Epoch [4/10], Batch [126/443], Loss: 0.0657\n",
      "Epoch [4/10], Batch [127/443], Loss: 0.1045\n",
      "Epoch [4/10], Batch [128/443], Loss: 0.0794\n",
      "Epoch [4/10], Batch [129/443], Loss: 0.0709\n",
      "Epoch [4/10], Batch [130/443], Loss: 0.1940\n",
      "Epoch [4/10], Batch [131/443], Loss: 0.0482\n",
      "Epoch [4/10], Batch [132/443], Loss: 0.0340\n",
      "Epoch [4/10], Batch [133/443], Loss: 0.1340\n",
      "Epoch [4/10], Batch [134/443], Loss: 0.0392\n",
      "Epoch [4/10], Batch [135/443], Loss: 0.0534\n",
      "Epoch [4/10], Batch [136/443], Loss: 0.1596\n",
      "Epoch [4/10], Batch [137/443], Loss: 0.0362\n",
      "Epoch [4/10], Batch [138/443], Loss: 0.0420\n",
      "Epoch [4/10], Batch [139/443], Loss: 0.1363\n",
      "Epoch [4/10], Batch [140/443], Loss: 0.0416\n",
      "Epoch [4/10], Batch [141/443], Loss: 0.0704\n",
      "Epoch [4/10], Batch [142/443], Loss: 0.0646\n",
      "Epoch [4/10], Batch [143/443], Loss: 0.1098\n",
      "Epoch [4/10], Batch [144/443], Loss: 0.0873\n",
      "Epoch [4/10], Batch [145/443], Loss: 0.1101\n",
      "Epoch [4/10], Batch [146/443], Loss: 0.0822\n",
      "Epoch [4/10], Batch [147/443], Loss: 0.0263\n",
      "Epoch [4/10], Batch [148/443], Loss: 0.0225\n",
      "Epoch [4/10], Batch [149/443], Loss: 0.0871\n",
      "Epoch [4/10], Batch [150/443], Loss: 0.0589\n",
      "Epoch [4/10], Batch [151/443], Loss: 0.0318\n",
      "Epoch [4/10], Batch [152/443], Loss: 0.0371\n",
      "Epoch [4/10], Batch [153/443], Loss: 0.0640\n",
      "Epoch [4/10], Batch [154/443], Loss: 0.0751\n",
      "Epoch [4/10], Batch [155/443], Loss: 0.0528\n",
      "Epoch [4/10], Batch [156/443], Loss: 0.1649\n",
      "Epoch [4/10], Batch [157/443], Loss: 0.0797\n",
      "Epoch [4/10], Batch [158/443], Loss: 0.0339\n",
      "Epoch [4/10], Batch [159/443], Loss: 0.0988\n",
      "Epoch [4/10], Batch [160/443], Loss: 0.1363\n",
      "Epoch [4/10], Batch [161/443], Loss: 0.0650\n",
      "Epoch [4/10], Batch [162/443], Loss: 0.0547\n",
      "Epoch [4/10], Batch [163/443], Loss: 0.0652\n",
      "Epoch [4/10], Batch [164/443], Loss: 0.1169\n",
      "Epoch [4/10], Batch [165/443], Loss: 0.0757\n",
      "Epoch [4/10], Batch [166/443], Loss: 0.0511\n",
      "Epoch [4/10], Batch [167/443], Loss: 0.0729\n",
      "Epoch [4/10], Batch [168/443], Loss: 0.1205\n",
      "Epoch [4/10], Batch [169/443], Loss: 0.0544\n",
      "Epoch [4/10], Batch [170/443], Loss: 0.0269\n",
      "Epoch [4/10], Batch [171/443], Loss: 0.1170\n",
      "Epoch [4/10], Batch [172/443], Loss: 0.0760\n",
      "Epoch [4/10], Batch [173/443], Loss: 0.0539\n",
      "Epoch [4/10], Batch [174/443], Loss: 0.1178\n",
      "Epoch [4/10], Batch [175/443], Loss: 0.0745\n",
      "Epoch [4/10], Batch [176/443], Loss: 0.0289\n",
      "Epoch [4/10], Batch [177/443], Loss: 0.2436\n",
      "Epoch [4/10], Batch [178/443], Loss: 0.0455\n",
      "Epoch [4/10], Batch [179/443], Loss: 0.1070\n",
      "Epoch [4/10], Batch [180/443], Loss: 0.1515\n",
      "Epoch [4/10], Batch [181/443], Loss: 0.0449\n",
      "Epoch [4/10], Batch [182/443], Loss: 0.0611\n",
      "Epoch [4/10], Batch [183/443], Loss: 0.0573\n",
      "Epoch [4/10], Batch [184/443], Loss: 0.0840\n",
      "Epoch [4/10], Batch [185/443], Loss: 0.0944\n",
      "Epoch [4/10], Batch [186/443], Loss: 0.1617\n",
      "Epoch [4/10], Batch [187/443], Loss: 0.0602\n",
      "Epoch [4/10], Batch [188/443], Loss: 0.0983\n",
      "Epoch [4/10], Batch [189/443], Loss: 0.0655\n",
      "Epoch [4/10], Batch [190/443], Loss: 0.0786\n",
      "Epoch [4/10], Batch [191/443], Loss: 0.1144\n",
      "Epoch [4/10], Batch [192/443], Loss: 0.0291\n",
      "Epoch [4/10], Batch [193/443], Loss: 0.0730\n",
      "Epoch [4/10], Batch [194/443], Loss: 0.0879\n",
      "Epoch [4/10], Batch [195/443], Loss: 0.1228\n",
      "Epoch [4/10], Batch [196/443], Loss: 0.0814\n",
      "Epoch [4/10], Batch [197/443], Loss: 0.0351\n",
      "Epoch [4/10], Batch [198/443], Loss: 0.0366\n",
      "Epoch [4/10], Batch [199/443], Loss: 0.0933\n",
      "Epoch [4/10], Batch [200/443], Loss: 0.1035\n",
      "Epoch [4/10], Batch [201/443], Loss: 0.0612\n",
      "Epoch [4/10], Batch [202/443], Loss: 0.0367\n",
      "Epoch [4/10], Batch [203/443], Loss: 0.0894\n",
      "Epoch [4/10], Batch [204/443], Loss: 0.0577\n",
      "Epoch [4/10], Batch [205/443], Loss: 0.0254\n",
      "Epoch [4/10], Batch [206/443], Loss: 0.0391\n",
      "Epoch [4/10], Batch [207/443], Loss: 0.0393\n",
      "Epoch [4/10], Batch [208/443], Loss: 0.1649\n",
      "Epoch [4/10], Batch [209/443], Loss: 0.0970\n",
      "Epoch [4/10], Batch [210/443], Loss: 0.1027\n",
      "Epoch [4/10], Batch [211/443], Loss: 0.1738\n",
      "Epoch [4/10], Batch [212/443], Loss: 0.0589\n",
      "Epoch [4/10], Batch [213/443], Loss: 0.0782\n",
      "Epoch [4/10], Batch [214/443], Loss: 0.0561\n",
      "Epoch [4/10], Batch [215/443], Loss: 0.0663\n",
      "Epoch [4/10], Batch [216/443], Loss: 0.1836\n",
      "Epoch [4/10], Batch [217/443], Loss: 0.0654\n",
      "Epoch [4/10], Batch [218/443], Loss: 0.0403\n",
      "Epoch [4/10], Batch [219/443], Loss: 0.1487\n",
      "Epoch [4/10], Batch [220/443], Loss: 0.0341\n",
      "Epoch [4/10], Batch [221/443], Loss: 0.1437\n",
      "Epoch [4/10], Batch [222/443], Loss: 0.1158\n",
      "Epoch [4/10], Batch [223/443], Loss: 0.0305\n",
      "Epoch [4/10], Batch [224/443], Loss: 0.0576\n",
      "Epoch [4/10], Batch [225/443], Loss: 0.1417\n",
      "Epoch [4/10], Batch [226/443], Loss: 0.0789\n",
      "Epoch [4/10], Batch [227/443], Loss: 0.0848\n",
      "Epoch [4/10], Batch [228/443], Loss: 0.0566\n",
      "Epoch [4/10], Batch [229/443], Loss: 0.0492\n",
      "Epoch [4/10], Batch [230/443], Loss: 0.0505\n",
      "Epoch [4/10], Batch [231/443], Loss: 0.1151\n",
      "Epoch [4/10], Batch [232/443], Loss: 0.1063\n",
      "Epoch [4/10], Batch [233/443], Loss: 0.1737\n",
      "Epoch [4/10], Batch [234/443], Loss: 0.0394\n",
      "Epoch [4/10], Batch [235/443], Loss: 0.0495\n",
      "Epoch [4/10], Batch [236/443], Loss: 0.0449\n",
      "Epoch [4/10], Batch [237/443], Loss: 0.0856\n",
      "Epoch [4/10], Batch [238/443], Loss: 0.1555\n",
      "Epoch [4/10], Batch [239/443], Loss: 0.0849\n",
      "Epoch [4/10], Batch [240/443], Loss: 0.0335\n",
      "Epoch [4/10], Batch [241/443], Loss: 0.0855\n",
      "Epoch [4/10], Batch [242/443], Loss: 0.0509\n",
      "Epoch [4/10], Batch [243/443], Loss: 0.1305\n",
      "Epoch [4/10], Batch [244/443], Loss: 0.0391\n",
      "Epoch [4/10], Batch [245/443], Loss: 0.1016\n",
      "Epoch [4/10], Batch [246/443], Loss: 0.1236\n",
      "Epoch [4/10], Batch [247/443], Loss: 0.0242\n",
      "Epoch [4/10], Batch [248/443], Loss: 0.0287\n",
      "Epoch [4/10], Batch [249/443], Loss: 0.0998\n",
      "Epoch [4/10], Batch [250/443], Loss: 0.0444\n",
      "Epoch [4/10], Batch [251/443], Loss: 0.0612\n",
      "Epoch [4/10], Batch [252/443], Loss: 0.0893\n",
      "Epoch [4/10], Batch [253/443], Loss: 0.0682\n",
      "Epoch [4/10], Batch [254/443], Loss: 0.0364\n",
      "Epoch [4/10], Batch [255/443], Loss: 0.0835\n",
      "Epoch [4/10], Batch [256/443], Loss: 0.1355\n",
      "Epoch [4/10], Batch [257/443], Loss: 0.1136\n",
      "Epoch [4/10], Batch [258/443], Loss: 0.0884\n",
      "Epoch [4/10], Batch [259/443], Loss: 0.0499\n",
      "Epoch [4/10], Batch [260/443], Loss: 0.0973\n",
      "Epoch [4/10], Batch [261/443], Loss: 0.0593\n",
      "Epoch [4/10], Batch [262/443], Loss: 0.0369\n",
      "Epoch [4/10], Batch [263/443], Loss: 0.0510\n",
      "Epoch [4/10], Batch [264/443], Loss: 0.2277\n",
      "Epoch [4/10], Batch [265/443], Loss: 0.0491\n",
      "Epoch [4/10], Batch [266/443], Loss: 0.0590\n",
      "Epoch [4/10], Batch [267/443], Loss: 0.0808\n",
      "Epoch [4/10], Batch [268/443], Loss: 0.0909\n",
      "Epoch [4/10], Batch [269/443], Loss: 0.0646\n",
      "Epoch [4/10], Batch [270/443], Loss: 0.0540\n",
      "Epoch [4/10], Batch [271/443], Loss: 0.0347\n",
      "Epoch [4/10], Batch [272/443], Loss: 0.1300\n",
      "Epoch [4/10], Batch [273/443], Loss: 0.0927\n",
      "Epoch [4/10], Batch [274/443], Loss: 0.0392\n",
      "Epoch [4/10], Batch [275/443], Loss: 0.0740\n",
      "Epoch [4/10], Batch [276/443], Loss: 0.1339\n",
      "Epoch [4/10], Batch [277/443], Loss: 0.0418\n",
      "Epoch [4/10], Batch [278/443], Loss: 0.0276\n",
      "Epoch [4/10], Batch [279/443], Loss: 0.0326\n",
      "Epoch [4/10], Batch [280/443], Loss: 0.0286\n",
      "Epoch [4/10], Batch [281/443], Loss: 0.1200\n",
      "Epoch [4/10], Batch [282/443], Loss: 0.1149\n",
      "Epoch [4/10], Batch [283/443], Loss: 0.0492\n",
      "Epoch [4/10], Batch [284/443], Loss: 0.0942\n",
      "Epoch [4/10], Batch [285/443], Loss: 0.0170\n",
      "Epoch [4/10], Batch [286/443], Loss: 0.0751\n",
      "Epoch [4/10], Batch [287/443], Loss: 0.1285\n",
      "Epoch [4/10], Batch [288/443], Loss: 0.0737\n",
      "Epoch [4/10], Batch [289/443], Loss: 0.1538\n",
      "Epoch [4/10], Batch [290/443], Loss: 0.0899\n",
      "Epoch [4/10], Batch [291/443], Loss: 0.0460\n",
      "Epoch [4/10], Batch [292/443], Loss: 0.0659\n",
      "Epoch [4/10], Batch [293/443], Loss: 0.0809\n",
      "Epoch [4/10], Batch [294/443], Loss: 0.0863\n",
      "Epoch [4/10], Batch [295/443], Loss: 0.0163\n",
      "Epoch [4/10], Batch [296/443], Loss: 0.0377\n",
      "Epoch [4/10], Batch [297/443], Loss: 0.0956\n",
      "Epoch [4/10], Batch [298/443], Loss: 0.0549\n",
      "Epoch [4/10], Batch [299/443], Loss: 0.1343\n",
      "Epoch [4/10], Batch [300/443], Loss: 0.1421\n",
      "Epoch [4/10], Batch [301/443], Loss: 0.0365\n",
      "Epoch [4/10], Batch [302/443], Loss: 0.0474\n",
      "Epoch [4/10], Batch [303/443], Loss: 0.0975\n",
      "Epoch [4/10], Batch [304/443], Loss: 0.1230\n",
      "Epoch [4/10], Batch [305/443], Loss: 0.1248\n",
      "Epoch [4/10], Batch [306/443], Loss: 0.0530\n",
      "Epoch [4/10], Batch [307/443], Loss: 0.1025\n",
      "Epoch [4/10], Batch [308/443], Loss: 0.0538\n",
      "Epoch [4/10], Batch [309/443], Loss: 0.0788\n",
      "Epoch [4/10], Batch [310/443], Loss: 0.0378\n",
      "Epoch [4/10], Batch [311/443], Loss: 0.0307\n",
      "Epoch [4/10], Batch [312/443], Loss: 0.0622\n",
      "Epoch [4/10], Batch [313/443], Loss: 0.1101\n",
      "Epoch [4/10], Batch [314/443], Loss: 0.0261\n",
      "Epoch [4/10], Batch [315/443], Loss: 0.0199\n",
      "Epoch [4/10], Batch [316/443], Loss: 0.1520\n",
      "Epoch [4/10], Batch [317/443], Loss: 0.1490\n",
      "Epoch [4/10], Batch [318/443], Loss: 0.1142\n",
      "Epoch [4/10], Batch [319/443], Loss: 0.0413\n",
      "Epoch [4/10], Batch [320/443], Loss: 0.0359\n",
      "Epoch [4/10], Batch [321/443], Loss: 0.0714\n",
      "Epoch [4/10], Batch [322/443], Loss: 0.0590\n",
      "Epoch [4/10], Batch [323/443], Loss: 0.0489\n",
      "Epoch [4/10], Batch [324/443], Loss: 0.1284\n",
      "Epoch [4/10], Batch [325/443], Loss: 0.1198\n",
      "Epoch [4/10], Batch [326/443], Loss: 0.0537\n",
      "Epoch [4/10], Batch [327/443], Loss: 0.1461\n",
      "Epoch [4/10], Batch [328/443], Loss: 0.0629\n",
      "Epoch [4/10], Batch [329/443], Loss: 0.0727\n",
      "Epoch [4/10], Batch [330/443], Loss: 0.0357\n",
      "Epoch [4/10], Batch [331/443], Loss: 0.0396\n",
      "Epoch [4/10], Batch [332/443], Loss: 0.0900\n",
      "Epoch [4/10], Batch [333/443], Loss: 0.0435\n",
      "Epoch [4/10], Batch [334/443], Loss: 0.0671\n",
      "Epoch [4/10], Batch [335/443], Loss: 0.0374\n",
      "Epoch [4/10], Batch [336/443], Loss: 0.2191\n",
      "Epoch [4/10], Batch [337/443], Loss: 0.0174\n",
      "Epoch [4/10], Batch [338/443], Loss: 0.0339\n",
      "Epoch [4/10], Batch [339/443], Loss: 0.0464\n",
      "Epoch [4/10], Batch [340/443], Loss: 0.1570\n",
      "Epoch [4/10], Batch [341/443], Loss: 0.0815\n",
      "Epoch [4/10], Batch [342/443], Loss: 0.0668\n",
      "Epoch [4/10], Batch [343/443], Loss: 0.0806\n",
      "Epoch [4/10], Batch [344/443], Loss: 0.2199\n",
      "Epoch [4/10], Batch [345/443], Loss: 0.0457\n",
      "Epoch [4/10], Batch [346/443], Loss: 0.0541\n",
      "Epoch [4/10], Batch [347/443], Loss: 0.1040\n",
      "Epoch [4/10], Batch [348/443], Loss: 0.1359\n",
      "Epoch [4/10], Batch [349/443], Loss: 0.1058\n",
      "Epoch [4/10], Batch [350/443], Loss: 0.0638\n",
      "Epoch [4/10], Batch [351/443], Loss: 0.0674\n",
      "Epoch [4/10], Batch [352/443], Loss: 0.0872\n",
      "Epoch [4/10], Batch [353/443], Loss: 0.0816\n",
      "Epoch [4/10], Batch [354/443], Loss: 0.0913\n",
      "Epoch [4/10], Batch [355/443], Loss: 0.0491\n",
      "Epoch [4/10], Batch [356/443], Loss: 0.0578\n",
      "Epoch [4/10], Batch [357/443], Loss: 0.1446\n",
      "Epoch [4/10], Batch [358/443], Loss: 0.0545\n",
      "Epoch [4/10], Batch [359/443], Loss: 0.0849\n",
      "Epoch [4/10], Batch [360/443], Loss: 0.0890\n",
      "Epoch [4/10], Batch [361/443], Loss: 0.0600\n",
      "Epoch [4/10], Batch [362/443], Loss: 0.0777\n",
      "Epoch [4/10], Batch [363/443], Loss: 0.0796\n",
      "Epoch [4/10], Batch [364/443], Loss: 0.0745\n",
      "Epoch [4/10], Batch [365/443], Loss: 0.0693\n",
      "Epoch [4/10], Batch [366/443], Loss: 0.0490\n",
      "Epoch [4/10], Batch [367/443], Loss: 0.0368\n",
      "Epoch [4/10], Batch [368/443], Loss: 0.0346\n",
      "Epoch [4/10], Batch [369/443], Loss: 0.0706\n",
      "Epoch [4/10], Batch [370/443], Loss: 0.0314\n",
      "Epoch [4/10], Batch [371/443], Loss: 0.0650\n",
      "Epoch [4/10], Batch [372/443], Loss: 0.0478\n",
      "Epoch [4/10], Batch [373/443], Loss: 0.0618\n",
      "Epoch [4/10], Batch [374/443], Loss: 0.0607\n",
      "Epoch [4/10], Batch [375/443], Loss: 0.1091\n",
      "Epoch [4/10], Batch [376/443], Loss: 0.0936\n",
      "Epoch [4/10], Batch [377/443], Loss: 0.0672\n",
      "Epoch [4/10], Batch [378/443], Loss: 0.1075\n",
      "Epoch [4/10], Batch [379/443], Loss: 0.0774\n",
      "Epoch [4/10], Batch [380/443], Loss: 0.0244\n",
      "Epoch [4/10], Batch [381/443], Loss: 0.0258\n",
      "Epoch [4/10], Batch [382/443], Loss: 0.0257\n",
      "Epoch [4/10], Batch [383/443], Loss: 0.0855\n",
      "Epoch [4/10], Batch [384/443], Loss: 0.0858\n",
      "Epoch [4/10], Batch [385/443], Loss: 0.0642\n",
      "Epoch [4/10], Batch [386/443], Loss: 0.1558\n",
      "Epoch [4/10], Batch [387/443], Loss: 0.0280\n",
      "Epoch [4/10], Batch [388/443], Loss: 0.0787\n",
      "Epoch [4/10], Batch [389/443], Loss: 0.0977\n",
      "Epoch [4/10], Batch [390/443], Loss: 0.0404\n",
      "Epoch [4/10], Batch [391/443], Loss: 0.0478\n",
      "Epoch [4/10], Batch [392/443], Loss: 0.0457\n",
      "Epoch [4/10], Batch [393/443], Loss: 0.0237\n",
      "Epoch [4/10], Batch [394/443], Loss: 0.0332\n",
      "Epoch [4/10], Batch [395/443], Loss: 0.0338\n",
      "Epoch [4/10], Batch [396/443], Loss: 0.0712\n",
      "Epoch [4/10], Batch [397/443], Loss: 0.0623\n",
      "Epoch [4/10], Batch [398/443], Loss: 0.0982\n",
      "Epoch [4/10], Batch [399/443], Loss: 0.0398\n",
      "Epoch [4/10], Batch [400/443], Loss: 0.1621\n",
      "Epoch [4/10], Batch [401/443], Loss: 0.1236\n",
      "Epoch [4/10], Batch [402/443], Loss: 0.0497\n",
      "Epoch [4/10], Batch [403/443], Loss: 0.0555\n",
      "Epoch [4/10], Batch [404/443], Loss: 0.0734\n",
      "Epoch [4/10], Batch [405/443], Loss: 0.0599\n",
      "Epoch [4/10], Batch [406/443], Loss: 0.0330\n",
      "Epoch [4/10], Batch [407/443], Loss: 0.0333\n",
      "Epoch [4/10], Batch [408/443], Loss: 0.1598\n",
      "Epoch [4/10], Batch [409/443], Loss: 0.0555\n",
      "Epoch [4/10], Batch [410/443], Loss: 0.0576\n",
      "Epoch [4/10], Batch [411/443], Loss: 0.1221\n",
      "Epoch [4/10], Batch [412/443], Loss: 0.0580\n",
      "Epoch [4/10], Batch [413/443], Loss: 0.0438\n",
      "Epoch [4/10], Batch [414/443], Loss: 0.1160\n",
      "Epoch [4/10], Batch [415/443], Loss: 0.0769\n",
      "Epoch [4/10], Batch [416/443], Loss: 0.0811\n",
      "Epoch [4/10], Batch [417/443], Loss: 0.0732\n",
      "Epoch [4/10], Batch [418/443], Loss: 0.0650\n",
      "Epoch [4/10], Batch [419/443], Loss: 0.0858\n",
      "Epoch [4/10], Batch [420/443], Loss: 0.0554\n",
      "Epoch [4/10], Batch [421/443], Loss: 0.0551\n",
      "Epoch [4/10], Batch [422/443], Loss: 0.1548\n",
      "Epoch [4/10], Batch [423/443], Loss: 0.0765\n",
      "Epoch [4/10], Batch [424/443], Loss: 0.0484\n",
      "Epoch [4/10], Batch [425/443], Loss: 0.0692\n",
      "Epoch [4/10], Batch [426/443], Loss: 0.0755\n",
      "Epoch [4/10], Batch [427/443], Loss: 0.0554\n",
      "Epoch [4/10], Batch [428/443], Loss: 0.0951\n",
      "Epoch [4/10], Batch [429/443], Loss: 0.0909\n",
      "Epoch [4/10], Batch [430/443], Loss: 0.0901\n",
      "Epoch [4/10], Batch [431/443], Loss: 0.0542\n",
      "Epoch [4/10], Batch [432/443], Loss: 0.1091\n",
      "Epoch [4/10], Batch [433/443], Loss: 0.0506\n",
      "Epoch [4/10], Batch [434/443], Loss: 0.0891\n",
      "Epoch [4/10], Batch [435/443], Loss: 0.1074\n",
      "Epoch [4/10], Batch [436/443], Loss: 0.2243\n",
      "Epoch [4/10], Batch [437/443], Loss: 0.0314\n",
      "Epoch [4/10], Batch [438/443], Loss: 0.1282\n",
      "Epoch [4/10], Batch [439/443], Loss: 0.2046\n",
      "Epoch [4/10], Batch [440/443], Loss: 0.0362\n",
      "Epoch [4/10], Batch [441/443], Loss: 0.0337\n",
      "Epoch [4/10], Batch [442/443], Loss: 0.1513\n",
      "Epoch [4/10], Batch [443/443], Loss: 0.1707\n",
      "Epoch [5/10], Average Loss: 0.0817\n",
      "Model saved at ./saved_models/model_epoch_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2228\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Batch [1/443], Loss: 0.0433\n",
      "Epoch [5/10], Batch [2/443], Loss: 0.0502\n",
      "Epoch [5/10], Batch [3/443], Loss: 0.0667\n",
      "Epoch [5/10], Batch [4/443], Loss: 0.0505\n",
      "Epoch [5/10], Batch [5/443], Loss: 0.0465\n",
      "Epoch [5/10], Batch [6/443], Loss: 0.1457\n",
      "Epoch [5/10], Batch [7/443], Loss: 0.0744\n",
      "Epoch [5/10], Batch [8/443], Loss: 0.2482\n",
      "Epoch [5/10], Batch [9/443], Loss: 0.0543\n",
      "Epoch [5/10], Batch [10/443], Loss: 0.0345\n",
      "Epoch [5/10], Batch [11/443], Loss: 0.1142\n",
      "Epoch [5/10], Batch [12/443], Loss: 0.0941\n",
      "Epoch [5/10], Batch [13/443], Loss: 0.0911\n",
      "Epoch [5/10], Batch [14/443], Loss: 0.0195\n",
      "Epoch [5/10], Batch [15/443], Loss: 0.1428\n",
      "Epoch [5/10], Batch [16/443], Loss: 0.0614\n",
      "Epoch [5/10], Batch [17/443], Loss: 0.0870\n",
      "Epoch [5/10], Batch [18/443], Loss: 0.0867\n",
      "Epoch [5/10], Batch [19/443], Loss: 0.0365\n",
      "Epoch [5/10], Batch [20/443], Loss: 0.0193\n",
      "Epoch [5/10], Batch [21/443], Loss: 0.1749\n",
      "Epoch [5/10], Batch [22/443], Loss: 0.1264\n",
      "Epoch [5/10], Batch [23/443], Loss: 0.0729\n",
      "Epoch [5/10], Batch [24/443], Loss: 0.0808\n",
      "Epoch [5/10], Batch [25/443], Loss: 0.0781\n",
      "Epoch [5/10], Batch [26/443], Loss: 0.2088\n",
      "Epoch [5/10], Batch [27/443], Loss: 0.0777\n",
      "Epoch [5/10], Batch [28/443], Loss: 0.0676\n",
      "Epoch [5/10], Batch [29/443], Loss: 0.0300\n",
      "Epoch [5/10], Batch [30/443], Loss: 0.1497\n",
      "Epoch [5/10], Batch [31/443], Loss: 0.1033\n",
      "Epoch [5/10], Batch [32/443], Loss: 0.1029\n",
      "Epoch [5/10], Batch [33/443], Loss: 0.1201\n",
      "Epoch [5/10], Batch [34/443], Loss: 0.2106\n",
      "Epoch [5/10], Batch [35/443], Loss: 0.0715\n",
      "Epoch [5/10], Batch [36/443], Loss: 0.0908\n",
      "Epoch [5/10], Batch [37/443], Loss: 0.0357\n",
      "Epoch [5/10], Batch [38/443], Loss: 0.0478\n",
      "Epoch [5/10], Batch [39/443], Loss: 0.1111\n",
      "Epoch [5/10], Batch [40/443], Loss: 0.0625\n",
      "Epoch [5/10], Batch [41/443], Loss: 0.1558\n",
      "Epoch [5/10], Batch [42/443], Loss: 0.2520\n",
      "Epoch [5/10], Batch [43/443], Loss: 0.0450\n",
      "Epoch [5/10], Batch [44/443], Loss: 0.1001\n",
      "Epoch [5/10], Batch [45/443], Loss: 0.0212\n",
      "Epoch [5/10], Batch [46/443], Loss: 0.0647\n",
      "Epoch [5/10], Batch [47/443], Loss: 0.0946\n",
      "Epoch [5/10], Batch [48/443], Loss: 0.0983\n",
      "Epoch [5/10], Batch [49/443], Loss: 0.0862\n",
      "Epoch [5/10], Batch [50/443], Loss: 0.0749\n",
      "Epoch [5/10], Batch [51/443], Loss: 0.0325\n",
      "Epoch [5/10], Batch [52/443], Loss: 0.0990\n",
      "Epoch [5/10], Batch [53/443], Loss: 0.0461\n",
      "Epoch [5/10], Batch [54/443], Loss: 0.0992\n",
      "Epoch [5/10], Batch [55/443], Loss: 0.0486\n",
      "Epoch [5/10], Batch [56/443], Loss: 0.0795\n",
      "Epoch [5/10], Batch [57/443], Loss: 0.2197\n",
      "Epoch [5/10], Batch [58/443], Loss: 0.0621\n",
      "Epoch [5/10], Batch [59/443], Loss: 0.2472\n",
      "Epoch [5/10], Batch [60/443], Loss: 0.0566\n",
      "Epoch [5/10], Batch [61/443], Loss: 0.0559\n",
      "Epoch [5/10], Batch [62/443], Loss: 0.1330\n",
      "Epoch [5/10], Batch [63/443], Loss: 0.1515\n",
      "Epoch [5/10], Batch [64/443], Loss: 0.0512\n",
      "Epoch [5/10], Batch [65/443], Loss: 0.0552\n",
      "Epoch [5/10], Batch [66/443], Loss: 0.0412\n",
      "Epoch [5/10], Batch [67/443], Loss: 0.0530\n",
      "Epoch [5/10], Batch [68/443], Loss: 0.0647\n",
      "Epoch [5/10], Batch [69/443], Loss: 0.0464\n",
      "Epoch [5/10], Batch [70/443], Loss: 0.0567\n",
      "Epoch [5/10], Batch [71/443], Loss: 0.0306\n",
      "Epoch [5/10], Batch [72/443], Loss: 0.0636\n",
      "Epoch [5/10], Batch [73/443], Loss: 0.0991\n",
      "Epoch [5/10], Batch [74/443], Loss: 0.0266\n",
      "Epoch [5/10], Batch [75/443], Loss: 0.0573\n",
      "Epoch [5/10], Batch [76/443], Loss: 0.0746\n",
      "Epoch [5/10], Batch [77/443], Loss: 0.0605\n",
      "Epoch [5/10], Batch [78/443], Loss: 0.2101\n",
      "Epoch [5/10], Batch [79/443], Loss: 0.0464\n",
      "Epoch [5/10], Batch [80/443], Loss: 0.0627\n",
      "Epoch [5/10], Batch [81/443], Loss: 0.0461\n",
      "Epoch [5/10], Batch [82/443], Loss: 0.0286\n",
      "Epoch [5/10], Batch [83/443], Loss: 0.0545\n",
      "Epoch [5/10], Batch [84/443], Loss: 0.1290\n",
      "Epoch [5/10], Batch [85/443], Loss: 0.0754\n",
      "Epoch [5/10], Batch [86/443], Loss: 0.0437\n",
      "Epoch [5/10], Batch [87/443], Loss: 0.0781\n",
      "Epoch [5/10], Batch [88/443], Loss: 0.0692\n",
      "Epoch [5/10], Batch [89/443], Loss: 0.0744\n",
      "Epoch [5/10], Batch [90/443], Loss: 0.0377\n",
      "Epoch [5/10], Batch [91/443], Loss: 0.0645\n",
      "Epoch [5/10], Batch [92/443], Loss: 0.0802\n",
      "Epoch [5/10], Batch [93/443], Loss: 0.0653\n",
      "Epoch [5/10], Batch [94/443], Loss: 0.0477\n",
      "Epoch [5/10], Batch [95/443], Loss: 0.1694\n",
      "Epoch [5/10], Batch [96/443], Loss: 0.0738\n",
      "Epoch [5/10], Batch [97/443], Loss: 0.0524\n",
      "Epoch [5/10], Batch [98/443], Loss: 0.0736\n",
      "Epoch [5/10], Batch [99/443], Loss: 0.0429\n",
      "Epoch [5/10], Batch [100/443], Loss: 0.0252\n",
      "Epoch [5/10], Batch [101/443], Loss: 0.0380\n",
      "Epoch [5/10], Batch [102/443], Loss: 0.0436\n",
      "Epoch [5/10], Batch [103/443], Loss: 0.0920\n",
      "Epoch [5/10], Batch [104/443], Loss: 0.2398\n",
      "Epoch [5/10], Batch [105/443], Loss: 0.0734\n",
      "Epoch [5/10], Batch [106/443], Loss: 0.0774\n",
      "Epoch [5/10], Batch [107/443], Loss: 0.0745\n",
      "Epoch [5/10], Batch [108/443], Loss: 0.0555\n",
      "Epoch [5/10], Batch [109/443], Loss: 0.0603\n",
      "Epoch [5/10], Batch [110/443], Loss: 0.0381\n",
      "Epoch [5/10], Batch [111/443], Loss: 0.1670\n",
      "Epoch [5/10], Batch [112/443], Loss: 0.0310\n",
      "Epoch [5/10], Batch [113/443], Loss: 0.1618\n",
      "Epoch [5/10], Batch [114/443], Loss: 0.1149\n",
      "Epoch [5/10], Batch [115/443], Loss: 0.0550\n",
      "Epoch [5/10], Batch [116/443], Loss: 0.0524\n",
      "Epoch [5/10], Batch [117/443], Loss: 0.0317\n",
      "Epoch [5/10], Batch [118/443], Loss: 0.0475\n",
      "Epoch [5/10], Batch [119/443], Loss: 0.2119\n",
      "Epoch [5/10], Batch [120/443], Loss: 0.1039\n",
      "Epoch [5/10], Batch [121/443], Loss: 0.0630\n",
      "Epoch [5/10], Batch [122/443], Loss: 0.1171\n",
      "Epoch [5/10], Batch [123/443], Loss: 0.0696\n",
      "Epoch [5/10], Batch [124/443], Loss: 0.1436\n",
      "Epoch [5/10], Batch [125/443], Loss: 0.0517\n",
      "Epoch [5/10], Batch [126/443], Loss: 0.0665\n",
      "Epoch [5/10], Batch [127/443], Loss: 0.0635\n",
      "Epoch [5/10], Batch [128/443], Loss: 0.0808\n",
      "Epoch [5/10], Batch [129/443], Loss: 0.1196\n",
      "Epoch [5/10], Batch [130/443], Loss: 0.0760\n",
      "Epoch [5/10], Batch [131/443], Loss: 0.0832\n",
      "Epoch [5/10], Batch [132/443], Loss: 0.0825\n",
      "Epoch [5/10], Batch [133/443], Loss: 0.0411\n",
      "Epoch [5/10], Batch [134/443], Loss: 0.0994\n",
      "Epoch [5/10], Batch [135/443], Loss: 0.1478\n",
      "Epoch [5/10], Batch [136/443], Loss: 0.1027\n",
      "Epoch [5/10], Batch [137/443], Loss: 0.0825\n",
      "Epoch [5/10], Batch [138/443], Loss: 0.0428\n",
      "Epoch [5/10], Batch [139/443], Loss: 0.0802\n",
      "Epoch [5/10], Batch [140/443], Loss: 0.0362\n",
      "Epoch [5/10], Batch [141/443], Loss: 0.0504\n",
      "Epoch [5/10], Batch [142/443], Loss: 0.0498\n",
      "Epoch [5/10], Batch [143/443], Loss: 0.1044\n",
      "Epoch [5/10], Batch [144/443], Loss: 0.1077\n",
      "Epoch [5/10], Batch [145/443], Loss: 0.0374\n",
      "Epoch [5/10], Batch [146/443], Loss: 0.0443\n",
      "Epoch [5/10], Batch [147/443], Loss: 0.0458\n",
      "Epoch [5/10], Batch [148/443], Loss: 0.1004\n",
      "Epoch [5/10], Batch [149/443], Loss: 0.0561\n",
      "Epoch [5/10], Batch [150/443], Loss: 0.0588\n",
      "Epoch [5/10], Batch [151/443], Loss: 0.0262\n",
      "Epoch [5/10], Batch [152/443], Loss: 0.1224\n",
      "Epoch [5/10], Batch [153/443], Loss: 0.0480\n",
      "Epoch [5/10], Batch [154/443], Loss: 0.1135\n",
      "Epoch [5/10], Batch [155/443], Loss: 0.0527\n",
      "Epoch [5/10], Batch [156/443], Loss: 0.0883\n",
      "Epoch [5/10], Batch [157/443], Loss: 0.0317\n",
      "Epoch [5/10], Batch [158/443], Loss: 0.0951\n",
      "Epoch [5/10], Batch [159/443], Loss: 0.0719\n",
      "Epoch [5/10], Batch [160/443], Loss: 0.1107\n",
      "Epoch [5/10], Batch [161/443], Loss: 0.1325\n",
      "Epoch [5/10], Batch [162/443], Loss: 0.0193\n",
      "Epoch [5/10], Batch [163/443], Loss: 0.1248\n",
      "Epoch [5/10], Batch [164/443], Loss: 0.0659\n",
      "Epoch [5/10], Batch [165/443], Loss: 0.0836\n",
      "Epoch [5/10], Batch [166/443], Loss: 0.1013\n",
      "Epoch [5/10], Batch [167/443], Loss: 0.0402\n",
      "Epoch [5/10], Batch [168/443], Loss: 0.1076\n",
      "Epoch [5/10], Batch [169/443], Loss: 0.0897\n",
      "Epoch [5/10], Batch [170/443], Loss: 0.0728\n",
      "Epoch [5/10], Batch [171/443], Loss: 0.0924\n",
      "Epoch [5/10], Batch [172/443], Loss: 0.0387\n",
      "Epoch [5/10], Batch [173/443], Loss: 0.0699\n",
      "Epoch [5/10], Batch [174/443], Loss: 0.0942\n",
      "Epoch [5/10], Batch [175/443], Loss: 0.1693\n",
      "Epoch [5/10], Batch [176/443], Loss: 0.0609\n",
      "Epoch [5/10], Batch [177/443], Loss: 0.0813\n",
      "Epoch [5/10], Batch [178/443], Loss: 0.0552\n",
      "Epoch [5/10], Batch [179/443], Loss: 0.0915\n",
      "Epoch [5/10], Batch [180/443], Loss: 0.0430\n",
      "Epoch [5/10], Batch [181/443], Loss: 0.0509\n",
      "Epoch [5/10], Batch [182/443], Loss: 0.1012\n",
      "Epoch [5/10], Batch [183/443], Loss: 0.0813\n",
      "Epoch [5/10], Batch [184/443], Loss: 0.0610\n",
      "Epoch [5/10], Batch [185/443], Loss: 0.0788\n",
      "Epoch [5/10], Batch [186/443], Loss: 0.1179\n",
      "Epoch [5/10], Batch [187/443], Loss: 0.1052\n",
      "Epoch [5/10], Batch [188/443], Loss: 0.1636\n",
      "Epoch [5/10], Batch [189/443], Loss: 0.0863\n",
      "Epoch [5/10], Batch [190/443], Loss: 0.0432\n",
      "Epoch [5/10], Batch [191/443], Loss: 0.0410\n",
      "Epoch [5/10], Batch [192/443], Loss: 0.1367\n",
      "Epoch [5/10], Batch [193/443], Loss: 0.0407\n",
      "Epoch [5/10], Batch [194/443], Loss: 0.1033\n",
      "Epoch [5/10], Batch [195/443], Loss: 0.0667\n",
      "Epoch [5/10], Batch [196/443], Loss: 0.0698\n",
      "Epoch [5/10], Batch [197/443], Loss: 0.0153\n",
      "Epoch [5/10], Batch [198/443], Loss: 0.0470\n",
      "Epoch [5/10], Batch [199/443], Loss: 0.0443\n",
      "Epoch [5/10], Batch [200/443], Loss: 0.0910\n",
      "Epoch [5/10], Batch [201/443], Loss: 0.0369\n",
      "Epoch [5/10], Batch [202/443], Loss: 0.0433\n",
      "Epoch [5/10], Batch [203/443], Loss: 0.0914\n",
      "Epoch [5/10], Batch [204/443], Loss: 0.0775\n",
      "Epoch [5/10], Batch [205/443], Loss: 0.0711\n",
      "Epoch [5/10], Batch [206/443], Loss: 0.0564\n",
      "Epoch [5/10], Batch [207/443], Loss: 0.2344\n",
      "Epoch [5/10], Batch [208/443], Loss: 0.0181\n",
      "Epoch [5/10], Batch [209/443], Loss: 0.0886\n",
      "Epoch [5/10], Batch [210/443], Loss: 0.0507\n",
      "Epoch [5/10], Batch [211/443], Loss: 0.1566\n",
      "Epoch [5/10], Batch [212/443], Loss: 0.0648\n",
      "Epoch [5/10], Batch [213/443], Loss: 0.0329\n",
      "Epoch [5/10], Batch [214/443], Loss: 0.0442\n",
      "Epoch [5/10], Batch [215/443], Loss: 0.1194\n",
      "Epoch [5/10], Batch [216/443], Loss: 0.1479\n",
      "Epoch [5/10], Batch [217/443], Loss: 0.0781\n",
      "Epoch [5/10], Batch [218/443], Loss: 0.1254\n",
      "Epoch [5/10], Batch [219/443], Loss: 0.0922\n",
      "Epoch [5/10], Batch [220/443], Loss: 0.1364\n",
      "Epoch [5/10], Batch [221/443], Loss: 0.0497\n",
      "Epoch [5/10], Batch [222/443], Loss: 0.0363\n",
      "Epoch [5/10], Batch [223/443], Loss: 0.0345\n",
      "Epoch [5/10], Batch [224/443], Loss: 0.0814\n",
      "Epoch [5/10], Batch [225/443], Loss: 0.0971\n",
      "Epoch [5/10], Batch [226/443], Loss: 0.0336\n",
      "Epoch [5/10], Batch [227/443], Loss: 0.0729\n",
      "Epoch [5/10], Batch [228/443], Loss: 0.1495\n",
      "Epoch [5/10], Batch [229/443], Loss: 0.0500\n",
      "Epoch [5/10], Batch [230/443], Loss: 0.1244\n",
      "Epoch [5/10], Batch [231/443], Loss: 0.1309\n",
      "Epoch [5/10], Batch [232/443], Loss: 0.1641\n",
      "Epoch [5/10], Batch [233/443], Loss: 0.0341\n",
      "Epoch [5/10], Batch [234/443], Loss: 0.1064\n",
      "Epoch [5/10], Batch [235/443], Loss: 0.1279\n",
      "Epoch [5/10], Batch [236/443], Loss: 0.0688\n",
      "Epoch [5/10], Batch [237/443], Loss: 0.1226\n",
      "Epoch [5/10], Batch [238/443], Loss: 0.0399\n",
      "Epoch [5/10], Batch [239/443], Loss: 0.1035\n",
      "Epoch [5/10], Batch [240/443], Loss: 0.1202\n",
      "Epoch [5/10], Batch [241/443], Loss: 0.1923\n",
      "Epoch [5/10], Batch [242/443], Loss: 0.2020\n",
      "Epoch [5/10], Batch [243/443], Loss: 0.2562\n",
      "Epoch [5/10], Batch [244/443], Loss: 0.0310\n",
      "Epoch [5/10], Batch [245/443], Loss: 0.0195\n",
      "Epoch [5/10], Batch [246/443], Loss: 0.1218\n",
      "Epoch [5/10], Batch [247/443], Loss: 0.0573\n",
      "Epoch [5/10], Batch [248/443], Loss: 0.1850\n",
      "Epoch [5/10], Batch [249/443], Loss: 0.0498\n",
      "Epoch [5/10], Batch [250/443], Loss: 0.1272\n",
      "Epoch [5/10], Batch [251/443], Loss: 0.1133\n",
      "Epoch [5/10], Batch [252/443], Loss: 0.0489\n",
      "Epoch [5/10], Batch [253/443], Loss: 0.0443\n",
      "Epoch [5/10], Batch [254/443], Loss: 0.0390\n",
      "Epoch [5/10], Batch [255/443], Loss: 0.1725\n",
      "Epoch [5/10], Batch [256/443], Loss: 0.0484\n",
      "Epoch [5/10], Batch [257/443], Loss: 0.0486\n",
      "Epoch [5/10], Batch [258/443], Loss: 0.0650\n",
      "Epoch [5/10], Batch [259/443], Loss: 0.0396\n",
      "Epoch [5/10], Batch [260/443], Loss: 0.0804\n",
      "Epoch [5/10], Batch [261/443], Loss: 0.0605\n",
      "Epoch [5/10], Batch [262/443], Loss: 0.0323\n",
      "Epoch [5/10], Batch [263/443], Loss: 0.0577\n",
      "Epoch [5/10], Batch [264/443], Loss: 0.0716\n",
      "Epoch [5/10], Batch [265/443], Loss: 0.0804\n",
      "Epoch [5/10], Batch [266/443], Loss: 0.0467\n",
      "Epoch [5/10], Batch [267/443], Loss: 0.0369\n",
      "Epoch [5/10], Batch [268/443], Loss: 0.0749\n",
      "Epoch [5/10], Batch [269/443], Loss: 0.0713\n",
      "Epoch [5/10], Batch [270/443], Loss: 0.0638\n",
      "Epoch [5/10], Batch [271/443], Loss: 0.1669\n",
      "Epoch [5/10], Batch [272/443], Loss: 0.0322\n",
      "Epoch [5/10], Batch [273/443], Loss: 0.1376\n",
      "Epoch [5/10], Batch [274/443], Loss: 0.0217\n",
      "Epoch [5/10], Batch [275/443], Loss: 0.0698\n",
      "Epoch [5/10], Batch [276/443], Loss: 0.0590\n",
      "Epoch [5/10], Batch [277/443], Loss: 0.0997\n",
      "Epoch [5/10], Batch [278/443], Loss: 0.0535\n",
      "Epoch [5/10], Batch [279/443], Loss: 0.0508\n",
      "Epoch [5/10], Batch [280/443], Loss: 0.0603\n",
      "Epoch [5/10], Batch [281/443], Loss: 0.0476\n",
      "Epoch [5/10], Batch [282/443], Loss: 0.0328\n",
      "Epoch [5/10], Batch [283/443], Loss: 0.0731\n",
      "Epoch [5/10], Batch [284/443], Loss: 0.0551\n",
      "Epoch [5/10], Batch [285/443], Loss: 0.1204\n",
      "Epoch [5/10], Batch [286/443], Loss: 0.1079\n",
      "Epoch [5/10], Batch [287/443], Loss: 0.0717\n",
      "Epoch [5/10], Batch [288/443], Loss: 0.0549\n",
      "Epoch [5/10], Batch [289/443], Loss: 0.0962\n",
      "Epoch [5/10], Batch [290/443], Loss: 0.1313\n",
      "Epoch [5/10], Batch [291/443], Loss: 0.0539\n",
      "Epoch [5/10], Batch [292/443], Loss: 0.0523\n",
      "Epoch [5/10], Batch [293/443], Loss: 0.0239\n",
      "Epoch [5/10], Batch [294/443], Loss: 0.0364\n",
      "Epoch [5/10], Batch [295/443], Loss: 0.0618\n",
      "Epoch [5/10], Batch [296/443], Loss: 0.0529\n",
      "Epoch [5/10], Batch [297/443], Loss: 0.1037\n",
      "Epoch [5/10], Batch [298/443], Loss: 0.0334\n",
      "Epoch [5/10], Batch [299/443], Loss: 0.2680\n",
      "Epoch [5/10], Batch [300/443], Loss: 0.1420\n",
      "Epoch [5/10], Batch [301/443], Loss: 0.0370\n",
      "Epoch [5/10], Batch [302/443], Loss: 0.0809\n",
      "Epoch [5/10], Batch [303/443], Loss: 0.0836\n",
      "Epoch [5/10], Batch [304/443], Loss: 0.0797\n",
      "Epoch [5/10], Batch [305/443], Loss: 0.0894\n",
      "Epoch [5/10], Batch [306/443], Loss: 0.0488\n",
      "Epoch [5/10], Batch [307/443], Loss: 0.1212\n",
      "Epoch [5/10], Batch [308/443], Loss: 0.0325\n",
      "Epoch [5/10], Batch [309/443], Loss: 0.0366\n",
      "Epoch [5/10], Batch [310/443], Loss: 0.0405\n",
      "Epoch [5/10], Batch [311/443], Loss: 0.0529\n",
      "Epoch [5/10], Batch [312/443], Loss: 0.0510\n",
      "Epoch [5/10], Batch [313/443], Loss: 0.0978\n",
      "Epoch [5/10], Batch [314/443], Loss: 0.0988\n",
      "Epoch [5/10], Batch [315/443], Loss: 0.0572\n",
      "Epoch [5/10], Batch [316/443], Loss: 0.1544\n",
      "Epoch [5/10], Batch [317/443], Loss: 0.1046\n",
      "Epoch [5/10], Batch [318/443], Loss: 0.0753\n",
      "Epoch [5/10], Batch [319/443], Loss: 0.0435\n",
      "Epoch [5/10], Batch [320/443], Loss: 0.0708\n",
      "Epoch [5/10], Batch [321/443], Loss: 0.0468\n",
      "Epoch [5/10], Batch [322/443], Loss: 0.0780\n",
      "Epoch [5/10], Batch [323/443], Loss: 0.0107\n",
      "Epoch [5/10], Batch [324/443], Loss: 0.1964\n",
      "Epoch [5/10], Batch [325/443], Loss: 0.0938\n",
      "Epoch [5/10], Batch [326/443], Loss: 0.0459\n",
      "Epoch [5/10], Batch [327/443], Loss: 0.0926\n",
      "Epoch [5/10], Batch [328/443], Loss: 0.1082\n",
      "Epoch [5/10], Batch [329/443], Loss: 0.0752\n",
      "Epoch [5/10], Batch [330/443], Loss: 0.1459\n",
      "Epoch [5/10], Batch [331/443], Loss: 0.0997\n",
      "Epoch [5/10], Batch [332/443], Loss: 0.0352\n",
      "Epoch [5/10], Batch [333/443], Loss: 0.0642\n",
      "Epoch [5/10], Batch [334/443], Loss: 0.0474\n",
      "Epoch [5/10], Batch [335/443], Loss: 0.0508\n",
      "Epoch [5/10], Batch [336/443], Loss: 0.0983\n",
      "Epoch [5/10], Batch [337/443], Loss: 0.1416\n",
      "Epoch [5/10], Batch [338/443], Loss: 0.1145\n",
      "Epoch [5/10], Batch [339/443], Loss: 0.1024\n",
      "Epoch [5/10], Batch [340/443], Loss: 0.1031\n",
      "Epoch [5/10], Batch [341/443], Loss: 0.0496\n",
      "Epoch [5/10], Batch [342/443], Loss: 0.0925\n",
      "Epoch [5/10], Batch [343/443], Loss: 0.1481\n",
      "Epoch [5/10], Batch [344/443], Loss: 0.1150\n",
      "Epoch [5/10], Batch [345/443], Loss: 0.0372\n",
      "Epoch [5/10], Batch [346/443], Loss: 0.0436\n",
      "Epoch [5/10], Batch [347/443], Loss: 0.0294\n",
      "Epoch [5/10], Batch [348/443], Loss: 0.0561\n",
      "Epoch [5/10], Batch [349/443], Loss: 0.1675\n",
      "Epoch [5/10], Batch [350/443], Loss: 0.0551\n",
      "Epoch [5/10], Batch [351/443], Loss: 0.0762\n",
      "Epoch [5/10], Batch [352/443], Loss: 0.0327\n",
      "Epoch [5/10], Batch [353/443], Loss: 0.0650\n",
      "Epoch [5/10], Batch [354/443], Loss: 0.0284\n",
      "Epoch [5/10], Batch [355/443], Loss: 0.1486\n",
      "Epoch [5/10], Batch [356/443], Loss: 0.0496\n",
      "Epoch [5/10], Batch [357/443], Loss: 0.0819\n",
      "Epoch [5/10], Batch [358/443], Loss: 0.0626\n",
      "Epoch [5/10], Batch [359/443], Loss: 0.0784\n",
      "Epoch [5/10], Batch [360/443], Loss: 0.0600\n",
      "Epoch [5/10], Batch [361/443], Loss: 0.1207\n",
      "Epoch [5/10], Batch [362/443], Loss: 0.0367\n",
      "Epoch [5/10], Batch [363/443], Loss: 0.0502\n",
      "Epoch [5/10], Batch [364/443], Loss: 0.0700\n",
      "Epoch [5/10], Batch [365/443], Loss: 0.1692\n",
      "Epoch [5/10], Batch [366/443], Loss: 0.0345\n",
      "Epoch [5/10], Batch [367/443], Loss: 0.0987\n",
      "Epoch [5/10], Batch [368/443], Loss: 0.0308\n",
      "Epoch [5/10], Batch [369/443], Loss: 0.0722\n",
      "Epoch [5/10], Batch [370/443], Loss: 0.1394\n",
      "Epoch [5/10], Batch [371/443], Loss: 0.0301\n",
      "Epoch [5/10], Batch [372/443], Loss: 0.0790\n",
      "Epoch [5/10], Batch [373/443], Loss: 0.0453\n",
      "Epoch [5/10], Batch [374/443], Loss: 0.0534\n",
      "Epoch [5/10], Batch [375/443], Loss: 0.3083\n",
      "Epoch [5/10], Batch [376/443], Loss: 0.1812\n",
      "Epoch [5/10], Batch [377/443], Loss: 0.0746\n",
      "Epoch [5/10], Batch [378/443], Loss: 0.0659\n",
      "Epoch [5/10], Batch [379/443], Loss: 0.1080\n",
      "Epoch [5/10], Batch [380/443], Loss: 0.0569\n",
      "Epoch [5/10], Batch [381/443], Loss: 0.0865\n",
      "Epoch [5/10], Batch [382/443], Loss: 0.1296\n",
      "Epoch [5/10], Batch [383/443], Loss: 0.0379\n",
      "Epoch [5/10], Batch [384/443], Loss: 0.0411\n",
      "Epoch [5/10], Batch [385/443], Loss: 0.0782\n",
      "Epoch [5/10], Batch [386/443], Loss: 0.0468\n",
      "Epoch [5/10], Batch [387/443], Loss: 0.0771\n",
      "Epoch [5/10], Batch [388/443], Loss: 0.0429\n",
      "Epoch [5/10], Batch [389/443], Loss: 0.0439\n",
      "Epoch [5/10], Batch [390/443], Loss: 0.2502\n",
      "Epoch [5/10], Batch [391/443], Loss: 0.0815\n",
      "Epoch [5/10], Batch [392/443], Loss: 0.0388\n",
      "Epoch [5/10], Batch [393/443], Loss: 0.1970\n",
      "Epoch [5/10], Batch [394/443], Loss: 0.1685\n",
      "Epoch [5/10], Batch [395/443], Loss: 0.1010\n",
      "Epoch [5/10], Batch [396/443], Loss: 0.0263\n",
      "Epoch [5/10], Batch [397/443], Loss: 0.1484\n",
      "Epoch [5/10], Batch [398/443], Loss: 0.1258\n",
      "Epoch [5/10], Batch [399/443], Loss: 0.0441\n",
      "Epoch [5/10], Batch [400/443], Loss: 0.0385\n",
      "Epoch [5/10], Batch [401/443], Loss: 0.0919\n",
      "Epoch [5/10], Batch [402/443], Loss: 0.1191\n",
      "Epoch [5/10], Batch [403/443], Loss: 0.0653\n",
      "Epoch [5/10], Batch [404/443], Loss: 0.0331\n",
      "Epoch [5/10], Batch [405/443], Loss: 0.0600\n",
      "Epoch [5/10], Batch [406/443], Loss: 0.0421\n",
      "Epoch [5/10], Batch [407/443], Loss: 0.1247\n",
      "Epoch [5/10], Batch [408/443], Loss: 0.0352\n",
      "Epoch [5/10], Batch [409/443], Loss: 0.1018\n",
      "Epoch [5/10], Batch [410/443], Loss: 0.0623\n",
      "Epoch [5/10], Batch [411/443], Loss: 0.1017\n",
      "Epoch [5/10], Batch [412/443], Loss: 0.0717\n",
      "Epoch [5/10], Batch [413/443], Loss: 0.1345\n",
      "Epoch [5/10], Batch [414/443], Loss: 0.0829\n",
      "Epoch [5/10], Batch [415/443], Loss: 0.0783\n",
      "Epoch [5/10], Batch [416/443], Loss: 0.0988\n",
      "Epoch [5/10], Batch [417/443], Loss: 0.0536\n",
      "Epoch [5/10], Batch [418/443], Loss: 0.0781\n",
      "Epoch [5/10], Batch [419/443], Loss: 0.0757\n",
      "Epoch [5/10], Batch [420/443], Loss: 0.0186\n",
      "Epoch [5/10], Batch [421/443], Loss: 0.0419\n",
      "Epoch [5/10], Batch [422/443], Loss: 0.0501\n",
      "Epoch [5/10], Batch [423/443], Loss: 0.0795\n",
      "Epoch [5/10], Batch [424/443], Loss: 0.0176\n",
      "Epoch [5/10], Batch [425/443], Loss: 0.0310\n",
      "Epoch [5/10], Batch [426/443], Loss: 0.0517\n",
      "Epoch [5/10], Batch [427/443], Loss: 0.1666\n",
      "Epoch [5/10], Batch [428/443], Loss: 0.0470\n",
      "Epoch [5/10], Batch [429/443], Loss: 0.2680\n",
      "Epoch [5/10], Batch [430/443], Loss: 0.0365\n",
      "Epoch [5/10], Batch [431/443], Loss: 0.1676\n",
      "Epoch [5/10], Batch [432/443], Loss: 0.1034\n",
      "Epoch [5/10], Batch [433/443], Loss: 0.1488\n",
      "Epoch [5/10], Batch [434/443], Loss: 0.0720\n",
      "Epoch [5/10], Batch [435/443], Loss: 0.1585\n",
      "Epoch [5/10], Batch [436/443], Loss: 0.2311\n",
      "Epoch [5/10], Batch [437/443], Loss: 0.0609\n",
      "Epoch [5/10], Batch [438/443], Loss: 0.0510\n",
      "Epoch [5/10], Batch [439/443], Loss: 0.0421\n",
      "Epoch [5/10], Batch [440/443], Loss: 0.0750\n",
      "Epoch [5/10], Batch [441/443], Loss: 0.0674\n",
      "Epoch [5/10], Batch [442/443], Loss: 0.1153\n",
      "Epoch [5/10], Batch [443/443], Loss: 0.0549\n",
      "Epoch [6/10], Average Loss: 0.0830\n",
      "Model saved at ./saved_models/model_epoch_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Validation Loss: 0.2228\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Batch [1/443], Loss: 0.0750\n",
      "Epoch [6/10], Batch [2/443], Loss: 0.1018\n",
      "Epoch [6/10], Batch [3/443], Loss: 0.2115\n",
      "Epoch [6/10], Batch [4/443], Loss: 0.0894\n",
      "Epoch [6/10], Batch [5/443], Loss: 0.1672\n",
      "Epoch [6/10], Batch [6/443], Loss: 0.0656\n",
      "Epoch [6/10], Batch [7/443], Loss: 0.1401\n",
      "Epoch [6/10], Batch [8/443], Loss: 0.0436\n",
      "Epoch [6/10], Batch [9/443], Loss: 0.0553\n",
      "Epoch [6/10], Batch [10/443], Loss: 0.2760\n",
      "Epoch [6/10], Batch [11/443], Loss: 0.0328\n",
      "Epoch [6/10], Batch [12/443], Loss: 0.0281\n",
      "Epoch [6/10], Batch [13/443], Loss: 0.0398\n",
      "Epoch [6/10], Batch [14/443], Loss: 0.1045\n",
      "Epoch [6/10], Batch [15/443], Loss: 0.0349\n",
      "Epoch [6/10], Batch [16/443], Loss: 0.1704\n",
      "Epoch [6/10], Batch [17/443], Loss: 0.0453\n",
      "Epoch [6/10], Batch [18/443], Loss: 0.0186\n",
      "Epoch [6/10], Batch [19/443], Loss: 0.1002\n",
      "Epoch [6/10], Batch [20/443], Loss: 0.1641\n",
      "Epoch [6/10], Batch [21/443], Loss: 0.0747\n",
      "Epoch [6/10], Batch [22/443], Loss: 0.1050\n",
      "Epoch [6/10], Batch [23/443], Loss: 0.0794\n",
      "Epoch [6/10], Batch [24/443], Loss: 0.1565\n",
      "Epoch [6/10], Batch [25/443], Loss: 0.0582\n",
      "Epoch [6/10], Batch [26/443], Loss: 0.0617\n",
      "Epoch [6/10], Batch [27/443], Loss: 0.0544\n",
      "Epoch [6/10], Batch [28/443], Loss: 0.0868\n",
      "Epoch [6/10], Batch [29/443], Loss: 0.1509\n",
      "Epoch [6/10], Batch [30/443], Loss: 0.0484\n",
      "Epoch [6/10], Batch [31/443], Loss: 0.0743\n",
      "Epoch [6/10], Batch [32/443], Loss: 0.0800\n",
      "Epoch [6/10], Batch [33/443], Loss: 0.0724\n",
      "Epoch [6/10], Batch [34/443], Loss: 0.0224\n",
      "Epoch [6/10], Batch [35/443], Loss: 0.0720\n",
      "Epoch [6/10], Batch [36/443], Loss: 0.2299\n",
      "Epoch [6/10], Batch [37/443], Loss: 0.0677\n",
      "Epoch [6/10], Batch [38/443], Loss: 0.0334\n",
      "Epoch [6/10], Batch [39/443], Loss: 0.0288\n",
      "Epoch [6/10], Batch [40/443], Loss: 0.1115\n",
      "Epoch [6/10], Batch [41/443], Loss: 0.0695\n",
      "Epoch [6/10], Batch [42/443], Loss: 0.0301\n",
      "Epoch [6/10], Batch [43/443], Loss: 0.1340\n",
      "Epoch [6/10], Batch [44/443], Loss: 0.0980\n",
      "Epoch [6/10], Batch [45/443], Loss: 0.1069\n",
      "Epoch [6/10], Batch [46/443], Loss: 0.0280\n",
      "Epoch [6/10], Batch [47/443], Loss: 0.0806\n",
      "Epoch [6/10], Batch [48/443], Loss: 0.0904\n",
      "Epoch [6/10], Batch [49/443], Loss: 0.0920\n",
      "Epoch [6/10], Batch [50/443], Loss: 0.0499\n",
      "Epoch [6/10], Batch [51/443], Loss: 0.0900\n",
      "Epoch [6/10], Batch [52/443], Loss: 0.0599\n",
      "Epoch [6/10], Batch [53/443], Loss: 0.0805\n",
      "Epoch [6/10], Batch [54/443], Loss: 0.0183\n",
      "Epoch [6/10], Batch [55/443], Loss: 0.0935\n",
      "Epoch [6/10], Batch [56/443], Loss: 0.0604\n",
      "Epoch [6/10], Batch [57/443], Loss: 0.0282\n",
      "Epoch [6/10], Batch [58/443], Loss: 0.0791\n",
      "Epoch [6/10], Batch [59/443], Loss: 0.1085\n",
      "Epoch [6/10], Batch [60/443], Loss: 0.0912\n",
      "Epoch [6/10], Batch [61/443], Loss: 0.0784\n",
      "Epoch [6/10], Batch [62/443], Loss: 0.0446\n",
      "Epoch [6/10], Batch [63/443], Loss: 0.0395\n",
      "Epoch [6/10], Batch [64/443], Loss: 0.0469\n",
      "Epoch [6/10], Batch [65/443], Loss: 0.1155\n",
      "Epoch [6/10], Batch [66/443], Loss: 0.0773\n",
      "Epoch [6/10], Batch [67/443], Loss: 0.0245\n",
      "Epoch [6/10], Batch [68/443], Loss: 0.1154\n",
      "Epoch [6/10], Batch [69/443], Loss: 0.0480\n",
      "Epoch [6/10], Batch [70/443], Loss: 0.0566\n",
      "Epoch [6/10], Batch [71/443], Loss: 0.1010\n",
      "Epoch [6/10], Batch [72/443], Loss: 0.0830\n",
      "Epoch [6/10], Batch [73/443], Loss: 0.1161\n",
      "Epoch [6/10], Batch [74/443], Loss: 0.1029\n",
      "Epoch [6/10], Batch [75/443], Loss: 0.1349\n",
      "Epoch [6/10], Batch [76/443], Loss: 0.1423\n",
      "Epoch [6/10], Batch [77/443], Loss: 0.0381\n",
      "Epoch [6/10], Batch [78/443], Loss: 0.1156\n",
      "Epoch [6/10], Batch [79/443], Loss: 0.0881\n",
      "Epoch [6/10], Batch [80/443], Loss: 0.0986\n",
      "Epoch [6/10], Batch [81/443], Loss: 0.0759\n",
      "Epoch [6/10], Batch [82/443], Loss: 0.0191\n",
      "Epoch [6/10], Batch [83/443], Loss: 0.0799\n",
      "Epoch [6/10], Batch [84/443], Loss: 0.0547\n",
      "Epoch [6/10], Batch [85/443], Loss: 0.1477\n",
      "Epoch [6/10], Batch [86/443], Loss: 0.0533\n",
      "Epoch [6/10], Batch [87/443], Loss: 0.0497\n",
      "Epoch [6/10], Batch [88/443], Loss: 0.1309\n",
      "Epoch [6/10], Batch [89/443], Loss: 0.0975\n",
      "Epoch [6/10], Batch [90/443], Loss: 0.0848\n",
      "Epoch [6/10], Batch [91/443], Loss: 0.0318\n",
      "Epoch [6/10], Batch [92/443], Loss: 0.0502\n",
      "Epoch [6/10], Batch [93/443], Loss: 0.0408\n",
      "Epoch [6/10], Batch [94/443], Loss: 0.0529\n",
      "Epoch [6/10], Batch [95/443], Loss: 0.1072\n",
      "Epoch [6/10], Batch [96/443], Loss: 0.0736\n",
      "Epoch [6/10], Batch [97/443], Loss: 0.0693\n",
      "Epoch [6/10], Batch [98/443], Loss: 0.0373\n",
      "Epoch [6/10], Batch [99/443], Loss: 0.1346\n",
      "Epoch [6/10], Batch [100/443], Loss: 0.0362\n",
      "Epoch [6/10], Batch [101/443], Loss: 0.0738\n",
      "Epoch [6/10], Batch [102/443], Loss: 0.0975\n",
      "Epoch [6/10], Batch [103/443], Loss: 0.1911\n",
      "Epoch [6/10], Batch [104/443], Loss: 0.0938\n",
      "Epoch [6/10], Batch [105/443], Loss: 0.0487\n",
      "Epoch [6/10], Batch [106/443], Loss: 0.0695\n",
      "Epoch [6/10], Batch [107/443], Loss: 0.1738\n",
      "Epoch [6/10], Batch [108/443], Loss: 0.1209\n",
      "Epoch [6/10], Batch [109/443], Loss: 0.1855\n",
      "Epoch [6/10], Batch [110/443], Loss: 0.1822\n",
      "Epoch [6/10], Batch [111/443], Loss: 0.0608\n",
      "Epoch [6/10], Batch [112/443], Loss: 0.1185\n",
      "Epoch [6/10], Batch [113/443], Loss: 0.0627\n",
      "Epoch [6/10], Batch [114/443], Loss: 0.1062\n",
      "Epoch [6/10], Batch [115/443], Loss: 0.0329\n",
      "Epoch [6/10], Batch [116/443], Loss: 0.0586\n",
      "Epoch [6/10], Batch [117/443], Loss: 0.1738\n",
      "Epoch [6/10], Batch [118/443], Loss: 0.0644\n",
      "Epoch [6/10], Batch [119/443], Loss: 0.0464\n",
      "Epoch [6/10], Batch [120/443], Loss: 0.0850\n",
      "Epoch [6/10], Batch [121/443], Loss: 0.0372\n",
      "Epoch [6/10], Batch [122/443], Loss: 0.0378\n",
      "Epoch [6/10], Batch [123/443], Loss: 0.0421\n",
      "Epoch [6/10], Batch [124/443], Loss: 0.2591\n",
      "Epoch [6/10], Batch [125/443], Loss: 0.0803\n",
      "Epoch [6/10], Batch [126/443], Loss: 0.0940\n",
      "Epoch [6/10], Batch [127/443], Loss: 0.0746\n",
      "Epoch [6/10], Batch [128/443], Loss: 0.0527\n",
      "Epoch [6/10], Batch [129/443], Loss: 0.0413\n",
      "Epoch [6/10], Batch [130/443], Loss: 0.0656\n",
      "Epoch [6/10], Batch [131/443], Loss: 0.0770\n",
      "Epoch [6/10], Batch [132/443], Loss: 0.0837\n",
      "Epoch [6/10], Batch [133/443], Loss: 0.0305\n",
      "Epoch [6/10], Batch [134/443], Loss: 0.0748\n",
      "Epoch [6/10], Batch [135/443], Loss: 0.1160\n",
      "Epoch [6/10], Batch [136/443], Loss: 0.0989\n",
      "Epoch [6/10], Batch [137/443], Loss: 0.1106\n",
      "Epoch [6/10], Batch [138/443], Loss: 0.1529\n",
      "Epoch [6/10], Batch [139/443], Loss: 0.0874\n",
      "Epoch [6/10], Batch [140/443], Loss: 0.1689\n",
      "Epoch [6/10], Batch [141/443], Loss: 0.0481\n",
      "Epoch [6/10], Batch [142/443], Loss: 0.1854\n",
      "Epoch [6/10], Batch [143/443], Loss: 0.0673\n",
      "Epoch [6/10], Batch [144/443], Loss: 0.1305\n",
      "Epoch [6/10], Batch [145/443], Loss: 0.0676\n",
      "Epoch [6/10], Batch [146/443], Loss: 0.1252\n",
      "Epoch [6/10], Batch [147/443], Loss: 0.0952\n",
      "Epoch [6/10], Batch [148/443], Loss: 0.0745\n",
      "Epoch [6/10], Batch [149/443], Loss: 0.0679\n",
      "Epoch [6/10], Batch [150/443], Loss: 0.0380\n",
      "Epoch [6/10], Batch [151/443], Loss: 0.0855\n",
      "Epoch [6/10], Batch [152/443], Loss: 0.0448\n",
      "Epoch [6/10], Batch [153/443], Loss: 0.0737\n",
      "Epoch [6/10], Batch [154/443], Loss: 0.0794\n",
      "Epoch [6/10], Batch [155/443], Loss: 0.0800\n",
      "Epoch [6/10], Batch [156/443], Loss: 0.0507\n",
      "Epoch [6/10], Batch [157/443], Loss: 0.0834\n",
      "Epoch [6/10], Batch [158/443], Loss: 0.0504\n",
      "Epoch [6/10], Batch [159/443], Loss: 0.0342\n",
      "Epoch [6/10], Batch [160/443], Loss: 0.0544\n",
      "Epoch [6/10], Batch [161/443], Loss: 0.1002\n",
      "Epoch [6/10], Batch [162/443], Loss: 0.0946\n",
      "Epoch [6/10], Batch [163/443], Loss: 0.0377\n",
      "Epoch [6/10], Batch [164/443], Loss: 0.0691\n",
      "Epoch [6/10], Batch [165/443], Loss: 0.2679\n",
      "Epoch [6/10], Batch [166/443], Loss: 0.0537\n",
      "Epoch [6/10], Batch [167/443], Loss: 0.0615\n",
      "Epoch [6/10], Batch [168/443], Loss: 0.0539\n",
      "Epoch [6/10], Batch [169/443], Loss: 0.0533\n",
      "Epoch [6/10], Batch [170/443], Loss: 0.0574\n",
      "Epoch [6/10], Batch [171/443], Loss: 0.0929\n",
      "Epoch [6/10], Batch [172/443], Loss: 0.0319\n",
      "Epoch [6/10], Batch [173/443], Loss: 0.0963\n",
      "Epoch [6/10], Batch [174/443], Loss: 0.1608\n",
      "Epoch [6/10], Batch [175/443], Loss: 0.0301\n",
      "Epoch [6/10], Batch [176/443], Loss: 0.0612\n",
      "Epoch [6/10], Batch [177/443], Loss: 0.0527\n",
      "Epoch [6/10], Batch [178/443], Loss: 0.1758\n",
      "Epoch [6/10], Batch [179/443], Loss: 0.0691\n",
      "Epoch [6/10], Batch [180/443], Loss: 0.0329\n",
      "Epoch [6/10], Batch [181/443], Loss: 0.0565\n",
      "Epoch [6/10], Batch [182/443], Loss: 0.0438\n",
      "Epoch [6/10], Batch [183/443], Loss: 0.1208\n",
      "Epoch [6/10], Batch [184/443], Loss: 0.0426\n",
      "Epoch [6/10], Batch [185/443], Loss: 0.0403\n",
      "Epoch [6/10], Batch [186/443], Loss: 0.0647\n",
      "Epoch [6/10], Batch [187/443], Loss: 0.0118\n",
      "Epoch [6/10], Batch [188/443], Loss: 0.0237\n",
      "Epoch [6/10], Batch [189/443], Loss: 0.1268\n",
      "Epoch [6/10], Batch [190/443], Loss: 0.0734\n",
      "Epoch [6/10], Batch [191/443], Loss: 0.0831\n",
      "Epoch [6/10], Batch [192/443], Loss: 0.0877\n",
      "Epoch [6/10], Batch [193/443], Loss: 0.0848\n",
      "Epoch [6/10], Batch [194/443], Loss: 0.0768\n",
      "Epoch [6/10], Batch [195/443], Loss: 0.0461\n",
      "Epoch [6/10], Batch [196/443], Loss: 0.1484\n",
      "Epoch [6/10], Batch [197/443], Loss: 0.0948\n",
      "Epoch [6/10], Batch [198/443], Loss: 0.0206\n",
      "Epoch [6/10], Batch [199/443], Loss: 0.0478\n",
      "Epoch [6/10], Batch [200/443], Loss: 0.0380\n",
      "Epoch [6/10], Batch [201/443], Loss: 0.0788\n",
      "Epoch [6/10], Batch [202/443], Loss: 0.0384\n",
      "Epoch [6/10], Batch [203/443], Loss: 0.0444\n",
      "Epoch [6/10], Batch [204/443], Loss: 0.1392\n",
      "Epoch [6/10], Batch [205/443], Loss: 0.0956\n",
      "Epoch [6/10], Batch [206/443], Loss: 0.0510\n",
      "Epoch [6/10], Batch [207/443], Loss: 0.1032\n",
      "Epoch [6/10], Batch [208/443], Loss: 0.0668\n",
      "Epoch [6/10], Batch [209/443], Loss: 0.0437\n",
      "Epoch [6/10], Batch [210/443], Loss: 0.1216\n",
      "Epoch [6/10], Batch [211/443], Loss: 0.0397\n",
      "Epoch [6/10], Batch [212/443], Loss: 0.0851\n",
      "Epoch [6/10], Batch [213/443], Loss: 0.0667\n",
      "Epoch [6/10], Batch [214/443], Loss: 0.0164\n",
      "Epoch [6/10], Batch [215/443], Loss: 0.2068\n",
      "Epoch [6/10], Batch [216/443], Loss: 0.0632\n",
      "Epoch [6/10], Batch [217/443], Loss: 0.0218\n",
      "Epoch [6/10], Batch [218/443], Loss: 0.0329\n",
      "Epoch [6/10], Batch [219/443], Loss: 0.0440\n",
      "Epoch [6/10], Batch [220/443], Loss: 0.0387\n",
      "Epoch [6/10], Batch [221/443], Loss: 0.0386\n",
      "Epoch [6/10], Batch [222/443], Loss: 0.0647\n",
      "Epoch [6/10], Batch [223/443], Loss: 0.0612\n",
      "Epoch [6/10], Batch [224/443], Loss: 0.0599\n",
      "Epoch [6/10], Batch [225/443], Loss: 0.0285\n",
      "Epoch [6/10], Batch [226/443], Loss: 0.0664\n",
      "Epoch [6/10], Batch [227/443], Loss: 0.0751\n",
      "Epoch [6/10], Batch [228/443], Loss: 0.0790\n",
      "Epoch [6/10], Batch [229/443], Loss: 0.0195\n",
      "Epoch [6/10], Batch [230/443], Loss: 0.1819\n",
      "Epoch [6/10], Batch [231/443], Loss: 0.0474\n",
      "Epoch [6/10], Batch [232/443], Loss: 0.1418\n",
      "Epoch [6/10], Batch [233/443], Loss: 0.0355\n",
      "Epoch [6/10], Batch [234/443], Loss: 0.0371\n",
      "Epoch [6/10], Batch [235/443], Loss: 0.0696\n",
      "Epoch [6/10], Batch [236/443], Loss: 0.1144\n",
      "Epoch [6/10], Batch [237/443], Loss: 0.0798\n",
      "Epoch [6/10], Batch [238/443], Loss: 0.0945\n",
      "Epoch [6/10], Batch [239/443], Loss: 0.0491\n",
      "Epoch [6/10], Batch [240/443], Loss: 0.0895\n",
      "Epoch [6/10], Batch [241/443], Loss: 0.0199\n",
      "Epoch [6/10], Batch [242/443], Loss: 0.0658\n",
      "Epoch [6/10], Batch [243/443], Loss: 0.0923\n",
      "Epoch [6/10], Batch [244/443], Loss: 0.0481\n",
      "Epoch [6/10], Batch [245/443], Loss: 0.1592\n",
      "Epoch [6/10], Batch [246/443], Loss: 0.0680\n",
      "Epoch [6/10], Batch [247/443], Loss: 0.0640\n",
      "Epoch [6/10], Batch [248/443], Loss: 0.0520\n",
      "Epoch [6/10], Batch [249/443], Loss: 0.0250\n",
      "Epoch [6/10], Batch [250/443], Loss: 0.0756\n",
      "Epoch [6/10], Batch [251/443], Loss: 0.0259\n",
      "Epoch [6/10], Batch [252/443], Loss: 0.1405\n",
      "Epoch [6/10], Batch [253/443], Loss: 0.0452\n",
      "Epoch [6/10], Batch [254/443], Loss: 0.1771\n",
      "Epoch [6/10], Batch [255/443], Loss: 0.0801\n",
      "Epoch [6/10], Batch [256/443], Loss: 0.0719\n",
      "Epoch [6/10], Batch [257/443], Loss: 0.1300\n",
      "Epoch [6/10], Batch [258/443], Loss: 0.0625\n",
      "Epoch [6/10], Batch [259/443], Loss: 0.0826\n",
      "Epoch [6/10], Batch [260/443], Loss: 0.0777\n",
      "Epoch [6/10], Batch [261/443], Loss: 0.0667\n",
      "Epoch [6/10], Batch [262/443], Loss: 0.0317\n",
      "Epoch [6/10], Batch [263/443], Loss: 0.0610\n",
      "Epoch [6/10], Batch [264/443], Loss: 0.0882\n",
      "Epoch [6/10], Batch [265/443], Loss: 0.1202\n",
      "Epoch [6/10], Batch [266/443], Loss: 0.1073\n",
      "Epoch [6/10], Batch [267/443], Loss: 0.0373\n",
      "Epoch [6/10], Batch [268/443], Loss: 0.1805\n",
      "Epoch [6/10], Batch [269/443], Loss: 0.0338\n",
      "Epoch [6/10], Batch [270/443], Loss: 0.2379\n",
      "Epoch [6/10], Batch [271/443], Loss: 0.1172\n",
      "Epoch [6/10], Batch [272/443], Loss: 0.0784\n",
      "Epoch [6/10], Batch [273/443], Loss: 0.1157\n",
      "Epoch [6/10], Batch [274/443], Loss: 0.1147\n",
      "Epoch [6/10], Batch [275/443], Loss: 0.0617\n",
      "Epoch [6/10], Batch [276/443], Loss: 0.0607\n",
      "Epoch [6/10], Batch [277/443], Loss: 0.0369\n",
      "Epoch [6/10], Batch [278/443], Loss: 0.0525\n",
      "Epoch [6/10], Batch [279/443], Loss: 0.0608\n",
      "Epoch [6/10], Batch [280/443], Loss: 0.0840\n",
      "Epoch [6/10], Batch [281/443], Loss: 0.1394\n",
      "Epoch [6/10], Batch [282/443], Loss: 0.0431\n",
      "Epoch [6/10], Batch [283/443], Loss: 0.0517\n",
      "Epoch [6/10], Batch [284/443], Loss: 0.2051\n",
      "Epoch [6/10], Batch [285/443], Loss: 0.1289\n",
      "Epoch [6/10], Batch [286/443], Loss: 0.0773\n",
      "Epoch [6/10], Batch [287/443], Loss: 0.0312\n",
      "Epoch [6/10], Batch [288/443], Loss: 0.0864\n",
      "Epoch [6/10], Batch [289/443], Loss: 0.0416\n",
      "Epoch [6/10], Batch [290/443], Loss: 0.0368\n",
      "Epoch [6/10], Batch [291/443], Loss: 0.0320\n",
      "Epoch [6/10], Batch [292/443], Loss: 0.0422\n",
      "Epoch [6/10], Batch [293/443], Loss: 0.0351\n",
      "Epoch [6/10], Batch [294/443], Loss: 0.0609\n",
      "Epoch [6/10], Batch [295/443], Loss: 0.0631\n",
      "Epoch [6/10], Batch [296/443], Loss: 0.0682\n",
      "Epoch [6/10], Batch [297/443], Loss: 0.1766\n",
      "Epoch [6/10], Batch [298/443], Loss: 0.2303\n",
      "Epoch [6/10], Batch [299/443], Loss: 0.0602\n",
      "Epoch [6/10], Batch [300/443], Loss: 0.1255\n",
      "Epoch [6/10], Batch [301/443], Loss: 0.0914\n",
      "Epoch [6/10], Batch [302/443], Loss: 0.1028\n",
      "Epoch [6/10], Batch [303/443], Loss: 0.0241\n",
      "Epoch [6/10], Batch [304/443], Loss: 0.0806\n",
      "Epoch [6/10], Batch [305/443], Loss: 0.0290\n",
      "Epoch [6/10], Batch [306/443], Loss: 0.1043\n",
      "Epoch [6/10], Batch [307/443], Loss: 0.0847\n",
      "Epoch [6/10], Batch [308/443], Loss: 0.0518\n",
      "Epoch [6/10], Batch [309/443], Loss: 0.0708\n",
      "Epoch [6/10], Batch [310/443], Loss: 0.0545\n",
      "Epoch [6/10], Batch [311/443], Loss: 0.0274\n",
      "Epoch [6/10], Batch [312/443], Loss: 0.0611\n",
      "Epoch [6/10], Batch [313/443], Loss: 0.1028\n",
      "Epoch [6/10], Batch [314/443], Loss: 0.1906\n",
      "Epoch [6/10], Batch [315/443], Loss: 0.1064\n",
      "Epoch [6/10], Batch [316/443], Loss: 0.0431\n",
      "Epoch [6/10], Batch [317/443], Loss: 0.0915\n",
      "Epoch [6/10], Batch [318/443], Loss: 0.0328\n",
      "Epoch [6/10], Batch [319/443], Loss: 0.0656\n",
      "Epoch [6/10], Batch [320/443], Loss: 0.0676\n",
      "Epoch [6/10], Batch [321/443], Loss: 0.0516\n",
      "Epoch [6/10], Batch [322/443], Loss: 0.0881\n",
      "Epoch [6/10], Batch [323/443], Loss: 0.0881\n",
      "Epoch [6/10], Batch [324/443], Loss: 0.0821\n",
      "Epoch [6/10], Batch [325/443], Loss: 0.1276\n",
      "Epoch [6/10], Batch [326/443], Loss: 0.0699\n",
      "Epoch [6/10], Batch [327/443], Loss: 0.1187\n",
      "Epoch [6/10], Batch [328/443], Loss: 0.0744\n",
      "Epoch [6/10], Batch [329/443], Loss: 0.1460\n",
      "Epoch [6/10], Batch [330/443], Loss: 0.1585\n",
      "Epoch [6/10], Batch [331/443], Loss: 0.0458\n",
      "Epoch [6/10], Batch [332/443], Loss: 0.1218\n",
      "Epoch [6/10], Batch [333/443], Loss: 0.0572\n",
      "Epoch [6/10], Batch [334/443], Loss: 0.1044\n",
      "Epoch [6/10], Batch [335/443], Loss: 0.0936\n",
      "Epoch [6/10], Batch [336/443], Loss: 0.0352\n",
      "Epoch [6/10], Batch [337/443], Loss: 0.0555\n",
      "Epoch [6/10], Batch [338/443], Loss: 0.0715\n",
      "Epoch [6/10], Batch [339/443], Loss: 0.0543\n",
      "Epoch [6/10], Batch [340/443], Loss: 0.0971\n",
      "Epoch [6/10], Batch [341/443], Loss: 0.0335\n",
      "Epoch [6/10], Batch [342/443], Loss: 0.0497\n",
      "Epoch [6/10], Batch [343/443], Loss: 0.1236\n",
      "Epoch [6/10], Batch [344/443], Loss: 0.1280\n",
      "Epoch [6/10], Batch [345/443], Loss: 0.0629\n",
      "Epoch [6/10], Batch [346/443], Loss: 0.0348\n",
      "Epoch [6/10], Batch [347/443], Loss: 0.0758\n",
      "Epoch [6/10], Batch [348/443], Loss: 0.1059\n",
      "Epoch [6/10], Batch [349/443], Loss: 0.0371\n",
      "Epoch [6/10], Batch [350/443], Loss: 0.1233\n",
      "Epoch [6/10], Batch [351/443], Loss: 0.0638\n",
      "Epoch [6/10], Batch [352/443], Loss: 0.0320\n",
      "Epoch [6/10], Batch [353/443], Loss: 0.0370\n",
      "Epoch [6/10], Batch [354/443], Loss: 0.0583\n",
      "Epoch [6/10], Batch [355/443], Loss: 0.0732\n",
      "Epoch [6/10], Batch [356/443], Loss: 0.0306\n",
      "Epoch [6/10], Batch [357/443], Loss: 0.0996\n",
      "Epoch [6/10], Batch [358/443], Loss: 0.0598\n",
      "Epoch [6/10], Batch [359/443], Loss: 0.0593\n",
      "Epoch [6/10], Batch [360/443], Loss: 0.0644\n",
      "Epoch [6/10], Batch [361/443], Loss: 0.0814\n",
      "Epoch [6/10], Batch [362/443], Loss: 0.2373\n",
      "Epoch [6/10], Batch [363/443], Loss: 0.0612\n",
      "Epoch [6/10], Batch [364/443], Loss: 0.0958\n",
      "Epoch [6/10], Batch [365/443], Loss: 0.0561\n",
      "Epoch [6/10], Batch [366/443], Loss: 0.0390\n",
      "Epoch [6/10], Batch [367/443], Loss: 0.0370\n",
      "Epoch [6/10], Batch [368/443], Loss: 0.0303\n",
      "Epoch [6/10], Batch [369/443], Loss: 0.0923\n",
      "Epoch [6/10], Batch [370/443], Loss: 0.0761\n",
      "Epoch [6/10], Batch [371/443], Loss: 0.1627\n",
      "Epoch [6/10], Batch [372/443], Loss: 0.1619\n",
      "Epoch [6/10], Batch [373/443], Loss: 0.0494\n",
      "Epoch [6/10], Batch [374/443], Loss: 0.1319\n",
      "Epoch [6/10], Batch [375/443], Loss: 0.0586\n",
      "Epoch [6/10], Batch [376/443], Loss: 0.0611\n",
      "Epoch [6/10], Batch [377/443], Loss: 0.0828\n",
      "Epoch [6/10], Batch [378/443], Loss: 0.0667\n",
      "Epoch [6/10], Batch [379/443], Loss: 0.1232\n",
      "Epoch [6/10], Batch [380/443], Loss: 0.0479\n",
      "Epoch [6/10], Batch [381/443], Loss: 0.0701\n",
      "Epoch [6/10], Batch [382/443], Loss: 0.0337\n",
      "Epoch [6/10], Batch [383/443], Loss: 0.1492\n",
      "Epoch [6/10], Batch [384/443], Loss: 0.1099\n",
      "Epoch [6/10], Batch [385/443], Loss: 0.0513\n",
      "Epoch [6/10], Batch [386/443], Loss: 0.0509\n",
      "Epoch [6/10], Batch [387/443], Loss: 0.0287\n",
      "Epoch [6/10], Batch [388/443], Loss: 0.0432\n",
      "Epoch [6/10], Batch [389/443], Loss: 0.0738\n",
      "Epoch [6/10], Batch [390/443], Loss: 0.0656\n",
      "Epoch [6/10], Batch [391/443], Loss: 0.1478\n",
      "Epoch [6/10], Batch [392/443], Loss: 0.0700\n",
      "Epoch [6/10], Batch [393/443], Loss: 0.0993\n",
      "Epoch [6/10], Batch [394/443], Loss: 0.0961\n",
      "Epoch [6/10], Batch [395/443], Loss: 0.0225\n",
      "Epoch [6/10], Batch [396/443], Loss: 0.0768\n",
      "Epoch [6/10], Batch [397/443], Loss: 0.0673\n",
      "Epoch [6/10], Batch [398/443], Loss: 0.0799\n",
      "Epoch [6/10], Batch [399/443], Loss: 0.0300\n",
      "Epoch [6/10], Batch [400/443], Loss: 0.1209\n",
      "Epoch [6/10], Batch [401/443], Loss: 0.1131\n",
      "Epoch [6/10], Batch [402/443], Loss: 0.0817\n",
      "Epoch [6/10], Batch [403/443], Loss: 0.0525\n",
      "Epoch [6/10], Batch [404/443], Loss: 0.0327\n",
      "Epoch [6/10], Batch [405/443], Loss: 0.0443\n",
      "Epoch [6/10], Batch [406/443], Loss: 0.0516\n",
      "Epoch [6/10], Batch [407/443], Loss: 0.0598\n",
      "Epoch [6/10], Batch [408/443], Loss: 0.0704\n",
      "Epoch [6/10], Batch [409/443], Loss: 0.0314\n",
      "Epoch [6/10], Batch [410/443], Loss: 0.0876\n",
      "Epoch [6/10], Batch [411/443], Loss: 0.1243\n",
      "Epoch [6/10], Batch [412/443], Loss: 0.0503\n",
      "Epoch [6/10], Batch [413/443], Loss: 0.0415\n",
      "Epoch [6/10], Batch [414/443], Loss: 0.1442\n",
      "Epoch [6/10], Batch [415/443], Loss: 0.1077\n",
      "Epoch [6/10], Batch [416/443], Loss: 0.1735\n",
      "Epoch [6/10], Batch [417/443], Loss: 0.0975\n",
      "Epoch [6/10], Batch [418/443], Loss: 0.0335\n",
      "Epoch [6/10], Batch [419/443], Loss: 0.0465\n",
      "Epoch [6/10], Batch [420/443], Loss: 0.1881\n",
      "Epoch [6/10], Batch [421/443], Loss: 0.0747\n",
      "Epoch [6/10], Batch [422/443], Loss: 0.0236\n",
      "Epoch [6/10], Batch [423/443], Loss: 0.2557\n",
      "Epoch [6/10], Batch [424/443], Loss: 0.0765\n",
      "Epoch [6/10], Batch [425/443], Loss: 0.0368\n",
      "Epoch [6/10], Batch [426/443], Loss: 0.1718\n",
      "Epoch [6/10], Batch [427/443], Loss: 0.0648\n",
      "Epoch [6/10], Batch [428/443], Loss: 0.0424\n",
      "Epoch [6/10], Batch [429/443], Loss: 0.0787\n",
      "Epoch [6/10], Batch [430/443], Loss: 0.0922\n",
      "Epoch [6/10], Batch [431/443], Loss: 0.0263\n",
      "Epoch [6/10], Batch [432/443], Loss: 0.0688\n",
      "Epoch [6/10], Batch [433/443], Loss: 0.1058\n",
      "Epoch [6/10], Batch [434/443], Loss: 0.1045\n",
      "Epoch [6/10], Batch [435/443], Loss: 0.0799\n",
      "Epoch [6/10], Batch [436/443], Loss: 0.1519\n",
      "Epoch [6/10], Batch [437/443], Loss: 0.0987\n",
      "Epoch [6/10], Batch [438/443], Loss: 0.1770\n",
      "Epoch [6/10], Batch [439/443], Loss: 0.3008\n",
      "Epoch [6/10], Batch [440/443], Loss: 0.1350\n",
      "Epoch [6/10], Batch [441/443], Loss: 0.1046\n",
      "Epoch [6/10], Batch [442/443], Loss: 0.0427\n",
      "Epoch [6/10], Batch [443/443], Loss: 0.2244\n",
      "Epoch [7/10], Average Loss: 0.0822\n",
      "Model saved at ./saved_models/model_epoch_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.model.to(device)\n",
    "model.configure_optimizers()\n",
    "# for i, batch in enumerate(dataloader):\n",
    "#     # x, y = batch                      moved to training_step\n",
    "#     # y_hat = model(x)                  moved to training_step\n",
    "#     # loss = loss_function(y_hat, y)    moved to training_step\n",
    "#     loss = lightning_module.training_step(batch, i)\n",
    "\n",
    "#     # Lighting handles automatically:\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "max_epochs = 10\n",
    "train_dataloader = model.train_dataloader()\n",
    "val_dataloader = model.val_dataloader()\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "    # 计算并打印平均损失值\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{max_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "    # model.training_epoch_end(outputs) # outputs是什么\n",
    "\n",
    "    save_dir = f\"./saved_models/model_epoch_{epoch + 1}\"\n",
    "    model.model.save_pretrained(save_dir)\n",
    "    print(f\"Model saved at {save_dir}\")\n",
    "\n",
    "    # 在验证集上计算损失\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        val_batch = {key: value.to(device) for key, value in val_batch.items()}  # 将批次数据移动到GPU上\n",
    "        val_loss += model.validation_step(val_batch, i).item()\n",
    "        num_val_batches += 1\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}  # 将批次数据移动到GPU上\n",
    "        loss = model.training_step(batch, i)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 打印每个批次的损失值\n",
    "        print(f\"Epoch [{epoch + 1}/{max_epochs}], Batch [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 执行优化步骤\n",
    "        model.optimizer_step()\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:49.405585Z",
     "iopub.status.busy": "2024-02-28T14:16:49.405011Z",
     "iopub.status.idle": "2024-02-28T14:16:49.410174Z",
     "shell.execute_reply": "2024-02-28T14:16:49.409675Z",
     "shell.execute_reply.started": "2024-02-28T14:16:49.405565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback():\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:53.624799Z",
     "iopub.status.busy": "2024-02-28T14:16:53.624457Z",
     "iopub.status.idle": "2024-02-28T14:16:53.629524Z",
     "shell.execute_reply": "2024-02-28T14:16:53.629044Z",
     "shell.execute_reply.started": "2024-02-28T14:16:53.624778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=args.output_dir+\"/checkpoint.pth\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    #early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    #amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "# train_params = dict(\n",
    "#     accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "#     ## gpus=args.n_gpu,\n",
    "#     max_epochs=args.num_train_epochs,\n",
    "#     #early_stop_callback=False,\n",
    "#     precision= 16 if args.fp_16 else 32,\n",
    "#     #amp_level=args.opt_level,\n",
    "#     gradient_clip_val=args.max_grad_norm,\n",
    "#     # callbacks=[LoggingCallback()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:59.154405Z",
     "iopub.status.busy": "2024-02-28T14:16:59.153852Z",
     "iopub.status.idle": "2024-02-28T14:16:59.157263Z",
     "shell.execute_reply": "2024-02-28T14:16:59.156780Z",
     "shell.execute_reply.started": "2024-02-28T14:16:59.154380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:17:03.095693Z",
     "iopub.status.busy": "2024-02-28T14:17:03.095368Z",
     "iopub.status.idle": "2024-02-28T14:17:03.102459Z",
     "shell.execute_reply": "2024-02-28T14:17:03.102015Z",
     "shell.execute_reply.started": "2024-02-28T14:17:03.095673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fb6a2a8c9d0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fb6a2a8c9d0>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:17:04.889255Z",
     "iopub.status.busy": "2024-02-28T14:17:04.888686Z",
     "iopub.status.idle": "2024-02-28T14:54:53.783173Z",
     "shell.execute_reply": "2024-02-28T14:54:53.782617Z",
     "shell.execute_reply.started": "2024-02-28T14:17:04.889232Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T15:15:30.833864Z",
     "iopub.status.busy": "2024-02-28T15:15:30.833522Z",
     "iopub.status.idle": "2024-02-28T15:15:37.034017Z",
     "shell.execute_reply": "2024-02-28T15:15:37.033415Z",
     "shell.execute_reply.started": "2024-02-28T15:15:30.833841Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.load_from_checkpoint(\"/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt\")\n",
    "model = model.model.from_pretrained(\"/mnt/workspace/ORL/saved_models/model_epoch_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "dataloader = DataLoader(input_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:15:39.989894Z",
     "iopub.status.busy": "2024-02-28T15:15:39.989324Z",
     "iopub.status.idle": "2024-02-28T15:15:53.553095Z",
     "shell.execute_reply": "2024-02-28T15:15:53.552563Z",
     "shell.execute_reply.started": "2024-02-28T15:15:39.989874Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: european countries are critical of bush 's  axis of evil '' remark . dse:are critical\n",
      "\n",
      "Actual Entities: agent: european countries; target: bush 's  axis of evil '' remark\n",
      "Predicted Entities: agent: european countries; target: bush 's  axis of evil\n",
      "=====================================================================\n",
      "\n",
      "text: there were two other similar episodes of permission being denied before that since the 1997\n",
      "handover of the former british colony to china . dse:permission being denied\n",
      "\n",
      "Actual Entities: \n",
      "Predicted Entities: \n",
      "=====================================================================\n",
      "\n",
      "text: ever since the president announced the  axis of evil '' statement in january , iran , iraq ,\n",
      "north korea have been trying hard , out of the fear of possible confrontation with the united states\n",
      ", to deny their involvement in terrorism or proliferation of mass destruction weapons . dse:to deny\n",
      "\n",
      "Actual Entities: target: their involvement in terrorism or proliferation of mass destruction weapons\n",
      "Predicted Entities: agent: iran , iraq , north korea\n",
      "=====================================================================\n",
      "\n",
      "text: paris , july 11 -lrb- cna -rrb- -- taiwan 's economy will become totally dependent on mainland\n",
      "china within the next few years , causing the island to lose control of its own political fate , a\n",
      "french weekly warned thursday . dse:warned\n",
      "\n",
      "Actual Entities: target: taiwan 's economy will become totally dependent on mainland china within the next few years; agent: french weekly\n",
      "Predicted Entities: target: taiwan 's economy will become totally dependent on mainland china within\n",
      "=====================================================================\n",
      "\n",
      "text: khatib , who also chairs the forum of arab council heads , said the leadership is\n",
      "diametrically opposed to any cutbacks , particularly when those affecting the lower socio-economic\n",
      "strata of society . dse:is diametrically opposed\n",
      "\n",
      "Actual Entities: agent: the leadership; target: any cutbacks , particularly when those affecting the lower socio-economic strata of society\n",
      "Predicted Entities: agent: the leadership; target: any cutbacks\n",
      "=====================================================================\n",
      "\n",
      "text: without attributing consciousness to an e. coli , or an autonomous agent we may create in the\n",
      "near future , i can not help but feel that the rudiments of value are present once autonomous agents\n",
      "are around . dse:feel\n",
      "\n",
      "Actual Entities: target: without attributing consciousness to an e. coli , or an autonomous agent we may create in the near future; agent: i\n",
      "Predicted Entities: agent: i; target: the rudiments of value\n",
      "=====================================================================\n",
      "\n",
      "text: in the case of objections to the christmas island space station , indonesia would need to be\n",
      "able to show that damage that has occurred within its territorial area was caused by space objects\n",
      "launched from the christmas island station . dse:objections\n",
      "\n",
      "Actual Entities: target: the christmas island space station\n",
      "Predicted Entities: target: the christmas island space station\n",
      "=====================================================================\n",
      "\n",
      "text: bush sparked international outcry when he rejected the kyoto protocol last march , saying it\n",
      "would mean sacrificing u.s. economic growth . dse:outcry\n",
      "\n",
      "Actual Entities: agent: international\n",
      "Predicted Entities: agent: international\n",
      "=====================================================================\n",
      "\n",
      "text: this march , catering to the interests of the us oil and coal barons who have made large\n",
      "political donations to the republican party , bush announced , without prior consultations with its\n",
      "allies , that the united states would not enforce the  kyoto protocol '' which is aimed at reducing\n",
      "greenhouse gas emissions in the world . dse:interests\n",
      "\n",
      "Actual Entities: agent: the us oil and coal barons\n",
      "Predicted Entities: agent: the us oil and coal barons\n",
      "=====================================================================\n",
      "\n",
      "text: they make this request  convinced of the russian federation 's resolute commitment to the\n",
      "kyoto protocol '' and believing that in this manner the eu and russia would continue to show the\n",
      "leadership '' which they believe they have shared  with such fruitful results , in international\n",
      "cooperation in the fight against climate change '' . dse:make this request\n",
      "\n",
      "Actual Entities: agent: they\n",
      "Predicted Entities: agent: they\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "targets = []\n",
    "texts = []\n",
    "cnt = 0\n",
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    texts.extend(text)\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    break\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    c = texts[i]\n",
    "    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual Entities: %s\" % target[i])\n",
    "    print(\"Predicted Entities: %s\" % outputs[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.keys():  odict_keys(['sequences', 'scores', 'past_key_values'])\n",
      "outputs.scores[0].shape: torch.Size([2, 32128])\n",
      "len(outputs.scores): 13\n",
      "outputs.scores: torch.Size([2, 32128])\n",
      "ddddddddddddddddddddddddddddddddddddd tensor(-22.2044)\n",
      "len(transition_scores[0]):  13\n",
      "transition_scores:  tensor([[-7.3570e-02, -1.5182e-04, -1.8435e-02, -1.8597e-01, -1.7682e-03,\n",
      "         -9.1794e-05, -1.1499e-02, -1.4008e-04, -2.0066e-01, -2.2774e-02,\n",
      "         -1.1470e-03, -4.0414e-04, -5.3514e-03],\n",
      "        [-9.9976e-02, -7.1911e-05, -1.6243e-02, -1.4897e-01, -1.2826e-04,\n",
      "         -1.0216e-04, -2.3009e-03, -3.8949e-05, -8.3902e-03, -5.1038e+01,\n",
      "         -2.2564e+01, -2.2113e+01, -2.1953e+01]])\n",
      "outputs.sequences:  tensor([[   0, 3102,   10,   27,  117, 2387,   10,    8,  568,  113,  114,    7,\n",
      "          140,    1],\n",
      "        [   0, 3102,   10,  216,  117, 2387,   10,  112, 2512,    1,    0,    0,\n",
      "            0,    0]])\n",
      "len(generated_tokens[0]): 13\n",
      "generated_tokens:  tensor([[3102,   10,   27,  117, 2387,   10,    8,  568,  113,  114,    7,  140,\n",
      "            1],\n",
      "        [3102,   10,  216,  117, 2387,   10,  112, 2512,    1,    0,    0,    0,\n",
      "            0]])\n",
      "|  3102 | agent    | -0.0736 |92.91%\n",
      "|    10 | :        | -0.0002 |99.98%\n",
      "|    27 | I        | -0.0184 |98.17%\n",
      "|   117 | ;        | -0.1860 |83.03%\n",
      "|  2387 | target   | -0.0018 |99.82%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|     8 | the      | -0.0115 |98.86%\n",
      "|   568 | person   | -0.0001 |99.99%\n",
      "|   113 | who      | -0.2007 |81.82%\n",
      "|   114 | like     | -0.0228 |97.75%\n",
      "|     7 | s        | -0.0011 |99.89%\n",
      "|   140 | me       | -0.0004 |99.96%\n",
      "|     1 | </s>     | -0.0054 |99.47%\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.1000 |90.49%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|   216 | He       | -0.0162 |98.39%\n",
      "|   117 | ;        | -0.1490 |86.16%\n",
      "|  2387 | target   | -0.0001 |99.99%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|   112 | his      | -0.0023 |99.77%\n",
      "|  2512 | wife     | -0.0000 |100.00%\n",
      "|     1 | </s>     | -0.0084 |99.16%\n",
      "|     0 | <pad>    | -51.0377 |0.00%\n",
      "|     0 | <pad>    | -22.5637 |0.00%\n",
      "|     0 | <pad>    | -22.1130 |0.00%\n",
      "|     0 | <pad>    | -21.9531 |0.00%\n",
      "----------------------------------------------\n",
      "agent: I; target: the person who likes me\n"
     ]
    }
   ],
   "source": [
    "# search:how to get the probability huggingface\n",
    "# https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075\n",
    "\n",
    "# 设置输入句子和相关参数\n",
    "input_text = \"I love the person who likes me. dse:love\"\n",
    "input_texts = [\"I love the person who likes me. dse:love\",\"He loves his wife. dse:loves\"]\n",
    "max_length = 32\n",
    "temperature = 1.0\n",
    "num_samples = 1\n",
    "\n",
    "# 推理\n",
    "# inputs = tokenizer([input_text], return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    input_texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.model.generate(**inputs,return_dict_in_generate=True,output_scores=True)\n",
    "# print(outputs)\n",
    "print(\"outputs.keys(): \",outputs.keys())\n",
    "print(\"outputs.scores[0].shape:\",outputs.scores[0].shape)\n",
    "print(\"len(outputs.scores):\",len(outputs.scores))\n",
    "#output.scores是一个长度为输出的token数量的元组，元组中的每个元素为batchsize*词表大小的tensor\n",
    "print(\"outputs.scores[0].shape:\",outputs.scores[0].shape) \n",
    "print(\"ddddddddddddddddddddddddddddddddddddd\",outputs.scores[0][0][0])\n",
    "\n",
    "transition_scores = model.model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "print(\"len(transition_scores[0]): \", len(transition_scores[0]))\n",
    "print(\"transition_scores: \", transition_scores)\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "# 由于t5是encoder-decoder架构，所以这里input_length取1\n",
    "generated_tokens = outputs.sequences[:, 1:] \n",
    "print(\"outputs.sequences: \", outputs.sequences)\n",
    "print(\"len(generated_tokens[0]):\",len(generated_tokens[0]))\n",
    "print(\"generated_tokens: \",generated_tokens)\n",
    "for i in range(generated_tokens.shape[0]):\n",
    "    for tok, score in zip(generated_tokens[i], transition_scores[i]):\n",
    "        # | token | token string | logits | probability\n",
    "        print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.4f} |{np.exp(score.numpy()):.2%}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  2387,    10,     8, 23997,  3368,   628,  2478,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,     3,  1258,\n",
      "           189,    63,   410,    48, 24522,     1,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8,   126, 25453,   648,     3,     6, 10381,\n",
      "         11831,    15,     7,   648,    11,  6179,  6029,   442,   117,  2387],\n",
      "        [    0,  2387,    10,     8,     3,   102,    29,   102,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,  7609,   261,    12,     8,  1075,    13,  1291,\n",
      "          4028,   127,     7,   117,  2387,    10,  4297,  3101,    84,  4840],\n",
      "        [    0,  3102,    10,   112,   117,  2387,    10, 26504,   323,    30,\n",
      "             8,   962,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,     9,  1021,  1370,   117,  2387,    10,\n",
      "          4326,  2856,   840,     3,     6,  2425,   871,    11,  8033,     3],\n",
      "        [    0,  3102,    10,  1440,   117,  2387,    10,     8,     3,  1795,\n",
      "          2420,    13,     8,     3,  3781,    32,   235, 10015,     1,     0],\n",
      "        [    0,  3102,    10,     8,     3, 26165,    29,     3, 16812,   257,\n",
      "           117,  2387,    10,     8,     3,  3781,    32,   235, 10015,     1],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,     3,    23,\n",
      "            47,   838,  1456,     7,     1,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8, 14864,  7021,   117,  2387,    10,   487,\n",
      "          1151,  7774,  3629,   581,   165,   789,     1,     0,     0,     0],\n",
      "        [    0,  2387,    10,     8,   711,  5492,    76,  1222,  4373,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,  2781,     7,  5003,   117,  2387,    10,\n",
      "           662,   931,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   381,    13, 28480,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,     3, 23064,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,   112, 12914, 12028,    45,  6525,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    62,   117,  2387,    10,     8,  6032,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   178,     9,   469,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,     8,  7648,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    79,   117,  2387,    10,     8,     3,  3781,\n",
      "            32,   235, 10015,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,  2753, 17907,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,   125,     3,\n",
      "            23,   103,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,   255,     3,    31,     7,  4896,   117,  2387,\n",
      "            10,   255,     3,    31,     7,   182,  2592,     3,     6,    11],\n",
      "        [    0,  3102,    10,  4942,   117,  2387,    10,     8,     3,  1033,\n",
      "          1559, 12315,  2426,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,    21,     8,     3,    76,     5,     7,     5,\n",
      "          2716,    12,  2665,     8, 22040,   136, 10302,   117,  3102,    10],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,     8,  1296,\n",
      "           779,  5217, 20576,    56, 11230,     1,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,     3,    17, 31081,   117,  3102,    10,     8,\n",
      "         12392,    13,     8,     3,    32,    17,    17,  7396, 13008, 14829],\n",
      "        [    0,  3102,    10,     3,  2781,     7,  5003,   117,  2387,    10,\n",
      "             3,    76,     5,     7,     5,  1058,    13,    20, 15354,    15],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,  4028,   112,\n",
      "          1291,    13, 17323,  9569,     1,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    11,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   126,     3,   776,   138,   232,  2959, 12748,\n",
      "          6323,     3, 18118,     3,   122,  1647,   117,  2387,    10,   126],\n",
      "        [    0,  3102,    10,   452,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "target: the christmas island space station\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    # dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    # target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "    #             for ids in batch[\"target_ids\"]]\n",
    "    # text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "    #             for ids in batch[\"source_ids\"]]\n",
    "    # texts.extend(text)\n",
    "    # outputs.extend(dec)\n",
    "    # targets.extend(target)\n",
    "    # cnt += 1\n",
    "    # if cnt > 10:\n",
    "    #     break\n",
    "    print(outs)\n",
    "    print(tokenizer.decode(outs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  2387,    10,     8, 23997,  3368,   628,  2478,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "target: the christmas island space station\n",
      "tensor([    0,  3102,    10,     3,    23,   117,  2387,    10,     3,  1258,\n",
      "          189,    63,   410,    48, 24522,     1,     0,     0,     0,     0])\n",
      "agent: i; target: kathy did this intentionally\n",
      "tensor(0)\n",
      "\n",
      "tensor(3102)\n",
      "agent\n",
      "tensor(10)\n",
      ":\n",
      "tensor(3)\n",
      "\n",
      "tensor(23)\n",
      "i\n",
      "tensor(117)\n",
      ";\n",
      "tensor(2387)\n",
      "target\n",
      "tensor(10)\n",
      ":\n",
      "tensor(3)\n",
      "\n",
      "tensor(1258)\n",
      "ka\n",
      "tensor(189)\n",
      "th\n",
      "tensor(63)\n",
      "y\n",
      "tensor(410)\n",
      "did\n",
      "tensor(48)\n",
      "this\n",
      "tensor(24522)\n",
      "intentionally\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])\n",
    "print(tokenizer.decode(outs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "\n",
    "print(outs[1])\n",
    "print(tokenizer.decode(outs[1], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "for i in range(15):\n",
    "    print(outs[1][i])\n",
    "    print(tokenizer.decode(outs[1][i], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:16:17.579251Z",
     "iopub.status.busy": "2024-02-28T15:16:17.578902Z",
     "iopub.status.idle": "2024-02-28T15:16:17.585178Z",
     "shell.execute_reply": "2024-02-28T15:16:17.584743Z",
     "shell.execute_reply.started": "2024-02-28T15:16:17.579228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_sub_list(sl, l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind:ind+sll] == sl:\n",
    "            results.append((ind, ind+sll-1))\n",
    "    return results\n",
    "\n",
    "def generate_label(input: str, target: str):\n",
    "    mapper = {\n",
    "        \"O\": 0,\n",
    "        \"B-AGENT\": 1,\n",
    "        \"I-AGENT\": 2,\n",
    "        \"B-DSE\": 3,\n",
    "        \"I-DSE\": 4,\n",
    "        \"B-TARGET\": 5,\n",
    "        \"I-TARGET\": 6\n",
    "    }\n",
    "    inv_mapper = {v: k for k, v in mapper.items()}\n",
    "\n",
    "    input = input.split(\" \")\n",
    "    target = target.split(\"; \")\n",
    "\n",
    "    init_target_label = [mapper['O']]*len(input)\n",
    "\n",
    "    for ent in target:\n",
    "        ent = ent.split(\": \")\n",
    "        try:\n",
    "            sent_end = ent[1].split(\" \")\n",
    "            index = find_sub_list(sent_end, input)\n",
    "        except:\n",
    "            continue\n",
    "        # print(index)\n",
    "        try:\n",
    "            init_target_label[index[0][0]] = mapper[f\"B-{ent[0].upper()}\"]\n",
    "            for i in range(index[0][0]+1, index[0][1]+1):\n",
    "                init_target_label[i] = mapper[f\"I-{ent[0].upper()}\"]\n",
    "        except:\n",
    "            continue\n",
    "    init_target_label = [inv_mapper[j] for j in init_target_label]\n",
    "    return init_target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:16:22.561581Z",
     "iopub.status.busy": "2024-02-28T15:16:22.560990Z",
     "iopub.status.idle": "2024-02-28T15:16:57.335332Z",
     "shell.execute_reply": "2024-02-28T15:16:57.334689Z",
     "shell.execute_reply.started": "2024-02-28T15:16:22.561561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 48/48 [00:33<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32,\n",
    "                             num_workers=2, shuffle=True)\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "outputs = []\n",
    "targets = []\n",
    "all_text = []\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['source_ids'].to(\"cuda\")\n",
    "    attention_mask = batch['source_mask'].to(\"cuda\")\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask)\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "    pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    true_labels.extend(true_label)\n",
    "    pred_labels.extend(pred_label)\n",
    "    all_text.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:17:33.521899Z",
     "iopub.status.busy": "2024-02-28T15:17:33.521562Z",
     "iopub.status.idle": "2024-02-28T15:17:35.880177Z",
     "shell.execute_reply": "2024-02-28T15:17:35.879656Z",
     "shell.execute_reply.started": "2024-02-28T15:17:33.521876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22079/1300780915.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  however , he ruled out that the statement by the us foreign secretary will lead to a change in the policies of president chavez . dse:ruled out\n",
      "Predicted Token Class:  ['O', 'O', 'B-AGENT', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'B-AGENT', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  that 's my opinion on it so what now dse:opinion\n",
      "Predicted Token Class:  ['B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-TARGET', 'I-TARGET', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  the president has called for a peaceful resolution of the lingering sovereignty dispute between taipei and beijing which split at the end of a civil war in 1949 . dse:called for\n",
      "Predicted Token Class:  ['B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  it seems our critics would prefer that the taliban and al qaeda terrorists instead stay at the ritz-carlton . dse:would prefer\n",
      "Predicted Token Class:  ['O', 'O', 'B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  a senior government official said :  i sympathize with those who want change in the presidential elections but judging by the groundwork that zanu pf is doing to deal with the opposition , i am afraid to say that i do n't see that change coming . '' dse:want change\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  they are learning to make positive choices concerning alcohol , tobacco and other drugs and to support each other when those choices are challenged . dse:learning\n",
      "Predicted Token Class:  ['B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  however , such a criticism is absent in the 2001 human rights report as the united states is planning to try the al-qa  ida and taliban detainees in military courts while refusing to grant them pow status . dse:such a criticism\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'I-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  instead of blindly accepting every demand made by the united states and responding positively to every american phone call , pakistan should take every single step carefully . dse:responding positively\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  parmentier said , on the one hand , the wto claims to support sustainable development and environmental protection and on the other , it does virtually nothing to protect the environment . dse:claims\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'I-AGENT', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  in a press conference on the sidelines of the current full session of parliament -- his annual opportunity to present china 's foreign policy objectives to the outside world -- tang largely steered clear of subjects over which beijing and washington disagree . dse:foreign policy objectives\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "{'AGENT': {'precision': 0.7254264825345248, 'recall': 0.7491610738255033, 'f1': 0.7371027651671481, 'number': 1192}, 'TARGET': {'precision': 0.48704663212435234, 'recall': 0.44940239043824703, 'f1': 0.4674678823041857, 'number': 1255}, 'overall_precision': 0.6098786102971955, 'overall_recall': 0.5954229668982427, 'overall_f1': 0.6025641025641025, 'overall_accuracy': 0.8704737629971275}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Text:  {all_text[i]}\")\n",
    "    print(f\"Predicted Token Class:  {pred_labels[i]}\")\n",
    "    print(f\"True Token Class:  {true_labels[i]}\")\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n",
    "print(metric.compute(predictions=pred_labels, references=true_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h8R2bLDhjlc"
   },
   "source": [
    "# Model\n",
    "\n",
    "Majority of the code here is adapted from [here](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) which uses the pytorch-lightning framework for training neural networks. T5 has shown that it can generate state of the art on many tasks as long as it can be cast as a text-to-text problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:24:59.853431Z",
     "iopub.status.busy": "2024-02-27T12:24:59.853050Z",
     "iopub.status.idle": "2024-02-27T12:24:59.864657Z",
     "shell.execute_reply": "2024-02-27T12:24:59.863958Z",
     "shell.execute_reply.started": "2024-02-27T12:24:59.853410Z"
    },
    "id": "KL8_p4YS6H0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparam):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparam = hparam\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparam.model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            hparam.model_name_or_path\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparam.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparam.learning_rate, eps=self.hparam.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self,\n",
    "                       epoch=None,\n",
    "                       batch_idx=None,\n",
    "                       optimizer=None,\n",
    "                       optimizer_idx=None,\n",
    "                       optimizer_closure=None,\n",
    "                       on_tpu=None,\n",
    "                       using_native_amp=None,\n",
    "                       using_lbfgs=None\n",
    "                       ):\n",
    "\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(\n",
    "            self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparam.train_batch_size,\n",
    "                                drop_last=True, shuffle=True, num_workers=2)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) //\n",
    "             (self.hparam.train_batch_size * max(1, self.hparam.n_gpu)))\n",
    "            // self.hparam.gradient_accumulation_steps\n",
    "            * float(self.hparam.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparam.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:01.768401Z",
     "iopub.status.busy": "2024-02-27T12:25:01.767846Z",
     "iopub.status.idle": "2024-02-27T12:25:01.773286Z",
     "shell.execute_reply": "2024-02-27T12:25:01.772660Z",
     "shell.execute_reply.started": "2024-02-27T12:25:01.768381Z"
    },
    "id": "6VQpUMNe6Wf8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:02.692693Z",
     "iopub.status.busy": "2024-02-27T12:25:02.692356Z",
     "iopub.status.idle": "2024-02-27T12:25:02.697059Z",
     "shell.execute_reply": "2024-02-27T12:25:02.696544Z",
     "shell.execute_reply.started": "2024-02-27T12:25:02.692672Z"
    },
    "id": "I55QMghp6YbE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"wikiann\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-small',\n",
    "    tokenizer_name_or_path='t5-small',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxV5l0Wcinfb"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Here, I used the popular [WikiANN](https://https://huggingface.co/datasets/wikiann) dataset which is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "b112c9a8184d4b479fc0e59698789b43",
      "e550224ab580420cb4973b805c2393af",
      "ebcc288979ff4cd5901e9223063751d1",
      "304c605d63584a298e8cefd4ffcf4ead",
      "7ca93c1fe9a9415e9c7d6f22a58b90d1",
      "56496f0e8cef4b5cb989f289af418f9f",
      "388d8e1e34a143c3afe9e4082be44ebb",
      "5d88a23982ac46b3b8077264a6998b9a",
      "dfdd904fb4924ad8948793222255cc3b",
      "43278de125c54c1e8b55baa0370c4cc1",
      "fa0489927bdd4bf4809c9fcc92d9d7b4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:09.149655Z",
     "iopub.status.busy": "2024-02-27T12:25:09.149329Z",
     "iopub.status.idle": "2024-02-27T12:25:26.708433Z",
     "shell.execute_reply": "2024-02-27T12:25:26.707926Z",
     "shell.execute_reply.started": "2024-02-27T12:25:09.149635Z"
    },
    "id": "soCZS7n07Ts1",
    "outputId": "6e0d1772-838a-414e-b028-e6362faeb653",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.709771Z",
     "iopub.status.busy": "2024-02-27T12:25:26.709381Z",
     "iopub.status.idle": "2024-02-27T12:25:26.713447Z",
     "shell.execute_reply": "2024-02-27T12:25:26.712901Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.709753Z"
    },
    "id": "7eSAir8g80rm",
    "outputId": "aaf53c53-939a-424a-e625-9ba8059fee67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.714288Z",
     "iopub.status.busy": "2024-02-27T12:25:26.713990Z",
     "iopub.status.idle": "2024-02-27T12:25:26.720368Z",
     "shell.execute_reply": "2024-02-27T12:25:26.719900Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.714273Z"
    },
    "id": "cWCXP8F373Nm",
    "outputId": "bc803c3d-6947-4c16-eb68-0fde40b8289c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R.H. Saunders ( St. Lawrence River ) ( 968 MW )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(dataset['train'][0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.721462Z",
     "iopub.status.busy": "2024-02-27T12:25:26.721156Z",
     "iopub.status.idle": "2024-02-27T12:25:26.725372Z",
     "shell.execute_reply": "2024-02-27T12:25:26.724914Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.721445Z"
    },
    "id": "rXjs7Khn9oi6",
    "outputId": "c96fa06e-f32d-4cad-bd85-513ffde91282",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['R.H.',\n",
       "  'Saunders',\n",
       "  '(',\n",
       "  'St.',\n",
       "  'Lawrence',\n",
       "  'River',\n",
       "  ')',\n",
       "  '(',\n",
       "  '968',\n",
       "  'MW',\n",
       "  ')'],\n",
       " 'ner_tags': [3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0],\n",
       " 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
       " 'spans': ['ORG: R.H. Saunders', 'ORG: St. Lawrence River']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5KDQPACi8FT"
   },
   "source": [
    "In this section, we create a custom dataset class where we cast the NER task as a text to text problem. This is done by concatenating the spans in the data as one line of string separated by a semi-colon (;). e.g\n",
    "\n",
    "*   **Input**: R.H. Saunders ( St. Lawrence River ) ( 968 MW )\n",
    "*   **Target**: ORG: R.H. Saunders; ORG: St. Lawrence River\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:28.595917Z",
     "iopub.status.busy": "2024-02-27T12:25:28.595563Z",
     "iopub.status.idle": "2024-02-27T12:25:28.602276Z",
     "shell.execute_reply": "2024-02-27T12:25:28.601623Z",
     "shell.execute_reply.started": "2024-02-27T12:25:28.595896Z"
    },
    "id": "LgZKb7T48Mzw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WikiAnnDataset(Dataset):\n",
    "  def __init__(self, tokenizer, dataset, type_path, max_len=512):\n",
    "\n",
    "    self.data = dataset[type_path]\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.tokenizer.max_length = max_len\n",
    "    self.tokenizer.model_max_length = max_len\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "  def _build(self):\n",
    "    for idx in range(len(self.data)):\n",
    "      input_, target = \" \".join(self.data[idx][\"tokens\"]), \"; \".join(self.data[idx][\"spans\"])\n",
    "\n",
    "      input_ = input_.lower() + ' </s>'\n",
    "      target = target.lower() + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target],max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:33.140341Z",
     "iopub.status.busy": "2024-02-27T12:25:33.140007Z",
     "iopub.status.idle": "2024-02-27T12:25:42.716000Z",
     "shell.execute_reply": "2024-02-27T12:25:42.715500Z",
     "shell.execute_reply.started": "2024-02-27T12:25:33.140323Z"
    },
    "id": "XrlJEayI-4tS",
    "outputId": "97284917-3b24-462e-fd2c-930f0061f79f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5TokenizerFast(name_or_path='../T5-base', vocab_size=32100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../T5-base\")\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "input_dataset = WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:42.717524Z",
     "iopub.status.busy": "2024-02-27T12:25:42.717015Z",
     "iopub.status.idle": "2024-02-27T12:25:42.871971Z",
     "shell.execute_reply": "2024-02-27T12:25:42.871475Z",
     "shell.execute_reply.started": "2024-02-27T12:25:42.717506Z"
    },
    "id": "Cyrnvv1zTKaw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(input_dataset)):\n",
    "    _ = input_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:42.873108Z",
     "iopub.status.busy": "2024-02-27T12:25:42.872825Z",
     "iopub.status.idle": "2024-02-27T12:25:42.876923Z",
     "shell.execute_reply": "2024-02-27T12:25:42.876466Z",
     "shell.execute_reply.started": "2024-02-27T12:25:42.873090Z"
    },
    "id": "OJ02SXgv_UW8",
    "outputId": "5c541c8e-9376-47ed-bc9f-7e138caee0e7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.h. saunders ( st. lawrence river ) ( 968 mw )</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "org: r.h. saunders; org: st. lawrence river</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data = input_dataset[0]\n",
    "\n",
    "print(tokenizer.decode(data[\"source_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(data[\"target_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:43.437406Z",
     "iopub.status.busy": "2024-02-27T12:25:43.437088Z",
     "iopub.status.idle": "2024-02-27T12:25:43.685932Z",
     "shell.execute_reply": "2024-02-27T12:25:43.685362Z",
     "shell.execute_reply.started": "2024-02-27T12:25:43.437387Z"
    },
    "id": "Sepl17_eCHy4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p t5_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:53.021540Z",
     "iopub.status.busy": "2024-02-27T12:25:53.021188Z",
     "iopub.status.idle": "2024-02-27T12:25:54.752236Z",
     "shell.execute_reply": "2024-02-27T12:25:54.751686Z",
     "shell.execute_reply.started": "2024-02-27T12:25:53.021519Z"
    },
    "id": "4Toa_qnXDrTw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:57.604520Z",
     "iopub.status.busy": "2024-02-27T12:25:57.604156Z",
     "iopub.status.idle": "2024-02-27T12:25:57.608734Z",
     "shell.execute_reply": "2024-02-27T12:25:57.608252Z",
     "shell.execute_reply.started": "2024-02-27T12:25:57.604493Z"
    },
    "id": "dIZ3LwE3DXNo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=args.output_dir+\"/checkpoint.pth\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    #early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    #amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "# train_params = dict(\n",
    "#     accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "#     ## gpus=args.n_gpu,\n",
    "#     max_epochs=args.num_train_epochs,\n",
    "#     #early_stop_callback=False,\n",
    "#     precision= 16 if args.fp_16 else 32,\n",
    "#     #amp_level=args.opt_level,\n",
    "#     gradient_clip_val=args.max_grad_norm,\n",
    "#     # callbacks=[LoggingCallback()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:01.158877Z",
     "iopub.status.busy": "2024-02-27T12:26:01.158529Z",
     "iopub.status.idle": "2024-02-27T12:26:01.162217Z",
     "shell.execute_reply": "2024-02-27T12:26:01.161716Z",
     "shell.execute_reply.started": "2024-02-27T12:26:01.158856Z"
    },
    "id": "MxBEgT6MDqe3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    tokenizer.max_length = args.max_seq_length\n",
    "    tokenizer.model_max_length = args.max_seq_length\n",
    "    dataset = load_dataset(args.data_dir, \"en\")\n",
    "    return WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path=type_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:01.731175Z",
     "iopub.status.busy": "2024-02-27T12:26:01.730847Z",
     "iopub.status.idle": "2024-02-27T12:26:01.738097Z",
     "shell.execute_reply": "2024-02-27T12:26:01.737626Z",
     "shell.execute_reply.started": "2024-02-27T12:26:01.731149Z"
    },
    "id": "KCKmlJ5DDaHw",
    "outputId": "e5db4e86-8699-475a-a794-cc03d8b615cc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f6e53eb2f70>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f6e53eb2f70>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445,
     "referenced_widgets": [
      "d2b327e2b0074354a6b889afc169a62e",
      "84ab2eb4f0b74ac3a334eb7ecd7c4d63",
      "f8ebf6d2f2954cbbb525e67de58a766a",
      "a0a3db286a1843b293b1d3b944a3d33a",
      "d68d5a7c97bd47719eb1e19f67d64805",
      "1fac6af61b4d411a94f3916061731428",
      "127c67d7658d4bf392408e7d5600a09c",
      "25b71debb82040a2960eb8348df2a4ca",
      "9428191170994f0aa7fd400caf0d1d07",
      "ef4fa30c44df4726a36cfc1da100fb4f",
      "dffd33c1015f4bfd91fd702095c2127e",
      "ba1298c2f3db42c48b7e3e3ee83d5000",
      "0e46dd857d504def992f555a9e9c9c5f",
      "285649e2ae134f87bd2f8dcf40a7758d",
      "83ec7a9ea11e410bb3fcad13c082d664",
      "5fd2178313d34631a030d79934dcffe0",
      "c3c43f7938044239b4eca004d07874ae",
      "b3e24f0b77884455868b103ab69ddd5d",
      "79385887dd0f479d993b251748f8cb2f",
      "429fbc36c65446d09d32298cfc4b606e",
      "b449b3c3b1b944a38293f2f13d517860",
      "c4dc2a989d794b44b8000ca88cf095fa",
      "97e269fbcb954d629b2314d30c41c57b",
      "84c7612e3ed740958159eec4ee147476",
      "93c2ea7279794b88b6264f832400e3c7",
      "45340f3b0bf4413182060f27c7458334",
      "6bf0078bd11d4ed980f1b7a9b612a091",
      "4fb2b190729943cb8c1f1fd2b84d6935",
      "a5a571c48de54900b2b696f2c6817a2e",
      "966b77941ef046babfb1e9ed597a6aee",
      "f16174670d2c4182ace507dda7812328",
      "e61579f72d51414782f8fe3c28e12e86",
      "dbcd62725c684fcc897583042ca44888",
      "ba6f4922558c41fea4a4b171a021b871",
      "5587ced0255640dd9bec68478cd973fb",
      "48bb2d6d069e411a9e969187d5ddf17d",
      "4fffbd7c89314024a7b8c128ad1ba248",
      "dea06faaa22b41b6bbfd7ff6d8ee39f6",
      "347f2c97cd4f46aa8bd1d30f79041b29",
      "81211b58e69b4973a9e1b500e621656b",
      "ce97e8d1d6b54e17a16ec75e1b3abf7f",
      "5d4e33c5404f49caa4811bfd55b0bee5",
      "d37bf18a5e9346fcb59242e0e5288307",
      "005a930db3d242a4be28a9c5923eed76",
      "0573bd35717d4189ab4d74fdc489653b",
      "06e6627b73a24e4291f316a281a20aea",
      "bd7b7a9f5f80492997ec2cbd4bf22627",
      "1b08c691a8db4cb79e6f7837a08ac799",
      "b904bbebe38943728a23a0d630f86a88",
      "9cd468cf60c643db96d0cd35923c8258",
      "cddc7d98bbb740a9a81bf8e244d4595d",
      "cfcb95376da04ce884ab18c57c2d63ed",
      "f69895f136a14e629614a45f2e7d8557",
      "47144694b20f49f0aa54ee1d83119fda",
      "3898654592194daca51999f513192a24",
      "39ceb75a4fa14f6e9a8a5f4ae1324cbc",
      "49901550f7c14e439505f80faaaa8357",
      "4bba40ddade644b79cce613925a4f47e",
      "aa38ae45e85545bebec6c8b43db7abbf",
      "37a7f3f9244a4fd991fe9e8847268ec1",
      "b537afbec50e4567885220cf27a5b761",
      "ce10ff94a3454674a5deb511394387ff",
      "be10758757c1426c8779c6706a445ecd",
      "d26f30f1432e4f2ab1c53484b44989ee",
      "0305993a3c1a4f8296faeb1aba61fa38",
      "fce9ba2896164d4391fea7f32dc3d811",
      "ffe6027ef4bb4fd9964717116a39f1a3",
      "4b15ff3c6b044b40a15b21f051bf53c8",
      "3ddd6cf350fb4378ad6cf2698f67d68f",
      "ceca8a9a02fe46d5a1a3aa093fdbcde7",
      "20d7306a9b474da3b0c8e80a3d7abe09",
      "3b924323276f4f92a66a64d295ba6c78",
      "21b88aca18f048759fb25ce63540c992",
      "b58e5e54e53e46d7a4f196a93812373c",
      "b849b43f5c4649deb018fb959191edc5",
      "95d7c71216094ea0b466a79efc95341d",
      "264fdba12e854d54b3ed2510458c7a1b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:10.156541Z",
     "iopub.status.busy": "2024-02-27T12:26:10.156209Z",
     "iopub.status.idle": "2024-02-27T12:50:26.838508Z",
     "shell.execute_reply": "2024-02-27T12:50:26.837953Z",
     "shell.execute_reply.started": "2024-02-27T12:26:10.156522Z"
    },
    "id": "sxQ-s0izFQGQ",
    "outputId": "fd7d2eef-efef-4367-dcc3-70ff9e3d18fb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:102: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "121.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3750 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/closure.py:35: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████▋   | 2500/3750 [06:36<03:18,  6.30it/s, loss=0.138, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  67%|██████▋   | 2502/3750 [06:37<03:18,  6.30it/s, loss=0.138, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  67%|██████▋   | 2504/3750 [06:37<03:17,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2506/3750 [06:37<03:17,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2508/3750 [06:37<03:16,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2510/3750 [06:37<03:16,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2512/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2514/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2516/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2518/3750 [06:37<03:14,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2520/3750 [06:38<03:14,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2522/3750 [06:38<03:13,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2524/3750 [06:38<03:13,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2526/3750 [06:38<03:13,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2528/3750 [06:38<03:12,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2530/3750 [06:38<03:12,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2532/3750 [06:38<03:11,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2534/3750 [06:38<03:11,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2536/3750 [06:38<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2538/3750 [06:39<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2540/3750 [06:39<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2542/3750 [06:39<03:09,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2544/3750 [06:39<03:09,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2546/3750 [06:39<03:08,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2548/3750 [06:39<03:08,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2550/3750 [06:39<03:08,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2552/3750 [06:39<03:07,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2554/3750 [06:39<03:07,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2556/3750 [06:40<03:06,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2558/3750 [06:40<03:06,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2560/3750 [06:40<03:06,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2562/3750 [06:40<03:05,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2564/3750 [06:40<03:05,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2566/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2568/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2570/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2572/3750 [06:40<03:03,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2574/3750 [06:41<03:03,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2576/3750 [06:41<03:02,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2578/3750 [06:41<03:02,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2580/3750 [06:41<03:02,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2582/3750 [06:41<03:01,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2584/3750 [06:41<03:01,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2586/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2588/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2590/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2592/3750 [06:42<02:59,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2594/3750 [06:42<02:59,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2596/3750 [06:42<02:58,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2598/3750 [06:42<02:58,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2600/3750 [06:42<02:58,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2602/3750 [06:42<02:57,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2604/3750 [06:42<02:57,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2606/3750 [06:42<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2608/3750 [06:42<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2610/3750 [06:43<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2612/3750 [06:43<02:55,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2614/3750 [06:43<02:55,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2616/3750 [06:43<02:54,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2618/3750 [06:43<02:54,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2620/3750 [06:43<02:54,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2622/3750 [06:43<02:53,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2624/3750 [06:43<02:53,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2626/3750 [06:43<02:52,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2628/3750 [06:44<02:52,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2630/3750 [06:44<02:52,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2632/3750 [06:44<02:51,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2634/3750 [06:44<02:51,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2636/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2638/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2640/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2642/3750 [06:44<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2644/3750 [06:45<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2646/3750 [06:45<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2648/3750 [06:45<02:48,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2650/3750 [06:45<02:48,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2652/3750 [06:45<02:47,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2654/3750 [06:45<02:47,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2656/3750 [06:45<02:47,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2658/3750 [06:45<02:46,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2660/3750 [06:45<02:46,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2662/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2664/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2666/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2668/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2670/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2672/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2674/3750 [06:46<02:43,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2676/3750 [06:46<02:43,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2678/3750 [06:46<02:42,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2680/3750 [06:47<02:42,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2682/3750 [06:47<02:42,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2684/3750 [06:47<02:41,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2686/3750 [06:47<02:41,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2688/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2690/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2692/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2694/3750 [06:47<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2696/3750 [06:47<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2698/3750 [06:48<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2700/3750 [06:48<02:38,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2702/3750 [06:48<02:38,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2704/3750 [06:48<02:37,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2706/3750 [06:48<02:37,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2708/3750 [06:48<02:37,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2710/3750 [06:48<02:36,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2712/3750 [06:48<02:36,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2714/3750 [06:48<02:36,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2716/3750 [06:49<02:35,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2718/3750 [06:49<02:35,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2720/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2722/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2724/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2726/3750 [06:49<02:33,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2728/3750 [06:49<02:33,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2730/3750 [06:49<02:33,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2732/3750 [06:49<02:32,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2734/3750 [06:50<02:32,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2736/3750 [06:50<02:32,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2738/3750 [06:50<02:31,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2740/3750 [06:50<02:31,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2742/3750 [06:50<02:30,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2744/3750 [06:50<02:30,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2746/3750 [06:50<02:30,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2748/3750 [06:50<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2750/3750 [06:50<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2752/3750 [06:51<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2754/3750 [06:51<02:28,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2756/3750 [06:51<02:28,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2758/3750 [06:51<02:27,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2760/3750 [06:51<02:27,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2762/3750 [06:51<02:27,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2764/3750 [06:51<02:26,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2766/3750 [06:51<02:26,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2768/3750 [06:52<02:26,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2770/3750 [06:52<02:25,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2772/3750 [06:52<02:25,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2774/3750 [06:52<02:25,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2776/3750 [06:52<02:24,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2778/3750 [06:52<02:24,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2780/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2782/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2784/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2786/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2788/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2790/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2792/3750 [06:53<02:21,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2794/3750 [06:53<02:21,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2796/3750 [06:53<02:21,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2798/3750 [06:53<02:20,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2800/3750 [06:53<02:20,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2802/3750 [06:53<02:20,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2804/3750 [06:54<02:19,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2806/3750 [06:54<02:19,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2808/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2810/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2812/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2814/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2816/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2818/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2820/3750 [06:54<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2822/3750 [06:55<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2824/3750 [06:55<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2826/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2828/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2830/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2832/3750 [06:55<02:14,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2834/3750 [06:55<02:14,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2836/3750 [06:55<02:14,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2838/3750 [06:55<02:13,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2840/3750 [06:56<02:13,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2842/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2844/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2846/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2848/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2850/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2852/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2854/3750 [06:56<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2856/3750 [06:56<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2858/3750 [06:57<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2860/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2862/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2864/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2866/3750 [06:57<02:08,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2868/3750 [06:57<02:08,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2870/3750 [06:57<02:08,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2872/3750 [06:57<02:07,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2874/3750 [06:57<02:07,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2876/3750 [06:58<02:07,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2878/3750 [06:58<02:06,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2880/3750 [06:58<02:06,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2882/3750 [06:58<02:06,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2884/3750 [06:58<02:05,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2886/3750 [06:58<02:05,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2888/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2890/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2892/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2894/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2896/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2898/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2900/3750 [06:59<02:02,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2902/3750 [06:59<02:02,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2904/3750 [06:59<02:02,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2906/3750 [06:59<02:01,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2908/3750 [06:59<02:01,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2910/3750 [07:00<02:01,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2912/3750 [07:00<02:00,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2914/3750 [07:00<02:00,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2916/3750 [07:00<02:00,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2918/3750 [07:00<01:59,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2920/3750 [07:00<01:59,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2922/3750 [07:00<01:59,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2924/3750 [07:00<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2926/3750 [07:00<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2928/3750 [07:01<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2930/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2932/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2934/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2936/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2938/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2940/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2942/3750 [07:01<01:55,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2944/3750 [07:01<01:55,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2946/3750 [07:02<01:55,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2948/3750 [07:02<01:54,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2950/3750 [07:02<01:54,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2952/3750 [07:02<01:54,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2954/3750 [07:02<01:53,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2956/3750 [07:02<01:53,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2958/3750 [07:02<01:53,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2960/3750 [07:02<01:52,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2962/3750 [07:02<01:52,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2964/3750 [07:03<01:52,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2966/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2968/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2970/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2972/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2974/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2976/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2978/3750 [07:03<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2980/3750 [07:03<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2982/3750 [07:04<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2984/3750 [07:04<01:48,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2986/3750 [07:04<01:48,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2988/3750 [07:04<01:48,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2990/3750 [07:04<01:47,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2992/3750 [07:04<01:47,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2994/3750 [07:04<01:47,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2996/3750 [07:04<01:46,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2998/3750 [07:04<01:46,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3000/3750 [07:05<01:46,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3002/3750 [07:05<01:45,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3004/3750 [07:05<01:45,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3006/3750 [07:05<01:45,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3008/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3010/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3012/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3014/3750 [07:05<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3016/3750 [07:05<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3018/3750 [07:06<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3020/3750 [07:06<01:43,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3022/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3024/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3026/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3028/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3030/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3032/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3034/3750 [07:06<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3036/3750 [07:07<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3038/3750 [07:07<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3040/3750 [07:07<01:39,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3042/3750 [07:07<01:39,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3044/3750 [07:07<01:39,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3046/3750 [07:07<01:38,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3048/3750 [07:07<01:38,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3050/3750 [07:07<01:38,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3052/3750 [07:08<01:37,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3054/3750 [07:08<01:37,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3056/3750 [07:08<01:37,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3058/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3060/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3062/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3064/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3066/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3068/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3070/3750 [07:09<01:35,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3072/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3074/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3076/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3078/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3080/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3082/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3084/3750 [07:09<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3086/3750 [07:09<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3088/3750 [07:10<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3090/3750 [07:10<01:31,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3092/3750 [07:10<01:31,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3094/3750 [07:10<01:31,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3096/3750 [07:10<01:30,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3098/3750 [07:10<01:30,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3100/3750 [07:10<01:30,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3102/3750 [07:10<01:30,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3104/3750 [07:10<01:29,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3106/3750 [07:11<01:29,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3108/3750 [07:11<01:29,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3110/3750 [07:11<01:28,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3112/3750 [07:11<01:28,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3114/3750 [07:11<01:28,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3116/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3118/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3120/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3122/3750 [07:11<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3124/3750 [07:12<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3126/3750 [07:12<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3128/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3130/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3132/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3134/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3136/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3138/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3140/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3142/3750 [07:13<01:23,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3144/3750 [07:13<01:23,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3146/3750 [07:13<01:23,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3148/3750 [07:13<01:22,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3150/3750 [07:13<01:22,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3152/3750 [07:13<01:22,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3154/3750 [07:13<01:21,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3156/3750 [07:13<01:21,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3158/3750 [07:13<01:21,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3160/3750 [07:14<01:21,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3162/3750 [07:14<01:20,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3164/3750 [07:14<01:20,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3166/3750 [07:14<01:20,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3168/3750 [07:14<01:19,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3170/3750 [07:14<01:19,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3172/3750 [07:14<01:19,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3174/3750 [07:14<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3176/3750 [07:15<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3178/3750 [07:15<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3180/3750 [07:15<01:18,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3182/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3184/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3186/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3188/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3190/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3192/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3194/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3196/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3198/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3200/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3202/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3204/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3206/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3208/3750 [07:16<01:13,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3210/3750 [07:16<01:13,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3212/3750 [07:17<01:13,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3214/3750 [07:17<01:12,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3216/3750 [07:17<01:12,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3218/3750 [07:17<01:12,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3220/3750 [07:17<01:12,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3222/3750 [07:17<01:11,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3224/3750 [07:17<01:11,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3226/3750 [07:17<01:11,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3228/3750 [07:17<01:10,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3230/3750 [07:18<01:10,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3232/3750 [07:18<01:10,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3234/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3236/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3238/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3240/3750 [07:18<01:09,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3242/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3244/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3246/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3248/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3250/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3252/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3254/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3256/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3258/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3260/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3262/3750 [07:19<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3264/3750 [07:19<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3266/3750 [07:20<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3268/3750 [07:20<01:04,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3270/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3272/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3274/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3276/3750 [07:20<01:03,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3278/3750 [07:20<01:03,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3280/3750 [07:20<01:03,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3282/3750 [07:20<01:02,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3284/3750 [07:21<01:02,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3286/3750 [07:21<01:02,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3288/3750 [07:21<01:02,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3290/3750 [07:21<01:01,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3292/3750 [07:21<01:01,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3294/3750 [07:21<01:01,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3296/3750 [07:21<01:00,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3298/3750 [07:21<01:00,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3300/3750 [07:22<01:00,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3302/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3304/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3306/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3308/3750 [07:22<00:59,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3310/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3312/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3314/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3316/3750 [07:22<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3318/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3320/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3322/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3324/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3326/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3328/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3330/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3332/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3334/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3336/3750 [07:24<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3338/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3340/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3342/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3344/3750 [07:24<00:53,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3346/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3348/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3350/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3352/3750 [07:24<00:52,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3354/3750 [07:25<00:52,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3356/3750 [07:25<00:52,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3358/3750 [07:25<00:51,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3360/3750 [07:25<00:51,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3362/3750 [07:25<00:51,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3364/3750 [07:25<00:51,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3366/3750 [07:25<00:50,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3368/3750 [07:25<00:50,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3370/3750 [07:25<00:50,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3372/3750 [07:26<00:50,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3374/3750 [07:26<00:49,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3376/3750 [07:26<00:49,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3378/3750 [07:26<00:49,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3380/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3382/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3384/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3386/3750 [07:26<00:48,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3388/3750 [07:26<00:47,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3390/3750 [07:27<00:47,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3392/3750 [07:27<00:47,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3394/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3396/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3398/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3400/3750 [07:27<00:46,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3402/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3404/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3406/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3408/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3410/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3412/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3414/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3416/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3418/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3420/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3422/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3424/3750 [07:28<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3426/3750 [07:29<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3428/3750 [07:29<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3430/3750 [07:29<00:41,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3432/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3434/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3436/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3438/3750 [07:29<00:40,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3440/3750 [07:29<00:40,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3442/3750 [07:30<00:40,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3444/3750 [07:30<00:39,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3446/3750 [07:30<00:39,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3448/3750 [07:30<00:39,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3450/3750 [07:30<00:39,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3452/3750 [07:30<00:38,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3454/3750 [07:30<00:38,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3456/3750 [07:30<00:38,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3458/3750 [07:30<00:38,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3460/3750 [07:31<00:37,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3462/3750 [07:31<00:37,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3464/3750 [07:31<00:37,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3466/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3468/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3470/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3472/3750 [07:31<00:36,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3474/3750 [07:31<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3476/3750 [07:31<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3478/3750 [07:32<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3480/3750 [07:32<00:35,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3482/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3484/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3486/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3488/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3490/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3492/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3494/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3496/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3498/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3500/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3502/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3504/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3506/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3508/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3510/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3512/3750 [07:33<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3514/3750 [07:34<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3516/3750 [07:34<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3518/3750 [07:34<00:29,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3520/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3522/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3524/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3526/3750 [07:34<00:28,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3528/3750 [07:34<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3530/3750 [07:34<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3532/3750 [07:35<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3534/3750 [07:35<00:27,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3536/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3538/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3540/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3542/3750 [07:35<00:26,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3544/3750 [07:35<00:26,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3546/3750 [07:35<00:26,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3548/3750 [07:35<00:25,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3550/3750 [07:36<00:25,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3552/3750 [07:36<00:25,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3554/3750 [07:36<00:25,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3556/3750 [07:36<00:24,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3558/3750 [07:36<00:24,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3560/3750 [07:36<00:24,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3562/3750 [07:36<00:24,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3564/3750 [07:36<00:23,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3566/3750 [07:37<00:23,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3568/3750 [07:37<00:23,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3570/3750 [07:37<00:23,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3572/3750 [07:37<00:22,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3574/3750 [07:37<00:22,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3576/3750 [07:37<00:22,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3578/3750 [07:37<00:22,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3580/3750 [07:37<00:21,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3582/3750 [07:37<00:21,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3584/3750 [07:38<00:21,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3586/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3588/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3590/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3592/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3594/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3596/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3598/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3600/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3602/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3604/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3606/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3608/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3610/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3612/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3614/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3616/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3618/3750 [07:39<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3620/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3622/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3624/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3626/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3628/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3630/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3632/3750 [07:40<00:14,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3634/3750 [07:40<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3636/3750 [07:40<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3638/3750 [07:41<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3640/3750 [07:41<00:13,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3642/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3644/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3646/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3648/3750 [07:41<00:12,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3650/3750 [07:41<00:12,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3652/3750 [07:41<00:12,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3654/3750 [07:41<00:12,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3656/3750 [07:42<00:11,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3658/3750 [07:42<00:11,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3660/3750 [07:42<00:11,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3662/3750 [07:42<00:11,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3664/3750 [07:42<00:10,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3666/3750 [07:42<00:10,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3668/3750 [07:42<00:10,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3670/3750 [07:42<00:10,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3672/3750 [07:42<00:09,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3674/3750 [07:43<00:09,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3676/3750 [07:43<00:09,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3678/3750 [07:43<00:09,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3680/3750 [07:43<00:08,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3682/3750 [07:43<00:08,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3684/3750 [07:43<00:08,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3686/3750 [07:43<00:08,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3688/3750 [07:43<00:07,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3690/3750 [07:43<00:07,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3692/3750 [07:44<00:07,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3694/3750 [07:44<00:07,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3696/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3698/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3700/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3702/3750 [07:44<00:06,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3704/3750 [07:44<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3706/3750 [07:44<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3708/3750 [07:45<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3710/3750 [07:45<00:05,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3712/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3714/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3716/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3718/3750 [07:45<00:04,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3720/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3722/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3724/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3726/3750 [07:46<00:03,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3728/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3730/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3732/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3734/3750 [07:46<00:01,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3736/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3738/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3740/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3742/3750 [07:46<00:00,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3744/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3746/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3748/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|██████████| 3750/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 1:   0%|          | 0/3750 [00:00<?, ?it/s, loss=0.138, v_num=0]           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 2500/3750 [06:38<03:19,  6.28it/s, loss=0.125, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:  67%|██████▋   | 2502/3750 [06:38<03:18,  6.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2504/3750 [06:38<03:18,  6.28it/s, loss=0.125, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:  67%|██████▋   | 2506/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2508/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2510/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2512/3750 [06:38<03:16,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2514/3750 [06:39<03:16,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2516/3750 [06:39<03:15,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2518/3750 [06:39<03:15,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2520/3750 [06:39<03:14,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2522/3750 [06:39<03:14,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2524/3750 [06:39<03:14,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2526/3750 [06:39<03:13,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2528/3750 [06:39<03:13,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2530/3750 [06:39<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2532/3750 [06:40<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2534/3750 [06:40<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2536/3750 [06:40<03:11,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2538/3750 [06:40<03:11,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2540/3750 [06:40<03:10,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2542/3750 [06:40<03:10,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2544/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2546/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2548/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2550/3750 [06:41<03:08,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2552/3750 [06:41<03:08,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2554/3750 [06:41<03:07,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2556/3750 [06:41<03:07,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2558/3750 [06:41<03:07,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2560/3750 [06:41<03:06,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2562/3750 [06:41<03:06,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2564/3750 [06:41<03:05,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2566/3750 [06:42<03:05,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2568/3750 [06:42<03:05,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2570/3750 [06:42<03:04,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2572/3750 [06:42<03:04,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2574/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2576/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2578/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2580/3750 [06:42<03:02,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2582/3750 [06:42<03:02,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2584/3750 [06:43<03:01,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2586/3750 [06:43<03:01,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2588/3750 [06:43<03:01,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2590/3750 [06:43<03:00,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2592/3750 [06:43<03:00,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2594/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2596/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2598/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2600/3750 [06:43<02:58,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2602/3750 [06:44<02:58,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2604/3750 [06:44<02:57,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2606/3750 [06:44<02:57,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2608/3750 [06:44<02:57,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2610/3750 [06:44<02:56,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2612/3750 [06:44<02:56,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2614/3750 [06:44<02:55,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2616/3750 [06:44<02:55,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2618/3750 [06:44<02:55,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2620/3750 [06:45<02:54,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2622/3750 [06:45<02:54,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2624/3750 [06:45<02:53,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2626/3750 [06:45<02:53,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2628/3750 [06:45<02:53,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2630/3750 [06:45<02:52,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2632/3750 [06:45<02:52,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2634/3750 [06:45<02:51,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2636/3750 [06:45<02:51,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2638/3750 [06:46<02:51,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2640/3750 [06:46<02:50,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2642/3750 [06:46<02:50,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2644/3750 [06:46<02:50,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2646/3750 [06:46<02:49,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2648/3750 [06:46<02:49,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2650/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2652/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2654/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2656/3750 [06:47<02:47,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2658/3750 [06:47<02:47,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2660/3750 [06:47<02:46,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2662/3750 [06:47<02:46,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2664/3750 [06:47<02:46,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2666/3750 [06:47<02:45,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2668/3750 [06:47<02:45,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2670/3750 [06:47<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2672/3750 [06:47<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2674/3750 [06:48<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2676/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2678/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2680/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2682/3750 [06:48<02:42,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2684/3750 [06:48<02:42,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2686/3750 [06:48<02:41,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2688/3750 [06:48<02:41,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2690/3750 [06:49<02:41,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2692/3750 [06:49<02:40,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2694/3750 [06:49<02:40,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2696/3750 [06:49<02:40,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2698/3750 [06:49<02:39,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2700/3750 [06:49<02:39,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2702/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2704/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2706/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2708/3750 [06:50<02:37,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2710/3750 [06:50<02:37,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2712/3750 [06:50<02:37,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2714/3750 [06:50<02:36,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2716/3750 [06:50<02:36,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2718/3750 [06:50<02:35,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2720/3750 [06:50<02:35,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2722/3750 [06:50<02:35,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2724/3750 [06:50<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2726/3750 [06:51<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2728/3750 [06:51<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2730/3750 [06:51<02:33,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2732/3750 [06:51<02:33,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2734/3750 [06:51<02:32,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2736/3750 [06:51<02:32,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2738/3750 [06:51<02:32,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2740/3750 [06:51<02:31,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2742/3750 [06:51<02:31,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2744/3750 [06:52<02:31,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2746/3750 [06:52<02:30,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2748/3750 [06:52<02:30,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2750/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2752/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2754/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2756/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2758/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2760/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2762/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2764/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2766/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2768/3750 [06:53<02:26,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2770/3750 [06:53<02:26,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2772/3750 [06:53<02:25,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2774/3750 [06:53<02:25,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2776/3750 [06:53<02:25,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2778/3750 [06:53<02:24,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2780/3750 [06:54<02:24,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2782/3750 [06:54<02:24,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2784/3750 [06:54<02:23,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2786/3750 [06:54<02:23,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2788/3750 [06:54<02:23,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2790/3750 [06:54<02:22,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2792/3750 [06:54<02:22,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2794/3750 [06:54<02:21,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2796/3750 [06:55<02:21,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2798/3750 [06:55<02:21,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2800/3750 [06:55<02:20,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2802/3750 [06:55<02:20,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2804/3750 [06:55<02:20,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2806/3750 [06:55<02:19,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2808/3750 [06:55<02:19,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2810/3750 [06:55<02:19,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2812/3750 [06:55<02:18,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2814/3750 [06:56<02:18,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2816/3750 [06:56<02:18,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2818/3750 [06:56<02:17,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2820/3750 [06:56<02:17,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2822/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2824/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2826/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2828/3750 [06:56<02:15,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2830/3750 [06:56<02:15,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2832/3750 [06:57<02:15,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2834/3750 [06:57<02:14,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2836/3750 [06:57<02:14,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2838/3750 [06:57<02:14,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2840/3750 [06:57<02:13,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2842/3750 [06:57<02:13,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2844/3750 [06:57<02:13,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2846/3750 [06:57<02:12,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2848/3750 [06:57<02:12,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2850/3750 [06:58<02:12,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2852/3750 [06:58<02:11,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2854/3750 [06:58<02:11,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2856/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2858/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2860/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2862/3750 [06:58<02:09,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2864/3750 [06:58<02:09,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2866/3750 [06:58<02:09,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2868/3750 [06:59<02:08,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2870/3750 [06:59<02:08,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2872/3750 [06:59<02:08,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2874/3750 [06:59<02:07,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2876/3750 [06:59<02:07,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2878/3750 [06:59<02:07,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2880/3750 [06:59<02:06,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2882/3750 [06:59<02:06,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2884/3750 [06:59<02:06,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2886/3750 [07:00<02:05,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2888/3750 [07:00<02:05,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2890/3750 [07:00<02:05,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2892/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2894/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2896/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2898/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2900/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2902/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2904/3750 [07:01<02:02,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2906/3750 [07:01<02:02,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2908/3750 [07:01<02:01,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2910/3750 [07:01<02:01,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2912/3750 [07:01<02:01,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2914/3750 [07:01<02:00,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2916/3750 [07:01<02:00,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2918/3750 [07:01<02:00,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2920/3750 [07:02<01:59,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2922/3750 [07:02<01:59,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2924/3750 [07:02<01:59,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2926/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2928/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2930/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2932/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2934/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2936/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2938/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2940/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2942/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2944/3750 [07:03<01:55,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2946/3750 [07:03<01:55,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2948/3750 [07:03<01:55,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2950/3750 [07:03<01:54,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2952/3750 [07:03<01:54,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2954/3750 [07:03<01:54,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2956/3750 [07:04<01:53,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2958/3750 [07:04<01:53,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2960/3750 [07:04<01:53,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2962/3750 [07:04<01:52,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2964/3750 [07:04<01:52,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2966/3750 [07:04<01:52,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2968/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2970/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2972/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2974/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2976/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2978/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2980/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2982/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2984/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2986/3750 [07:05<01:48,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2988/3750 [07:05<01:48,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2990/3750 [07:05<01:48,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2992/3750 [07:06<01:47,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2994/3750 [07:06<01:47,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2996/3750 [07:06<01:47,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2998/3750 [07:06<01:46,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3000/3750 [07:06<01:46,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3002/3750 [07:06<01:46,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3004/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3006/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3008/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3010/3750 [07:07<01:45,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3012/3750 [07:07<01:44,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3014/3750 [07:07<01:44,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3016/3750 [07:07<01:44,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3018/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3020/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3022/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3024/3750 [07:07<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3026/3750 [07:08<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3028/3750 [07:08<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3030/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3032/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3034/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3036/3750 [07:08<01:40,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3038/3750 [07:08<01:40,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3040/3750 [07:08<01:40,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3042/3750 [07:08<01:39,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3044/3750 [07:09<01:39,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3046/3750 [07:09<01:39,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3048/3750 [07:09<01:38,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3050/3750 [07:09<01:38,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3052/3750 [07:09<01:38,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3054/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3056/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3058/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3060/3750 [07:09<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3062/3750 [07:10<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3064/3750 [07:10<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3066/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3068/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3070/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3072/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3074/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3076/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3078/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3080/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3082/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3084/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3086/3750 [07:11<01:32,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3088/3750 [07:11<01:32,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3090/3750 [07:11<01:32,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3092/3750 [07:11<01:31,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3094/3750 [07:11<01:31,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3096/3750 [07:11<01:31,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3098/3750 [07:12<01:30,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3100/3750 [07:12<01:30,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3102/3750 [07:12<01:30,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3104/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3106/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3108/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3110/3750 [07:12<01:29,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3112/3750 [07:12<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3114/3750 [07:12<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3116/3750 [07:13<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3118/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3120/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3122/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3124/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3126/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3128/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3130/3750 [07:13<01:25,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3132/3750 [07:13<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3134/3750 [07:14<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3136/3750 [07:14<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3138/3750 [07:14<01:24,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3140/3750 [07:14<01:24,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3142/3750 [07:14<01:24,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3144/3750 [07:14<01:23,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3146/3750 [07:14<01:23,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3148/3750 [07:14<01:23,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3150/3750 [07:15<01:22,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3152/3750 [07:15<01:22,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3154/3750 [07:15<01:22,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3156/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3158/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3160/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3162/3750 [07:15<01:21,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3164/3750 [07:15<01:20,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3166/3750 [07:15<01:20,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3168/3750 [07:16<01:20,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3170/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3172/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3174/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3176/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3178/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3180/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3182/3750 [07:16<01:17,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3184/3750 [07:16<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3186/3750 [07:17<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3188/3750 [07:17<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3190/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3192/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3194/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3196/3750 [07:17<01:15,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3198/3750 [07:17<01:15,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3200/3750 [07:17<01:15,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3202/3750 [07:17<01:14,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3204/3750 [07:18<01:14,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3206/3750 [07:18<01:14,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3208/3750 [07:18<01:14,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3210/3750 [07:18<01:13,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3212/3750 [07:18<01:13,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3214/3750 [07:18<01:13,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3216/3750 [07:18<01:12,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3218/3750 [07:18<01:12,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3220/3750 [07:18<01:12,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3222/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3224/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3226/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3228/3750 [07:19<01:11,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3230/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3232/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3234/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3236/3750 [07:19<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3238/3750 [07:19<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3240/3750 [07:20<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3242/3750 [07:20<01:08,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3244/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3246/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3248/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3250/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3252/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3254/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3256/3750 [07:21<01:06,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3258/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3260/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3262/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3264/3750 [07:21<01:05,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3266/3750 [07:21<01:05,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3268/3750 [07:21<01:05,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3270/3750 [07:21<01:04,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3272/3750 [07:21<01:04,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3274/3750 [07:22<01:04,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3276/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3278/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3280/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3282/3750 [07:22<01:03,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3284/3750 [07:22<01:02,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3286/3750 [07:22<01:02,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3288/3750 [07:22<01:02,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3290/3750 [07:22<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3292/3750 [07:23<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3294/3750 [07:23<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3296/3750 [07:23<01:01,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3298/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3300/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3302/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3304/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3306/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3308/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3310/3750 [07:24<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3312/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3314/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3316/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3318/3750 [07:24<00:57,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3320/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3322/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3324/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3326/3750 [07:24<00:56,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3328/3750 [07:25<00:56,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3330/3750 [07:25<00:56,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3332/3750 [07:25<00:55,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3334/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3336/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3338/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3340/3750 [07:25<00:54,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3342/3750 [07:25<00:54,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3344/3750 [07:25<00:54,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3346/3750 [07:26<00:53,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3348/3750 [07:26<00:53,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3350/3750 [07:26<00:53,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3352/3750 [07:26<00:53,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3354/3750 [07:26<00:52,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3356/3750 [07:26<00:52,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3358/3750 [07:26<00:52,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3360/3750 [07:26<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3362/3750 [07:26<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3364/3750 [07:27<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3366/3750 [07:27<00:51,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3368/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3370/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3372/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3374/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3376/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3378/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3380/3750 [07:28<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3382/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3384/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3386/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3388/3750 [07:28<00:47,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3390/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3392/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3394/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3396/3750 [07:28<00:46,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3398/3750 [07:29<00:46,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3400/3750 [07:29<00:46,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3402/3750 [07:29<00:45,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3404/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3406/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3408/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3410/3750 [07:29<00:44,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3412/3750 [07:29<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3414/3750 [07:29<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3416/3750 [07:30<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3418/3750 [07:30<00:43,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3420/3750 [07:30<00:43,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3422/3750 [07:30<00:43,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3424/3750 [07:30<00:42,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3426/3750 [07:30<00:42,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3428/3750 [07:30<00:42,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3430/3750 [07:30<00:42,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3432/3750 [07:30<00:41,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3434/3750 [07:31<00:41,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3436/3750 [07:31<00:41,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3438/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3440/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3442/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3444/3750 [07:31<00:40,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3446/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3448/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3450/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3452/3750 [07:32<00:39,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3454/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3456/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3458/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3460/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3462/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3464/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3466/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3468/3750 [07:32<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3470/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3472/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3474/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3476/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3478/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3480/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3482/3750 [07:33<00:34,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3484/3750 [07:33<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3486/3750 [07:33<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3488/3750 [07:34<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3490/3750 [07:34<00:33,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3492/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3494/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3496/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3498/3750 [07:34<00:32,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3500/3750 [07:34<00:32,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3502/3750 [07:34<00:32,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3504/3750 [07:35<00:31,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3506/3750 [07:35<00:31,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3508/3750 [07:35<00:31,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3510/3750 [07:35<00:31,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3512/3750 [07:35<00:30,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3514/3750 [07:35<00:30,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3516/3750 [07:35<00:30,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3518/3750 [07:35<00:30,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3520/3750 [07:35<00:29,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3522/3750 [07:36<00:29,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3524/3750 [07:36<00:29,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3526/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3528/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3530/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3532/3750 [07:36<00:28,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3534/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3536/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3538/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3540/3750 [07:37<00:27,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3542/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3544/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3546/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3548/3750 [07:37<00:26,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3550/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3552/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3554/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3556/3750 [07:37<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3558/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3560/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3562/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3564/3750 [07:38<00:23,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3566/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3568/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3570/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3572/3750 [07:38<00:22,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3574/3750 [07:38<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3576/3750 [07:39<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3578/3750 [07:39<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3580/3750 [07:39<00:21,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3582/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3584/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3586/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3588/3750 [07:39<00:20,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3590/3750 [07:39<00:20,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3592/3750 [07:39<00:20,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3594/3750 [07:40<00:19,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3596/3750 [07:40<00:19,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3598/3750 [07:40<00:19,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3600/3750 [07:40<00:19,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3602/3750 [07:40<00:18,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3604/3750 [07:40<00:18,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3606/3750 [07:40<00:18,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3608/3750 [07:40<00:18,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3610/3750 [07:40<00:17,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3612/3750 [07:41<00:17,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3614/3750 [07:41<00:17,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3616/3750 [07:41<00:17,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3618/3750 [07:41<00:16,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3620/3750 [07:41<00:16,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3622/3750 [07:41<00:16,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3624/3750 [07:41<00:16,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3626/3750 [07:41<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3628/3750 [07:42<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3630/3750 [07:42<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3632/3750 [07:42<00:15,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3634/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3636/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3638/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3640/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3642/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3644/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3646/3750 [07:43<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3648/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3650/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3652/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3654/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3656/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3658/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3660/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3662/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3664/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3666/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3668/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3670/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3672/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3674/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3676/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3678/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3680/3750 [07:44<00:08,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3682/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3684/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3686/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3688/3750 [07:45<00:07,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3690/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3692/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3694/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3696/3750 [07:45<00:06,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3698/3750 [07:45<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3700/3750 [07:46<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3702/3750 [07:46<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3704/3750 [07:46<00:05,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3706/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3708/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3710/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3712/3750 [07:46<00:04,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3714/3750 [07:46<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3716/3750 [07:46<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3718/3750 [07:47<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3720/3750 [07:47<00:03,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3722/3750 [07:47<00:03,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3724/3750 [07:47<00:03,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3726/3750 [07:47<00:03,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3728/3750 [07:47<00:02,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3730/3750 [07:47<00:02,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3732/3750 [07:47<00:02,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3734/3750 [07:48<00:02,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3736/3750 [07:48<00:01,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3738/3750 [07:48<00:01,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3740/3750 [07:48<00:01,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3742/3750 [07:48<00:01,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3744/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3746/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3748/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|██████████| 3750/3750 [07:49<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 2:   0%|          | 0/3750 [00:00<?, ?it/s, loss=0.125, v_num=0]           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 2500/3750 [06:38<03:19,  6.28it/s, loss=0.0849, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:  67%|██████▋   | 2502/3750 [06:38<03:18,  6.28it/s, loss=0.0849, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 2504/3750 [06:38<03:18,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2506/3750 [06:38<03:17,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2508/3750 [06:38<03:17,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2510/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2512/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2514/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2516/3750 [06:39<03:15,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2518/3750 [06:39<03:15,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2520/3750 [06:39<03:14,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2522/3750 [06:39<03:14,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2524/3750 [06:39<03:14,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2526/3750 [06:39<03:13,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2528/3750 [06:39<03:13,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2530/3750 [06:39<03:12,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2532/3750 [06:39<03:12,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2534/3750 [06:40<03:11,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2536/3750 [06:40<03:11,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2538/3750 [06:40<03:11,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2540/3750 [06:40<03:10,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2542/3750 [06:40<03:10,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2544/3750 [06:40<03:09,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2546/3750 [06:40<03:09,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2548/3750 [06:40<03:09,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2550/3750 [06:40<03:08,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2552/3750 [06:41<03:08,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2554/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2556/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2558/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2560/3750 [06:41<03:06,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2562/3750 [06:41<03:06,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2564/3750 [06:41<03:05,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2566/3750 [06:41<03:05,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2568/3750 [06:41<03:05,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2570/3750 [06:42<03:04,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2572/3750 [06:42<03:04,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2574/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2576/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2578/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2580/3750 [06:42<03:02,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2582/3750 [06:42<03:02,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2584/3750 [06:42<03:01,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2586/3750 [06:42<03:01,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2588/3750 [06:43<03:00,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2590/3750 [06:43<03:00,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2592/3750 [06:43<03:00,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2594/3750 [06:43<02:59,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2596/3750 [06:43<02:59,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2598/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2600/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2602/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2604/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2606/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2608/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2610/3750 [06:44<02:56,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2612/3750 [06:44<02:56,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2614/3750 [06:44<02:55,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2616/3750 [06:44<02:55,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2618/3750 [06:44<02:55,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2620/3750 [06:44<02:54,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2622/3750 [06:45<02:54,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2624/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2626/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2628/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2630/3750 [06:45<02:52,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2632/3750 [06:45<02:52,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2634/3750 [06:45<02:51,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2636/3750 [06:45<02:51,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2638/3750 [06:45<02:51,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2640/3750 [06:46<02:50,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2642/3750 [06:46<02:50,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2644/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2646/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2648/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2650/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2652/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2654/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2656/3750 [06:46<02:47,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2658/3750 [06:47<02:47,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2660/3750 [06:47<02:46,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2662/3750 [06:47<02:46,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2664/3750 [06:47<02:46,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2666/3750 [06:47<02:45,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2668/3750 [06:47<02:45,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2670/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2672/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2674/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2676/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2678/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2680/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2682/3750 [06:48<02:42,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2684/3750 [06:48<02:42,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2686/3750 [06:48<02:41,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2688/3750 [06:48<02:41,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2690/3750 [06:48<02:41,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2692/3750 [06:48<02:40,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2694/3750 [06:49<02:40,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2696/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2698/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2700/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2702/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2704/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2706/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2708/3750 [06:49<02:37,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2710/3750 [06:50<02:37,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2712/3750 [06:50<02:36,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2714/3750 [06:50<02:36,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2716/3750 [06:50<02:36,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2718/3750 [06:50<02:35,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2720/3750 [06:50<02:35,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2722/3750 [06:50<02:35,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2724/3750 [06:50<02:34,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2726/3750 [06:50<02:34,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2728/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2730/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2732/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2734/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2736/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2738/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2740/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2742/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2744/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2746/3750 [06:52<02:30,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2748/3750 [06:52<02:30,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2750/3750 [06:52<02:29,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2752/3750 [06:52<02:29,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2754/3750 [06:52<02:29,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2756/3750 [06:52<02:28,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2758/3750 [06:52<02:28,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2760/3750 [06:52<02:28,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2762/3750 [06:52<02:27,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2764/3750 [06:53<02:27,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2766/3750 [06:53<02:26,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2768/3750 [06:53<02:26,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2770/3750 [06:53<02:26,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2772/3750 [06:53<02:25,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2774/3750 [06:53<02:25,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2776/3750 [06:53<02:25,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2778/3750 [06:53<02:24,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2780/3750 [06:53<02:24,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2782/3750 [06:54<02:24,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2784/3750 [06:54<02:23,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2786/3750 [06:54<02:23,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2788/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2790/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2792/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2794/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2796/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2798/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2800/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2802/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2804/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2806/3750 [06:55<02:19,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2808/3750 [06:55<02:19,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2810/3750 [06:55<02:19,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2812/3750 [06:55<02:18,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2814/3750 [06:55<02:18,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2816/3750 [06:55<02:17,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2818/3750 [06:56<02:17,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2820/3750 [06:56<02:17,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2822/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2824/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2826/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2828/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2830/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2832/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2834/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2836/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2838/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2840/3750 [06:57<02:13,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2842/3750 [06:57<02:13,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2844/3750 [06:57<02:13,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2846/3750 [06:57<02:12,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2848/3750 [06:57<02:12,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2850/3750 [06:57<02:11,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2852/3750 [06:58<02:11,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2854/3750 [06:58<02:11,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2856/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2858/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2860/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2862/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2864/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2866/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2868/3750 [06:58<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2870/3750 [06:59<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2872/3750 [06:59<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2874/3750 [06:59<02:07,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2876/3750 [06:59<02:07,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2878/3750 [06:59<02:07,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2880/3750 [06:59<02:06,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2882/3750 [06:59<02:06,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2884/3750 [06:59<02:06,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2886/3750 [06:59<02:05,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2888/3750 [07:00<02:05,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2890/3750 [07:00<02:05,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2892/3750 [07:00<02:04,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2894/3750 [07:00<02:04,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2896/3750 [07:00<02:04,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2898/3750 [07:00<02:03,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2900/3750 [07:00<02:03,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2902/3750 [07:00<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2904/3750 [07:00<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2906/3750 [07:01<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2908/3750 [07:01<02:01,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2910/3750 [07:01<02:01,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2912/3750 [07:01<02:01,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2914/3750 [07:01<02:00,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2916/3750 [07:01<02:00,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2918/3750 [07:01<02:00,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2920/3750 [07:01<01:59,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2922/3750 [07:01<01:59,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2924/3750 [07:02<01:59,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2926/3750 [07:02<01:58,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2928/3750 [07:02<01:58,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2930/3750 [07:02<01:58,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2932/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2934/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2936/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2938/3750 [07:02<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2940/3750 [07:02<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2942/3750 [07:03<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2944/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2946/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2948/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2950/3750 [07:03<01:54,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2952/3750 [07:03<01:54,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2954/3750 [07:03<01:54,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2956/3750 [07:03<01:53,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2958/3750 [07:04<01:53,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2960/3750 [07:04<01:53,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2962/3750 [07:04<01:52,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2964/3750 [07:04<01:52,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2966/3750 [07:04<01:52,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2968/3750 [07:04<01:51,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2970/3750 [07:04<01:51,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2972/3750 [07:04<01:51,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2974/3750 [07:04<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2976/3750 [07:05<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2978/3750 [07:05<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2980/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2982/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2984/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2986/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2988/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2990/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2992/3750 [07:05<01:47,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2994/3750 [07:06<01:47,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2996/3750 [07:06<01:47,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2998/3750 [07:06<01:46,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3000/3750 [07:06<01:46,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3002/3750 [07:06<01:46,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3004/3750 [07:06<01:45,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3006/3750 [07:06<01:45,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3008/3750 [07:06<01:45,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3010/3750 [07:06<01:44,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3012/3750 [07:07<01:44,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3014/3750 [07:07<01:44,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3016/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3018/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3020/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3022/3750 [07:07<01:43,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3024/3750 [07:07<01:42,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3026/3750 [07:07<01:42,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3028/3750 [07:07<01:42,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3030/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3032/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3034/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3036/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3038/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3040/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3042/3750 [07:08<01:39,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3044/3750 [07:08<01:39,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3046/3750 [07:08<01:39,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3048/3750 [07:09<01:38,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3050/3750 [07:09<01:38,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3052/3750 [07:09<01:38,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3054/3750 [07:09<01:37,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3056/3750 [07:09<01:37,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3058/3750 [07:09<01:37,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3060/3750 [07:09<01:36,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3062/3750 [07:09<01:36,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3064/3750 [07:10<01:36,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3066/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3068/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3070/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3072/3750 [07:10<01:35,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3074/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3076/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3078/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3080/3750 [07:10<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3082/3750 [07:11<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3084/3750 [07:11<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3086/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3088/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3090/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3092/3750 [07:11<01:31,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3094/3750 [07:11<01:31,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3096/3750 [07:11<01:31,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3098/3750 [07:11<01:30,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3100/3750 [07:12<01:30,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3102/3750 [07:12<01:30,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3104/3750 [07:12<01:29,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3106/3750 [07:12<01:29,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3108/3750 [07:12<01:29,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3110/3750 [07:12<01:29,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3112/3750 [07:12<01:28,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3114/3750 [07:12<01:28,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3116/3750 [07:12<01:28,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3118/3750 [07:13<01:27,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3120/3750 [07:13<01:27,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3122/3750 [07:13<01:27,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3124/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3126/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3128/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3130/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3132/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3134/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3136/3750 [07:14<01:24,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3138/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3140/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3142/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3144/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3146/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3148/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3150/3750 [07:14<01:22,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3152/3750 [07:14<01:22,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3154/3750 [07:15<01:22,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3156/3750 [07:15<01:21,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3158/3750 [07:15<01:21,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3160/3750 [07:15<01:21,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3162/3750 [07:15<01:20,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3164/3750 [07:15<01:20,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3166/3750 [07:15<01:20,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3168/3750 [07:15<01:20,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3170/3750 [07:15<01:19,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3172/3750 [07:16<01:19,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3174/3750 [07:16<01:19,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3176/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3178/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3180/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3182/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3184/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3186/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3188/3750 [07:17<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3190/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3192/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3194/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3196/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3198/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3200/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3202/3750 [07:17<01:14,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3204/3750 [07:17<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3206/3750 [07:18<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3208/3750 [07:18<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3210/3750 [07:18<01:13,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3212/3750 [07:18<01:13,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3214/3750 [07:18<01:13,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3216/3750 [07:18<01:12,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3218/3750 [07:18<01:12,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3220/3750 [07:18<01:12,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3222/3750 [07:18<01:11,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3224/3750 [07:19<01:11,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3226/3750 [07:19<01:11,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3228/3750 [07:19<01:11,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3230/3750 [07:19<01:10,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3232/3750 [07:19<01:10,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3234/3750 [07:19<01:10,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3236/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3238/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3240/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3242/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3244/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3246/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3248/3750 [07:20<01:08,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3250/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3252/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3254/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3256/3750 [07:20<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3258/3750 [07:20<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3260/3750 [07:21<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3262/3750 [07:21<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3264/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3266/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3268/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3270/3750 [07:21<01:04,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3272/3750 [07:21<01:04,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3274/3750 [07:21<01:04,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3276/3750 [07:21<01:03,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3278/3750 [07:22<01:03,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3280/3750 [07:22<01:03,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3282/3750 [07:22<01:03,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3284/3750 [07:22<01:02,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3286/3750 [07:22<01:02,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3288/3750 [07:22<01:02,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3290/3750 [07:22<01:01,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3292/3750 [07:22<01:01,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3294/3750 [07:22<01:01,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3296/3750 [07:23<01:01,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3298/3750 [07:23<01:00,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3300/3750 [07:23<01:00,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3302/3750 [07:23<01:00,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3304/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3306/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3308/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3310/3750 [07:23<00:59,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3312/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3314/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3316/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3318/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3320/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3322/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3324/3750 [07:24<00:56,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3326/3750 [07:24<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3328/3750 [07:24<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3330/3750 [07:25<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3332/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3334/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3336/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3338/3750 [07:25<00:54,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3340/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3342/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3344/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3346/3750 [07:25<00:53,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3348/3750 [07:26<00:53,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3350/3750 [07:26<00:53,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3352/3750 [07:26<00:52,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3354/3750 [07:26<00:52,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3356/3750 [07:26<00:52,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3358/3750 [07:26<00:52,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3360/3750 [07:26<00:51,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3362/3750 [07:26<00:51,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3364/3750 [07:26<00:51,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3366/3750 [07:27<00:51,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3368/3750 [07:27<00:50,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3370/3750 [07:27<00:50,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3372/3750 [07:27<00:50,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3374/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3376/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3378/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3380/3750 [07:27<00:49,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3382/3750 [07:27<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3384/3750 [07:28<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3386/3750 [07:28<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3388/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3390/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3392/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3394/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3396/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3398/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3400/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3402/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3404/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3406/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3408/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3410/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3412/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3414/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3416/3750 [07:29<00:43,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3418/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3420/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3422/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3424/3750 [07:30<00:42,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3426/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3428/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3430/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3432/3750 [07:30<00:41,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3434/3750 [07:30<00:41,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3436/3750 [07:31<00:41,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3438/3750 [07:31<00:40,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3440/3750 [07:31<00:40,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3442/3750 [07:31<00:40,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3444/3750 [07:31<00:40,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3446/3750 [07:31<00:39,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3448/3750 [07:31<00:39,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3450/3750 [07:31<00:39,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3452/3750 [07:31<00:39,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3454/3750 [07:32<00:38,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3456/3750 [07:32<00:38,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3458/3750 [07:32<00:38,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3460/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3462/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3464/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3466/3750 [07:32<00:37,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3468/3750 [07:32<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3470/3750 [07:32<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3472/3750 [07:33<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3474/3750 [07:33<00:36,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3476/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3478/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3480/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3482/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3484/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3486/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3488/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3490/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3492/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3494/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3496/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3498/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3500/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3502/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3504/3750 [07:34<00:31,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3506/3750 [07:34<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3508/3750 [07:35<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3510/3750 [07:35<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3512/3750 [07:35<00:30,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3514/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3516/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3518/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3520/3750 [07:35<00:29,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3522/3750 [07:35<00:29,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3524/3750 [07:35<00:29,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3526/3750 [07:36<00:28,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3528/3750 [07:36<00:28,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3530/3750 [07:36<00:28,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3532/3750 [07:36<00:28,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3534/3750 [07:36<00:27,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3536/3750 [07:36<00:27,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3538/3750 [07:36<00:27,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3540/3750 [07:36<00:27,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3542/3750 [07:37<00:26,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3544/3750 [07:37<00:26,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3546/3750 [07:37<00:26,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3548/3750 [07:37<00:26,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3550/3750 [07:37<00:25,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3552/3750 [07:37<00:25,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3554/3750 [07:37<00:25,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3556/3750 [07:37<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3558/3750 [07:37<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3560/3750 [07:38<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3562/3750 [07:38<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3564/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3566/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3568/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3570/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3572/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3574/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3576/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3578/3750 [07:39<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3580/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3582/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3584/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3586/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3588/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3590/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3592/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3594/3750 [07:39<00:19,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3596/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3598/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3600/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3602/3750 [07:40<00:18,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3604/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3606/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3608/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3610/3750 [07:40<00:17,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3612/3750 [07:40<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3614/3750 [07:41<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3616/3750 [07:41<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3618/3750 [07:41<00:16,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3620/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3622/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3624/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3626/3750 [07:41<00:15,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3628/3750 [07:41<00:15,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3630/3750 [07:41<00:15,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3632/3750 [07:42<00:15,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3634/3750 [07:42<00:14,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3636/3750 [07:42<00:14,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3638/3750 [07:42<00:14,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3640/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3642/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3644/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3646/3750 [07:42<00:13,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3648/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3650/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3652/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3654/3750 [07:43<00:12,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3656/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3658/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3660/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3662/3750 [07:43<00:11,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3664/3750 [07:43<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3666/3750 [07:44<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3668/3750 [07:44<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3670/3750 [07:44<00:10,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3672/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3674/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3676/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3678/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3680/3750 [07:44<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3682/3750 [07:44<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3684/3750 [07:45<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3686/3750 [07:45<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3688/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3690/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3692/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3694/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3696/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3698/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3700/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3702/3750 [07:46<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3704/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3706/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3708/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3710/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3712/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3714/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3716/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3718/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3720/3750 [07:47<00:03,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3722/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3724/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3726/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3728/3750 [07:47<00:02,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3730/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3732/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3734/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3736/3750 [07:47<00:01,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3738/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3740/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3742/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3744/3750 [07:48<00:00,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3746/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3748/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|██████████| 3750/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|██████████| 3750/3750 [07:50<00:00,  7.98it/s, loss=0.0849, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nan0hu-nj5Zm"
   },
   "source": [
    "## Load the Stored Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:03:06.550253Z",
     "iopub.status.busy": "2024-02-27T13:03:06.549917Z",
     "iopub.status.idle": "2024-02-27T13:03:08.694709Z",
     "shell.execute_reply": "2024-02-27T13:03:08.694174Z",
     "shell.execute_reply.started": "2024-02-27T13:03:06.550225Z"
    },
    "id": "27yNiUp_W1pn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(\"/mnt/workspace/ORL/lightning_logs/version_0/checkpoints/epoch=2-step=470.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:03:16.977947Z",
     "iopub.status.busy": "2024-02-27T13:03:16.977623Z",
     "iopub.status.idle": "2024-02-27T13:03:24.844373Z",
     "shell.execute_reply": "2024-02-27T13:03:24.843720Z",
     "shell.execute_reply.started": "2024-02-27T13:03:16.977926Z"
    },
    "id": "-yVoc97oPged",
    "outputId": "06ddd83e-dae5-43f4-b585-fd1056192d07",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: calhoun city , mississippi\n",
      "\n",
      "Actual Entities: loc: calhoun city , mississippi\n",
      "Predicted Entities: loc: calhoun city , mississippi\n",
      "=====================================================================\n",
      "\n",
      "text: honorary fellow of the american institute of architects ( 1993 ) and of the royal institute of\n",
      "british architects ( 1995 ) .\n",
      "\n",
      "Actual Entities: org: american institute of architects; org: royal institute of british architects\n",
      "Predicted Entities: org: american institute of architects; org: royal institute of british architects\n",
      "=====================================================================\n",
      "\n",
      "text: museo di storia naturale di venezia , venice\n",
      "\n",
      "Actual Entities: org: museo di storia naturale di venezia; loc: venice\n",
      "Predicted Entities: loc: museo di storia naturale di venezia\n",
      "=====================================================================\n",
      "\n",
      "text: edgar allan poe in popular culture\n",
      "\n",
      "Actual Entities: org: edgar allan poe in popular culture\n",
      "Predicted Entities: per: edgar allan poe in popular culture\n",
      "=====================================================================\n",
      "\n",
      "text: '' shine my shoes ''\n",
      "\n",
      "Actual Entities: org: shine my shoes\n",
      "Predicted Entities: org: shine my shoes\n",
      "=====================================================================\n",
      "\n",
      "text: värmlands län , seat no .\n",
      "\n",
      "Actual Entities: org: värmlands län\n",
      "Predicted Entities: loc: värmlands län\n",
      "=====================================================================\n",
      "\n",
      "text: their first retail store was set up at shaw house and centre on scotts road .\n",
      "\n",
      "Actual Entities: org: shaw house and centre; loc: scotts road\n",
      "Predicted Entities: org: shaw house and centre; loc: scotts road\n",
      "=====================================================================\n",
      "\n",
      "text: on 28 july 2006 , it was sold to air greenland .\n",
      "\n",
      "Actual Entities: org: air greenland\n",
      "Predicted Entities: org: air greenland\n",
      "=====================================================================\n",
      "\n",
      "text: buchanan 's birthplace state park ( franklin county )\n",
      "\n",
      "Actual Entities: org: buchanan 's birthplace state park; loc: franklin county\n",
      "Predicted Entities: loc: buchanan 's birthplace state park; loc: frankli\n",
      "=====================================================================\n",
      "\n",
      "text: *american singer-guitarist glen campbell covered the song on his 12th album wichita lineman ''\n",
      ", released in 1968 by capitol records .\n",
      "\n",
      "Actual Entities: per: glen campbell; org: wichita lineman; org: capitol records\n",
      "Predicted Entities: per: glen campbell; org: wichita lineman;\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "dataloader = DataLoader(input_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
    "model.model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "outputs = []\n",
    "targets = []\n",
    "texts = []\n",
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    texts.extend(text)\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    break\n",
    "\n",
    "for i in range(10):\n",
    "    c = texts[i]\n",
    "    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual Entities: %s\" % target[i])\n",
    "    print(\"Predicted Entities: %s\" % outputs[i])\n",
    "    print(\"=====================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T13:04:07.805434Z",
     "iopub.status.busy": "2024-02-27T13:04:07.805085Z",
     "iopub.status.idle": "2024-02-27T13:04:07.811701Z",
     "shell.execute_reply": "2024-02-27T13:04:07.811208Z",
     "shell.execute_reply.started": "2024-02-27T13:04:07.805408Z"
    },
    "id": "Q358Ph_JUeSA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_sub_list(sl, l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind:ind+sll] == sl:\n",
    "            results.append((ind, ind+sll-1))\n",
    "    return results\n",
    "\n",
    "def generate_label(input: str, target: str):\n",
    "    mapper = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-PER': 3,\n",
    "              'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8}\n",
    "    inv_mapper = {v: k for k, v in mapper.items()}\n",
    "\n",
    "    input = input.split(\" \")\n",
    "    target = target.split(\"; \")\n",
    "\n",
    "    init_target_label = [mapper['O']]*len(input)\n",
    "\n",
    "    for ent in target:\n",
    "        ent = ent.split(\": \")\n",
    "        try:\n",
    "            sent_end = ent[1].split(\" \")\n",
    "            index = find_sub_list(sent_end, input)\n",
    "        except:\n",
    "            continue\n",
    "        # print(index)\n",
    "        try:\n",
    "            init_target_label[index[0][0]] = mapper[f\"B-{ent[0].upper()}\"]\n",
    "            for i in range(index[0][0]+1, index[0][1]+1):\n",
    "                init_target_label[i] = mapper[f\"I-{ent[0].upper()}\"]\n",
    "        except:\n",
    "            continue\n",
    "    init_target_label = [inv_mapper[j] for j in init_target_label]\n",
    "    return init_target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:04:11.030985Z",
     "iopub.status.busy": "2024-02-27T13:04:11.030664Z",
     "iopub.status.idle": "2024-02-27T13:05:34.504615Z",
     "shell.execute_reply": "2024-02-27T13:05:34.504054Z",
     "shell.execute_reply.started": "2024-02-27T13:04:11.030962Z"
    },
    "id": "s8b7-Y07T5qV",
    "outputId": "6ff28c77-103b-4a3a-f50b-0b3c1dbee16c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [01:18<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32,\n",
    "                             num_workers=2, shuffle=True)\n",
    "model.model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "outputs = []\n",
    "targets = []\n",
    "all_text = []\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['source_ids'].to(\"cuda\")\n",
    "    attention_mask = batch['source_mask'].to(\"cuda\")\n",
    "    outs = model.model.generate(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask)\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "    pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    true_labels.extend(true_label)\n",
    "    pred_labels.extend(pred_label)\n",
    "    all_text.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:05:43.154403Z",
     "iopub.status.busy": "2024-02-27T13:05:43.154049Z",
     "iopub.status.idle": "2024-02-27T13:05:43.159246Z",
     "shell.execute_reply": "2024-02-27T13:05:43.158785Z",
     "shell.execute_reply.started": "2024-02-27T13:05:43.154377Z"
    },
    "id": "MTDdTHwdadFx",
    "outputId": "845d8b90-cd00-490e-f0a9-96d71eff167e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"orguss '' - additional voices\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:06:23.049881Z",
     "iopub.status.busy": "2024-02-27T13:06:23.049554Z",
     "iopub.status.idle": "2024-02-27T13:06:28.187486Z",
     "shell.execute_reply": "2024-02-27T13:06:28.186898Z",
     "shell.execute_reply.started": "2024-02-27T13:06:23.049861Z"
    },
    "id": "ZrFA-BSHerhf",
    "outputId": "3df4f246-e26e-46ff-dd92-cf5a6da5cc4f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  reginald beckwith as kenniston .\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'O', 'B-PER', 'O']\n",
      "True Token Class:  ['B-PER', 'I-PER', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  orguss '' - additional voices\n",
      "Predicted Token Class:  ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  dihedral symmetry groups with even-orders have a number of subgroups .\n",
      "Predicted Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  council of southern africa football associations\n",
      "Predicted Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "=====================================================================\n",
      "\n",
      "Text:  he died in 1924 and was buried in the hôtel des invalides .\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  paul warren rieger .\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O']\n",
      "True Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  diggy liggy lo '' ( j. d. miller ) – 2:16\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  '' pereza '' '\n",
      "Predicted Token Class:  ['O', 'B-LOC', 'O', 'O']\n",
      "True Token Class:  ['O', 'B-ORG', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  atkinson , new hampshire\n",
      "Predicted Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "True Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "=====================================================================\n",
      "\n",
      "Text:  san diego county , california\n",
      "Predicted Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "True Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "=====================================================================\n",
      "\n",
      "{'LOC': {'precision': 0.7034047919293821, 'recall': 0.6028966709900562, 'f1': 0.6492841345594227, 'number': 4626}, 'ORG': {'precision': 0.6855995410212278, 'recall': 0.5156418554476807, 'f1': 0.5885974633665805, 'number': 4635}, 'PER': {'precision': 0.7557732680195941, 'recall': 0.7139709122961657, 'f1': 0.7342776203966006, 'number': 4538}, 'overall_precision': 0.7172431419321861, 'overall_recall': 0.6101166751213856, 'overall_f1': 0.6593570113952305, 'overall_accuracy': 0.8310771188661119}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Text:  {all_text[i]}\")\n",
    "    print(f\"Predicted Token Class:  {pred_labels[i]}\")\n",
    "    print(f\"True Token Class:  {true_labels[i]}\")\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n",
    "print(metric.compute(predictions=pred_labels, references=true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF7loiUFVbQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "T5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "005a930db3d242a4be28a9c5923eed76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0305993a3c1a4f8296faeb1aba61fa38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0573bd35717d4189ab4d74fdc489653b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd7b7a9f5f80492997ec2cbd4bf22627",
       "IPY_MODEL_1b08c691a8db4cb79e6f7837a08ac799",
       "IPY_MODEL_b904bbebe38943728a23a0d630f86a88"
      ],
      "layout": "IPY_MODEL_06e6627b73a24e4291f316a281a20aea"
     }
    },
    "06e6627b73a24e4291f316a281a20aea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "0e46dd857d504def992f555a9e9c9c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "127c67d7658d4bf392408e7d5600a09c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b08c691a8db4cb79e6f7837a08ac799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f69895f136a14e629614a45f2e7d8557",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfcb95376da04ce884ab18c57c2d63ed",
      "value": 1250
     }
    },
    "1fac6af61b4d411a94f3916061731428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d7306a9b474da3b0c8e80a3d7abe09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_264fdba12e854d54b3ed2510458c7a1b",
      "placeholder": "​",
      "style": "IPY_MODEL_95d7c71216094ea0b466a79efc95341d",
      "value": " 1250/1250 [02:30&lt;00:00,  8.20it/s]"
     }
    },
    "21b88aca18f048759fb25ce63540c992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25b71debb82040a2960eb8348df2a4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "264fdba12e854d54b3ed2510458c7a1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285649e2ae134f87bd2f8dcf40a7758d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3e24f0b77884455868b103ab69ddd5d",
      "placeholder": "​",
      "style": "IPY_MODEL_c3c43f7938044239b4eca004d07874ae",
      "value": "100%"
     }
    },
    "304c605d63584a298e8cefd4ffcf4ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfdd904fb4924ad8948793222255cc3b",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d88a23982ac46b3b8077264a6998b9a",
      "value": 3
     }
    },
    "347f2c97cd4f46aa8bd1d30f79041b29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37a7f3f9244a4fd991fe9e8847268ec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fce9ba2896164d4391fea7f32dc3d811",
      "placeholder": "​",
      "style": "IPY_MODEL_0305993a3c1a4f8296faeb1aba61fa38",
      "value": " 1250/1250 [02:30&lt;00:00,  8.20it/s]"
     }
    },
    "388d8e1e34a143c3afe9e4082be44ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3898654592194daca51999f513192a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ceb75a4fa14f6e9a8a5f4ae1324cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4bba40ddade644b79cce613925a4f47e",
       "IPY_MODEL_aa38ae45e85545bebec6c8b43db7abbf",
       "IPY_MODEL_37a7f3f9244a4fd991fe9e8847268ec1"
      ],
      "layout": "IPY_MODEL_49901550f7c14e439505f80faaaa8357"
     }
    },
    "3b924323276f4f92a66a64d295ba6c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ddd6cf350fb4378ad6cf2698f67d68f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21b88aca18f048759fb25ce63540c992",
      "placeholder": "​",
      "style": "IPY_MODEL_3b924323276f4f92a66a64d295ba6c78",
      "value": "Validating: 100%"
     }
    },
    "429fbc36c65446d09d32298cfc4b606e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43278de125c54c1e8b55baa0370c4cc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45340f3b0bf4413182060f27c7458334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f16174670d2c4182ace507dda7812328",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b77941ef046babfb1e9ed597a6aee",
      "value": 3
     }
    },
    "47144694b20f49f0aa54ee1d83119fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48bb2d6d069e411a9e969187d5ddf17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81211b58e69b4973a9e1b500e621656b",
      "placeholder": "​",
      "style": "IPY_MODEL_347f2c97cd4f46aa8bd1d30f79041b29",
      "value": "Epoch 2: 100%"
     }
    },
    "49901550f7c14e439505f80faaaa8357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4b15ff3c6b044b40a15b21f051bf53c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4bba40ddade644b79cce613925a4f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce10ff94a3454674a5deb511394387ff",
      "placeholder": "​",
      "style": "IPY_MODEL_b537afbec50e4567885220cf27a5b761",
      "value": "Validating: 100%"
     }
    },
    "4fb2b190729943cb8c1f1fd2b84d6935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fffbd7c89314024a7b8c128ad1ba248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4e33c5404f49caa4811bfd55b0bee5",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce97e8d1d6b54e17a16ec75e1b3abf7f",
      "value": 3750
     }
    },
    "5587ced0255640dd9bec68478cd973fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "56496f0e8cef4b5cb989f289af418f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d4e33c5404f49caa4811bfd55b0bee5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d88a23982ac46b3b8077264a6998b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fd2178313d34631a030d79934dcffe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4dc2a989d794b44b8000ca88cf095fa",
      "placeholder": "​",
      "style": "IPY_MODEL_b449b3c3b1b944a38293f2f13d517860",
      "value": " 3/3 [00:00&lt;00:00, 64.65it/s]"
     }
    },
    "6bf0078bd11d4ed980f1b7a9b612a091": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbcd62725c684fcc897583042ca44888",
      "placeholder": "​",
      "style": "IPY_MODEL_e61579f72d51414782f8fe3c28e12e86",
      "value": " 3/3 [00:00&lt;00:00, 73.09it/s]"
     }
    },
    "79385887dd0f479d993b251748f8cb2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ca93c1fe9a9415e9c7d6f22a58b90d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa0489927bdd4bf4809c9fcc92d9d7b4",
      "placeholder": "​",
      "style": "IPY_MODEL_43278de125c54c1e8b55baa0370c4cc1",
      "value": " 3/3 [00:00&lt;00:00, 48.52it/s]"
     }
    },
    "81211b58e69b4973a9e1b500e621656b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83ec7a9ea11e410bb3fcad13c082d664": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_429fbc36c65446d09d32298cfc4b606e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79385887dd0f479d993b251748f8cb2f",
      "value": 3
     }
    },
    "84ab2eb4f0b74ac3a334eb7ecd7c4d63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "84c7612e3ed740958159eec4ee147476": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93c2ea7279794b88b6264f832400e3c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5a571c48de54900b2b696f2c6817a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_4fb2b190729943cb8c1f1fd2b84d6935",
      "value": "100%"
     }
    },
    "9428191170994f0aa7fd400caf0d1d07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d7c71216094ea0b466a79efc95341d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966b77941ef046babfb1e9ed597a6aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97e269fbcb954d629b2314d30c41c57b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93c2ea7279794b88b6264f832400e3c7",
       "IPY_MODEL_45340f3b0bf4413182060f27c7458334",
       "IPY_MODEL_6bf0078bd11d4ed980f1b7a9b612a091"
      ],
      "layout": "IPY_MODEL_84c7612e3ed740958159eec4ee147476"
     }
    },
    "9cd468cf60c643db96d0cd35923c8258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0a3db286a1843b293b1d3b944a3d33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9428191170994f0aa7fd400caf0d1d07",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25b71debb82040a2960eb8348df2a4ca",
      "value": 2
     }
    },
    "a5a571c48de54900b2b696f2c6817a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa38ae45e85545bebec6c8b43db7abbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d26f30f1432e4f2ab1c53484b44989ee",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be10758757c1426c8779c6706a445ecd",
      "value": 1250
     }
    },
    "b112c9a8184d4b479fc0e59698789b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebcc288979ff4cd5901e9223063751d1",
       "IPY_MODEL_304c605d63584a298e8cefd4ffcf4ead",
       "IPY_MODEL_7ca93c1fe9a9415e9c7d6f22a58b90d1"
      ],
      "layout": "IPY_MODEL_e550224ab580420cb4973b805c2393af"
     }
    },
    "b3e24f0b77884455868b103ab69ddd5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b449b3c3b1b944a38293f2f13d517860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b537afbec50e4567885220cf27a5b761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b58e5e54e53e46d7a4f196a93812373c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b849b43f5c4649deb018fb959191edc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b904bbebe38943728a23a0d630f86a88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3898654592194daca51999f513192a24",
      "placeholder": "​",
      "style": "IPY_MODEL_47144694b20f49f0aa54ee1d83119fda",
      "value": " 1250/1250 [02:30&lt;00:00,  8.22it/s]"
     }
    },
    "ba1298c2f3db42c48b7e3e3ee83d5000": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_285649e2ae134f87bd2f8dcf40a7758d",
       "IPY_MODEL_83ec7a9ea11e410bb3fcad13c082d664",
       "IPY_MODEL_5fd2178313d34631a030d79934dcffe0"
      ],
      "layout": "IPY_MODEL_0e46dd857d504def992f555a9e9c9c5f"
     }
    },
    "ba6f4922558c41fea4a4b171a021b871": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48bb2d6d069e411a9e969187d5ddf17d",
       "IPY_MODEL_4fffbd7c89314024a7b8c128ad1ba248",
       "IPY_MODEL_dea06faaa22b41b6bbfd7ff6d8ee39f6"
      ],
      "layout": "IPY_MODEL_5587ced0255640dd9bec68478cd973fb"
     }
    },
    "bd7b7a9f5f80492997ec2cbd4bf22627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cddc7d98bbb740a9a81bf8e244d4595d",
      "placeholder": "​",
      "style": "IPY_MODEL_9cd468cf60c643db96d0cd35923c8258",
      "value": "Validating: 100%"
     }
    },
    "be10758757c1426c8779c6706a445ecd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3c43f7938044239b4eca004d07874ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4dc2a989d794b44b8000ca88cf095fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cddc7d98bbb740a9a81bf8e244d4595d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce10ff94a3454674a5deb511394387ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce97e8d1d6b54e17a16ec75e1b3abf7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ceca8a9a02fe46d5a1a3aa093fdbcde7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b849b43f5c4649deb018fb959191edc5",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b58e5e54e53e46d7a4f196a93812373c",
      "value": 1250
     }
    },
    "cfcb95376da04ce884ab18c57c2d63ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d26f30f1432e4f2ab1c53484b44989ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2b327e2b0074354a6b889afc169a62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8ebf6d2f2954cbbb525e67de58a766a",
       "IPY_MODEL_a0a3db286a1843b293b1d3b944a3d33a",
       "IPY_MODEL_d68d5a7c97bd47719eb1e19f67d64805"
      ],
      "layout": "IPY_MODEL_84ab2eb4f0b74ac3a334eb7ecd7c4d63"
     }
    },
    "d37bf18a5e9346fcb59242e0e5288307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d68d5a7c97bd47719eb1e19f67d64805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dffd33c1015f4bfd91fd702095c2127e",
      "placeholder": "​",
      "style": "IPY_MODEL_ef4fa30c44df4726a36cfc1da100fb4f",
      "value": " 2/2 [00:00&lt;00:00,  3.48it/s]"
     }
    },
    "dbcd62725c684fcc897583042ca44888": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dea06faaa22b41b6bbfd7ff6d8ee39f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_005a930db3d242a4be28a9c5923eed76",
      "placeholder": "​",
      "style": "IPY_MODEL_d37bf18a5e9346fcb59242e0e5288307",
      "value": " 3750/3750 [15:58&lt;00:00,  3.91it/s, loss=0.103, v_num=0]"
     }
    },
    "dfdd904fb4924ad8948793222255cc3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dffd33c1015f4bfd91fd702095c2127e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e550224ab580420cb4973b805c2393af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61579f72d51414782f8fe3c28e12e86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebcc288979ff4cd5901e9223063751d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_388d8e1e34a143c3afe9e4082be44ebb",
      "placeholder": "​",
      "style": "IPY_MODEL_56496f0e8cef4b5cb989f289af418f9f",
      "value": "100%"
     }
    },
    "ef4fa30c44df4726a36cfc1da100fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f16174670d2c4182ace507dda7812328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f69895f136a14e629614a45f2e7d8557": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8ebf6d2f2954cbbb525e67de58a766a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_127c67d7658d4bf392408e7d5600a09c",
      "placeholder": "​",
      "style": "IPY_MODEL_1fac6af61b4d411a94f3916061731428",
      "value": "Validation sanity check: 100%"
     }
    },
    "fa0489927bdd4bf4809c9fcc92d9d7b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce9ba2896164d4391fea7f32dc3d811": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffe6027ef4bb4fd9964717116a39f1a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ddd6cf350fb4378ad6cf2698f67d68f",
       "IPY_MODEL_ceca8a9a02fe46d5a1a3aa093fdbcde7",
       "IPY_MODEL_20d7306a9b474da3b0c8e80a3d7abe09"
      ],
      "layout": "IPY_MODEL_4b15ff3c6b044b40a15b21f051bf53c8"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
