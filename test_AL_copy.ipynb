{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/zhangxl2002/ORL/blob/main/T5_Ner_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T11:46:03.615350Z",
     "iopub.status.busy": "2024-02-28T11:46:03.615160Z",
     "iopub.status.idle": "2024-02-28T11:46:03.618572Z",
     "shell.execute_reply": "2024-02-28T11:46:03.617968Z",
     "shell.execute_reply.started": "2024-02-28T11:46:03.615332Z"
    },
    "id": "4rfSWZLR6E7k",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 21 00:39:59 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A10          Off  | 00000000:00:08.0 Off |                    0 |\n",
      "|  0%   47C    P8    11W / 150W |      0MiB / 22731MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BlIYFZKgmOU"
   },
   "source": [
    "# Named Entity Recognition with T5\n",
    "\n",
    "This notebook shows how to finetune [T5 Model](https://https://huggingface.co/docs/transformers/model_doc/t5) for token classification or named entity recognition with pytorch lighning. In this demo, I used the T5-Small and cast the entities as a text using the text to text framework used in the t5 paper. During Eval the generated tokens are then split and classifies into their specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:23.741652Z",
     "iopub.status.busy": "2024-02-28T14:14:23.741327Z",
     "iopub.status.idle": "2024-02-28T14:14:27.411717Z",
     "shell.execute_reply": "2024-02-28T14:14:27.411232Z",
     "shell.execute_reply.started": "2024-02-28T14:14:23.741632Z"
    },
    "id": "cBQiMj-p5lfz",
    "outputId": "7802de39-2c8b-4e20-c061-164dcbb6af9e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    MT5ForConditionalGeneration,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:29.473867Z",
     "iopub.status.busy": "2024-02-28T14:14:29.473454Z",
     "iopub.status.idle": "2024-02-28T14:14:29.527177Z",
     "shell.execute_reply": "2024-02-28T14:14:29.526681Z",
     "shell.execute_reply.started": "2024-02-28T14:14:29.473846Z"
    },
    "id": "0WqcwP916Dwq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sub_list(sl, l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind:ind+sll] == sl:\n",
    "            results.append((ind, ind+sll-1))\n",
    "    return results\n",
    "\n",
    "def generate_label(input: str, target: str):\n",
    "    mapper = {\n",
    "        \"O\": 0,\n",
    "        \"B-AGENT\": 1,\n",
    "        \"I-AGENT\": 2,\n",
    "        \"B-DSE\": 3,\n",
    "        \"I-DSE\": 4,\n",
    "        \"B-TARGET\": 5,\n",
    "        \"I-TARGET\": 6\n",
    "    }\n",
    "    inv_mapper = {v: k for k, v in mapper.items()}\n",
    "\n",
    "    input = input.split(\" \")\n",
    "    target = target.split(\"; \")\n",
    "\n",
    "    init_target_label = [mapper['O']]*len(input)\n",
    "\n",
    "    for ent in target:\n",
    "        ent = ent.split(\": \")\n",
    "        try:\n",
    "            sent_end = ent[1].split(\" \")\n",
    "            index = find_sub_list(sent_end, input)\n",
    "        except:\n",
    "            continue\n",
    "        # print(index)\n",
    "        try:\n",
    "            init_target_label[index[0][0]] = mapper[f\"B-{ent[0].upper()}\"]\n",
    "            for i in range(index[0][0]+1, index[0][1]+1):\n",
    "                init_target_label[i] = mapper[f\"I-{ent[0].upper()}\"]\n",
    "        except:\n",
    "            continue\n",
    "    init_target_label = [inv_mapper[j] for j in init_target_label]\n",
    "    return init_target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(name, table):\n",
    "    np.save(name, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:14:36.446536Z",
     "iopub.status.busy": "2024-02-28T14:14:36.446076Z",
     "iopub.status.idle": "2024-02-28T14:15:04.104154Z",
     "shell.execute_reply": "2024-02-28T14:15:04.103481Z",
     "shell.execute_reply.started": "2024-02-28T14:14:36.446513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"zhangxl2002/mpqa_ORL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:03.808350Z",
     "iopub.status.busy": "2024-02-28T14:16:03.808004Z",
     "iopub.status.idle": "2024-02-28T14:16:03.814544Z",
     "shell.execute_reply": "2024-02-28T14:16:03.814024Z",
     "shell.execute_reply.started": "2024-02-28T14:16:03.808329Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MPQADataset(Dataset):\n",
    "  def __init__(self, tokenizer, dataset, type_path, max_len=512):\n",
    "\n",
    "    self.data = dataset[type_path]\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.tokenizer.max_length = max_len\n",
    "    self.tokenizer.model_max_length = max_len\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "  def _build(self):\n",
    "    for idx in range(len(self.data)):\n",
    "      input_, target = \" \".join(self.data[idx][\"words\"]), \"; \".join(self.data[idx][\"spans\"])\n",
    "      input_ = input_ + \" DSE:\" + self.data[idx][\"dse\"]\n",
    "\n",
    "      input_ = input_.lower() + ' </s>'\n",
    "      target = target.lower() + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target],max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:07.466521Z",
     "iopub.status.busy": "2024-02-28T14:16:07.466195Z",
     "iopub.status.idle": "2024-02-28T14:16:10.213091Z",
     "shell.execute_reply": "2024-02-28T14:16:10.212619Z",
     "shell.execute_reply.started": "2024-02-28T14:16:07.466502Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../T5-base\")\n",
    "\n",
    "# print(tokenizer)\n",
    "\n",
    "# input_dataset = MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:13.859761Z",
     "iopub.status.busy": "2024-02-28T14:16:13.859213Z",
     "iopub.status.idle": "2024-02-28T14:16:13.865043Z",
     "shell.execute_reply": "2024-02-28T14:16:13.864610Z",
     "shell.execute_reply.started": "2024-02-28T14:16:13.859741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = input_dataset[0]\n",
    "# # print(data)\n",
    "# print(tokenizer.decode(data[\"source_ids\"][400]))\n",
    "# print((tokenizer.decode(data[\"source_ids\"][0:5])))\n",
    "# print(tokenizer.encode(\"<unk>\"))\n",
    "# print(data[\"source_ids\"][0:2])\n",
    "# print(tokenizer.pad_token_id)\n",
    "# print(tokenizer.decode(data[\"source_ids\"], skip_special_tokens=False))\n",
    "# print(tokenizer.decode(data[\"target_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:33.181671Z",
     "iopub.status.busy": "2024-02-28T14:16:33.181353Z",
     "iopub.status.idle": "2024-02-28T14:16:33.185008Z",
     "shell.execute_reply": "2024-02-28T14:16:33.184575Z",
     "shell.execute_reply.started": "2024-02-28T14:16:33.181650Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"zhangxl2002/mpqa_ORL\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:34.903982Z",
     "iopub.status.busy": "2024-02-28T14:16:34.903434Z",
     "iopub.status.idle": "2024-02-28T14:16:34.906468Z",
     "shell.execute_reply": "2024-02-28T14:16:34.905943Z",
     "shell.execute_reply.started": "2024-02-28T14:16:34.903962Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTuner():\n",
    "    def __init__(self, hparam):\n",
    "        self.hparam = hparam\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     hparam.model_name_or_path)\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     \"saved_models/model_epoch_7\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"t5-base\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            hparam.model_name_or_path\n",
    "        )\n",
    "        self.configure_optimizers()\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        # tensorboard_logs = {\"train_loss\": loss}\n",
    "        # return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "        return loss\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            loss = self._step(batch)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparam.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparam.learning_rate, eps=self.hparam.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self,\n",
    "                       epoch=None,\n",
    "                       batch_idx=None,\n",
    "                       optimizer=None,\n",
    "                       optimizer_idx=None,\n",
    "                       optimizer_closure=None,\n",
    "                       on_tpu=None,\n",
    "                       using_native_amp=None,\n",
    "                       using_lbfgs=None\n",
    "                       ):\n",
    "\n",
    "        # optimizer.step(closure=optimizer_closure)\n",
    "        # optimizer.zero_grad()\n",
    "        self.opt.step()\n",
    "        self.opt.zero_grad()\n",
    "        # self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(\n",
    "            self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparam.train_batch_size,\n",
    "                                drop_last=True, shuffle=True, num_workers=2)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) //\n",
    "             (self.hparam.train_batch_size * max(1, self.hparam.n_gpu)))\n",
    "            // self.hparam.gradient_accumulation_steps\n",
    "            * float(self.hparam.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparam.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)\n",
    "    def test_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"test\", args=self.hparam)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:39.522728Z",
     "iopub.status.busy": "2024-02-28T14:16:39.522408Z",
     "iopub.status.idle": "2024-02-28T14:16:46.303899Z",
     "shell.execute_reply": "2024-02-28T14:16:46.303415Z",
     "shell.execute_reply.started": "2024-02-28T14:16:39.522708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import T5ForConditionalGeneration, T5Config\n",
    "# config = T5Config.from_pretrained(\"t5-base\")\n",
    "# # 创建一个与原始模型相同架构的模型实例\n",
    "# model = T5ForConditionalGeneration(config=config)\n",
    "\n",
    "# # 加载之前保存的PyTorch模型的参数\n",
    "# model.load_state_dict(torch.load(\"saved_models/model_epoch_3.pth\"))\n",
    "\n",
    "# # 将模型保存为Hugging Face的格式\n",
    "# model.save_pretrained(\"./test_saved_models\")\n",
    "\n",
    "# # 模型现在已经转换为Hugging Face的形式并保存到指定路径下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointPath = \"saveCheckpointPath/checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    tokenizer.max_length = args.max_seq_length\n",
    "    tokenizer.model_max_length = args.max_seq_length\n",
    "    # dataset = load_dataset(args.data_dir)\n",
    "    dataset = load_dataset(\"zhangxl2002/mpqa_ORL\")\n",
    "    return MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path=type_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.model.to(device)\n",
    "# max_epochs = 10\n",
    "# train_dataloader = model.train_dataloader()\n",
    "# val_dataloader = model.val_dataloader()\n",
    "# for epoch in range(max_epochs):\n",
    "#     # 在验证集上计算损失\n",
    "#     val_loss = 0.0\n",
    "#     num_val_batches = 0\n",
    "#     for i, val_batch in enumerate(val_dataloader):\n",
    "#         val_batch = {key: value.to(device) for key, value in val_batch.items()}  # 将批次数据移动到GPU上\n",
    "#         val_loss += model.validation_step(val_batch, i).item()\n",
    "#         num_val_batches += 1\n",
    "#     avg_val_loss = val_loss / num_val_batches\n",
    "#     print(\"---------------------------------------\")\n",
    "#     print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "#     print(\"---------------------------------------\")\n",
    "    \n",
    "#     total_loss = 0.0\n",
    "#     num_batches = 0\n",
    "#     for i, batch in enumerate(train_dataloader):\n",
    "#         batch = {key: value.to(device) for key, value in batch.items()}  # 将批次数据移动到GPU上\n",
    "#         loss = model.training_step(batch, i)\n",
    "#         loss.backward()\n",
    "#         total_loss += loss.item()\n",
    "#         num_batches += 1\n",
    "\n",
    "#         # 打印每个批次的损失值\n",
    "#         print(f\"Epoch [{epoch + 1}/{max_epochs}], Batch [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "#         # 执行优化步骤\n",
    "#         model.optimizer_step()\n",
    "#     # 计算并打印平均损失值\n",
    "#     avg_loss = total_loss / num_batches\n",
    "#     print(f\"Epoch [{epoch + 1}/{max_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "#     # 在验证集上计算损失\n",
    "#     model.model.eval()\n",
    "#     val_loss = 0.0\n",
    "#     num_val_batches = 0\n",
    "#     for i, val_batch in enumerate(val_dataloader):\n",
    "#         val_batch = {key: value.to(device) for key, value in val_batch.items()}  # 将批次数据移动到GPU上\n",
    "#         val_loss += model.validation_step(val_batch, i).item()\n",
    "#         num_val_batches += 1\n",
    "#     avg_val_loss = val_loss / num_val_batches\n",
    "#     print(\"---------------------------------------\")\n",
    "#     print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "#     print(\"---------------------------------------\")\n",
    "#     model.model.train()\n",
    "\n",
    "\n",
    "#     save_dir = f\"./saved_models/model_epoch_{epoch + 1}_trainloss_{avg_loss}_valloss_{avg_val_loss}\"\n",
    "#     model.model.save_pretrained(save_dir)\n",
    "#     print(f\"Model saved at {save_dir}\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用AL对训练过程进行改写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datapool():\n",
    "    def __init__(self, warmstart_percentage, train_dl):\n",
    "        # self.train_dl = train_dl #\n",
    "        # print(\"total_num:\",len(train_dl.dataset))\n",
    "        self.train_data = self.dataloader_to_list(train_dl)\n",
    "        self.total_num = len(self.train_data)\n",
    "        # print(\"total_num:\",self.total_num)\n",
    "        annotated_num = int(self.total_num * warmstart_percentage)\n",
    "        annotated_indices = random.sample(range(self.total_num), annotated_num)\n",
    "        annotated_indices.sort()\n",
    "        self.annotated_data = [self.train_data[i] for i in annotated_indices]\n",
    "        self.unannotated_data = [self.train_data[i] for i in range(self.total_num) if i not in annotated_indices]\n",
    "        # print(\"len(annotated_data)\",len(self.annotated_data))\n",
    "        # print(\"len(unannotated_data)\",len(self.unannotated_data))\n",
    "        # print(\"self.annotated_data[0]\",self.annotated_data[0])\n",
    "\n",
    "        self.an_dataloader = DataLoader(self.annotated_data, batch_size=8, shuffle=False, drop_last=True, num_workers=2)\n",
    "        self.un_dataloader = DataLoader(self.unannotated_data, batch_size=8, shuffle=False, drop_last=True, num_workers=2)\n",
    "\n",
    "    def dataloader_to_list(self, dataloader):\n",
    "        data_list = []\n",
    "        cnt = 0\n",
    "        for batch in dataloader:  \n",
    "            for i in range(dataloader.batch_size):      \n",
    "                data_list.append({\n",
    "                    'source_ids':batch['source_ids'][i],\n",
    "                    'target_ids':batch['target_ids'][i],\n",
    "                    'source_mask':batch['source_mask'][i],\n",
    "                    'target_mask':batch['target_mask'][i]\n",
    "                })\n",
    "        return data_list\n",
    "\n",
    "    def getAnnotatedData(self):\n",
    "        return self.annotated_data\n",
    "    def getAnnotatedDataloader(self):\n",
    "        return self.an_dataloader\n",
    "    def getUnannotatedData(self):\n",
    "        return self.unannotated_data\n",
    "    def getUnannotatedDataloader(self):\n",
    "        return self.un_dataloader\n",
    "    def addAnnotatedData(self, selected_indices=[]):\n",
    "        annotated_indices = selected_indices\n",
    "        # annotated_indices.sort()\n",
    "        self.annotated_data = self.annotated_data + [self.unannotated_data[i] for i in annotated_indices]\n",
    "        self.unannotated_data = [self.unannotated_data[i] for i in range(len(self.unannotated_data)) if i not in annotated_indices]\n",
    "        self.an_dataloader = DataLoader(self.annotated_data, batch_size=8, shuffle=True, drop_last=True, num_workers=2)\n",
    "        self.un_dataloader = DataLoader(self.unannotated_data, batch_size=8, shuffle=False, drop_last=True, num_workers=2)\n",
    "        self.showDetail()\n",
    "    def showDetail(self):\n",
    "        print(\"annotated_samples_num:\", len(self.an_dataloader.dataset))\n",
    "        print(\"unannotated_samples_num:\", len(self.un_dataloader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5FineTunerWithAL(T5FineTuner):\n",
    "    def __init__(self, hparam, strategy_name=\"RANDOM\", warmstart_percentage=0.05):\n",
    "        super().__init__(hparam)\n",
    "        self.warmstart_percentage = warmstart_percentage\n",
    "        self.strategy = {\n",
    "            # \"RANDOM\": RandomStrategy,\n",
    "            # \"LC\": LeastConfidenceStrategy,\n",
    "            # \"NLC\": NormalizedLeastConfidenceStrategy,\n",
    "            # \"LTP\": LeastTokenProbabilityStrategy,\n",
    "            # \"MTP\": MinimumTokenProbabilityStrategy,\n",
    "            # \"MTE\": MaximumTokenEntropyStrategy,\n",
    "            # \"LONG\": LongStrategy,\n",
    "            # \"TE\": TokenEntropyStrategy,\n",
    "        }\n",
    "        self.strategy_name = strategy_name\n",
    "        # self.strategy = self.strategy[strategy_name]\n",
    "        self.datapool = Datapool(warmstart_percentage, super().train_dataloader())\n",
    "    def resetModel(self):\n",
    "        # 重置模型权重\n",
    "        del self.model\n",
    "        # self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "        #     \"saved_models/model_epoch_7\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            \"t5-base\")\n",
    "        # 重置优化器\n",
    "        del self.opt\n",
    "        self.configure_optimizers()\n",
    "    def resetDatapool(self):\n",
    "        del self.datapool\n",
    "        self.datapool = Datapool(self.warmstart_percentage, super().train_dataloader())\n",
    "    def update_datapool(self, strategy=\"RANDOM\", add_percentage=0.0):\n",
    "        un_data = self.datapool.getUnannotatedData()\n",
    "        if strategy == \"RANDOM\":\n",
    "            add_num = int(self.datapool.total_num * add_percentage)\n",
    "            selected_idx = random.sample(range(len(self.datapool.unannotated_data)), add_num)\n",
    "            # 展示得分高的 和得分低的数据\n",
    "            show_num = 10\n",
    "            print(\"--------------------show samples-----------------------\")\n",
    "            for sample in range(show_num):\n",
    "                print(\"source:\",self.tokenizer.decode(un_data[sample][\"source_ids\"], skip_special_tokens=True))\n",
    "                print(\"target:\",self.tokenizer.decode(un_data[sample][\"target_ids\"], skip_special_tokens=True))\n",
    "            self.datapool.addAnnotatedData(selected_idx)\n",
    "            return\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.model.to(device)\n",
    "        cnt = 0\n",
    "\n",
    "        un_dataloader = DataLoader(un_data, batch_size=8, shuffle=False)\n",
    "        \n",
    "        # for i in range(len(un_data)):\n",
    "        #     outputs = model.model.generate(input_ids=un_data[i][\"source_ids\"],\\\n",
    "        #                 attention_mask=un_data[i][\"source_mask\"],return_dict_in_generate=True,output_scores=True,max_length=100)\n",
    "        #     #output.scores是一个长度为输出的token数量的元组，元组中的每个元素为[batchsize,词表大小]的tensor\n",
    "        #     transition_scores = model.model.compute_transition_scores(\n",
    "        #         outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        #     )\n",
    "\n",
    "        # TODO: 删除旧策略，新策略使用beam search找到得分最高的几个sequence\n",
    "\n",
    "        # TODO:number_beams调大一点\n",
    "        # 不同iter写死不同的epochnum\n",
    "        # 应该采用所有epoch中在验证集上表现最较好的一个来进行主动学习\n",
    "        # 保存断点\n",
    "        # 是否数据集本身存在问题？\n",
    "            \n",
    "        #output.scores是一个长度为输出的token数量的元组，元组中的每个元素为[batchsize*num_return_sequences,词表大小]的tensor\n",
    "        if add_percentage > 0:\n",
    "            if strategy == \"BEAM\":\n",
    "                beam_scores=[]\n",
    "                number_beams = 2\n",
    "                len_penalty = 0.0\n",
    "                idx = 0\n",
    "                for i, batch in enumerate(un_dataloader):\n",
    "                    batch = {key: value.to(device) for key, value in batch.items()} \n",
    "                    outputs = model.model.generate(\n",
    "                        input_ids=batch[\"source_ids\"],\n",
    "                        attention_mask=batch[\"source_mask\"],\n",
    "                        num_beams=number_beams,\n",
    "                        num_return_sequences=number_beams,\n",
    "                        return_dict_in_generate=True,\n",
    "                        output_scores=True,\n",
    "                        length_penalty=len_penalty,\n",
    "                        max_length=500\n",
    "                    )\n",
    "                    transition_scores = model.model.compute_transition_scores(\n",
    "                        outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=True\n",
    "                    )\n",
    "                    # compute_transition_scores计算出来的transition_scores是一个[batchsize*num_return_sequences,输出的token数量]的tensor，相当于对于每个输出的token一个评分\n",
    "\n",
    "                    # input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "                    # 由于t5是encoder-decoder架构，所以这里input_length取1\n",
    "                    generated_tokens = outputs.sequences[:, 1:] \n",
    "                    # generated_tokens是一个[batchsize,输出的token数量]的tensor，相当于每一个输出的token对应的id\n",
    "\n",
    "                    output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
    "                    reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**len_penalty)\n",
    "                    # 遍历batch中的每一条样本，计算对每一条样本的不确定程度\n",
    "                    for i in range(0,int(generated_tokens.shape[0]/number_beams)):\n",
    "                        beam_scores.append((idx, reconstructed_scores[i*number_beams]))\n",
    "                        idx += 1\n",
    "                sorted_beam_scores = sorted(beam_scores, key=lambda x: x[1])\n",
    "                # print(\"sorted_beam_scores:\", sorted_beam_scores)\n",
    "                    \n",
    "                selected_idx = [item[0] for item in sorted_beam_scores][:int(len(sorted_beam_scores) * add_percentage)]\n",
    "                self.datapool.addAnnotatedData(selected_idx)\n",
    "            elif strategy == \"OTHER\":\n",
    "                # 假设已经知道实际的标签，根据产生的内容和实际的标签进行对比\n",
    "                overall_f1_scores = []\n",
    "                metric = load_metric(\"seqeval\")\n",
    "                len_penalty = 0.0\n",
    "                idx = 0\n",
    "                for i, batch in enumerate(un_dataloader):\n",
    "                    batch = {key: value.to(device) for key, value in batch.items()} \n",
    "                    input_ids = batch['source_ids']\n",
    "                    attention_mask = batch['source_mask']\n",
    "                    outs = model.model.generate(input_ids=input_ids,\n",
    "                                                attention_mask=attention_mask)\n",
    "                    dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                                            clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "                    target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                                for ids in batch[\"target_ids\"]]\n",
    "                    texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                                for ids in batch[\"source_ids\"]]\n",
    "                    true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "                        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "                    pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "                        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "                    for i in range(len(batch[\"source_ids\"])):\n",
    "                        overall_f1_scores.append((idx,metric.compute(predictions=[pred_label[i]], references=[true_label[i]])['overall_f1']))\n",
    "                        idx += 1\n",
    "                sorted_overall_f1_scores = sorted(overall_f1_scores, key=lambda x: x[1])\n",
    "                print(sorted_overall_f1_scores[:10])\n",
    "                selected_idx = [item[0] for item in sorted_overall_f1_scores][:int(len(sorted_overall_f1_scores) * add_percentage)]\n",
    "                self.datapool.addAnnotatedData(selected_idx)            \n",
    "\n",
    "        # print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))\n",
    "        \n",
    "\n",
    "    # def val_dataloader(self):\n",
    "    #     val_dataset = get_dataset(\n",
    "    #         tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n",
    "    #     return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设这是你的两个二维数组\n",
    "\n",
    "# array1 = np.load(\"OTHER_result/OTHER_test_F1_overall_record.npy\")[:-1]\n",
    "# array2 = np.array([\n",
    "#     [0.3919766 , 0.43706224, 0.48401826, 0.50829171, 0.46697039, 0.45134666, 0.46143705, 0.50239533, 0.44962166, 0.47957993, 0.        ],\n",
    "#     [0.44843243, 0.48948765, 0.53057918, 0.53354633, 0.54318374, 0.52995008, 0.54117647, 0.5549958 , 0.54854175, 0.54972376, 0.        ],\n",
    "#     [0.5159596 , 0.51100244, 0.53495066, 0.52618557, 0.53450493, 0.56368564, 0.5617039 , 0.570604  , 0.56399584, 0.56173842, 0.        ],\n",
    "#     [0.52214547, 0.55932203, 0.54526621, 0.56973657, 0.53943692, 0.55845449, 0.55762987, 0.55578684, 0.57567623, 0.57160545, 0.        ],\n",
    "#     [0.49553398, 0.52719495, 0.52411509, 0.5567418 , 0.57843341, 0.58064516, 0.57746777, 0.56535815, 0.56737297, 0.57748777, 0.        ],\n",
    "#     [0.54474867, 0.56700824, 0.57449443, 0.567718  , 0.5724367 , 0.58377688, 0.59086079, 0.59287894, 0.58019923, 0.57206749, 0.        ],\n",
    "#     [0.55152962, 0.53707089, 0.57739558, 0.58707692, 0.58072241, 0.60165223, 0.59777424, 0.60672731, 0.57714286, 0.57736905, 0.        ],\n",
    "#     [0.5247691 , 0.58113362, 0.5768162 , 0.58033673, 0.59340215, 0.59010309, 0.5845115 , 0.59825509, 0.58798999, 0.57053942, 0.        ],\n",
    "#     [0.5792089 , 0.55747966, 0.59683313, 0.58494799, 0.5950041 , 0.600244  , 0.58571122, 0.57721621, 0.58067158, 0.58794788, 0.        ],\n",
    "#     [0.55697227, 0.556917  , 0.59746203, 0.60741916, 0.59250052, 0.57984752, 0.59491595, 0.5828039 , 0.59759635, 0.57862154, 0.        ],\n",
    "#     [0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        , 0.        ]\n",
    "# ])[:-1]\n",
    "array2 = np.load(\"RANDOM_test_F1_overall_record.npy\")[:-1]\n",
    "array3 = np.load(\"BEAM_test_F1_overall_record.npy\")[:-1]\n",
    "# array4 = np.load(\"BEAM_result/BEAM_test_F1_overall_record.npy\")[:-1]\n",
    "# 求每行的最大值\n",
    "# max_values_array1 = np.amax(array1, axis=1)\n",
    "max_values_array2 = np.amax(array2, axis=1)\n",
    "max_values_array3 = np.amax(array3, axis=1)\n",
    "# max_values_array4 = np.amax(array4, axis=1)\n",
    "array0 = np.array([0.65965985]*len(max_values_array2))\n",
    "# 绘制折线图\n",
    "# plt.plot(max_values_array1, label='')\n",
    "plt.plot(max_values_array2, label='RANDOM')\n",
    "plt.plot(max_values_array3, label='LEAST CONFIDENT')\n",
    "# plt.plot(max_values_array4, label='LEAST CONFIDENT')\n",
    "plt.plot(array0, linestyle=\"--\",label='FULL')\n",
    "\n",
    "# 添加图例、标题和标签\n",
    "plt.legend()\n",
    "plt.title('titile')\n",
    "plt.xlabel('iter num')\n",
    "plt.ylabel('overall f1')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60675422, 0.65965985, 0.64662226, 0.6496465 , 0.65445332,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array0 = np.load(\"FULL_test_F1_overall_record.npy\")\n",
    "array0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array1 = np.load(\"OTHER_test_F1_overall_record.npy\")[:-1]\n",
    "# np.amax(array1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = T5FineTunerWithAL(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_beams=4 跑一次7分半\n",
    "# model.update_datapool(strategy=\"OTHER\",add_percentage=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 测试加入到annotateddata中的数据是否是预期中指标排在前面的数据\n",
    "# metric = load_metric(\"seqeval\")\n",
    "# top5_samples = model.datapool.getAnnotatedData()[0:2]\n",
    "# top5_dl = DataLoader(top5_samples, batch_size=8, shuffle=False, drop_last=False, num_workers=2)\n",
    "# for i, batch in enumerate(top5_dl):\n",
    "#     batch = {key: value.to(\"cuda\") for key, value in batch.items()} \n",
    "#     input_ids = batch['source_ids']\n",
    "#     attention_mask = batch['source_mask']\n",
    "#     outs = model.model.generate(input_ids=input_ids,\n",
    "#                                 attention_mask=attention_mask,\n",
    "#                                 max_length=500,\n",
    "#                                 num_beams=10)\n",
    "#     print(outs)\n",
    "#     dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "#                             clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "#     target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "#                 for ids in batch[\"target_ids\"]]\n",
    "#     texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "#                 for ids in batch[\"source_ids\"]]\n",
    "#     true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "#         \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "#     pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "#         \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "#     for i in range(len(batch[\"source_ids\"])):\n",
    "#         print(metric.compute(predictions=[pred_label[i]], references=[true_label[i]])['overall_f1'])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试加入到annotateddata中的数据是否是预期中指标排在前面的数据\n",
    "# top5_samples = model.datapool.getAnnotatedData()[0:3]\n",
    "# top5_dl = DataLoader(top5_samples, batch_size=8, shuffle=False, drop_last=False, num_workers=2)\n",
    "\n",
    "# for i, batch in enumerate(top5_dl):\n",
    "#     batch = {key: value.to(\"cuda\") for key, value in batch.items()} \n",
    "#     outputs = model.model.generate(\n",
    "#         input_ids=batch[\"source_ids\"],\n",
    "#         attention_mask=batch[\"source_mask\"],\n",
    "#         num_beams=4,\n",
    "#         num_return_sequences=4,\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         length_penalty=1.0,\n",
    "#         max_length=100\n",
    "#     )\n",
    "#     transition_scores = model.model.compute_transition_scores(\n",
    "#     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=True\n",
    "#     )\n",
    "#     output_length = np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
    "#     length_penalty = 1\n",
    "#     reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
    "#     torch.set_printoptions(precision=20)\n",
    "#     print(outputs.sequences_scores)\n",
    "#     print(reconstructed_scores)\n",
    "#     print(np.exp(outputs.sequences_scores.cpu()))\n",
    "#     print(np.exp(reconstructed_scores))\n",
    "#     print(np.exp(reconstructed_scores).sum())\n",
    "#     print(np.allclose(outputs.sequences_scores.cpu(), reconstructed_scores))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(\"MIN_result_v2/MIN_val_F1_overall_record.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_10485/777656607.py:166: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/c8563af43bdce095d0f9e8b8b79c9c96d5ea5499b3bf66f90301c9cb82910f11 (last modified on Tue Feb 27 21:06:26 2024) since it couldn't be found locally at seqeval, or remotely on the Hugging Face Hub.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# 使用AL的做法\n",
    "# model = T5FineTunerWithAL(args)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_file_name = f\"{current_time}.log\"\n",
    "\n",
    "write_log = True\n",
    "\n",
    "# 将标准输出重定向到文件\n",
    "if write_log:\n",
    "    sys.stdout = open('log/'+log_file_name, 'w+')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "max_epochs = 15\n",
    "epoch_nums = [6,7,8,9,10,11,12,13,14,15,15,15]\n",
    "\n",
    "\n",
    "max_iters = 12 # 每个iterator选择新的数据加入已标注数据的集合并重新训练模型\n",
    "\n",
    "# 初始化，默认初始标注数据占总数量的1%，使用RANDOM选择策略\n",
    "model = T5FineTunerWithAL(args) \n",
    "\n",
    "# 验证集始终是一样的\n",
    "test_dataloader = model.test_dataloader()\n",
    "# test_dataloader = model.test_dataloader()\n",
    "\n",
    "# strategies = [\"RANDOM\",\"BEAM\",\"OTHER\"]\n",
    "strategies = [\"RANDOM\"]\n",
    "\n",
    "print_log = True\n",
    "\n",
    "# 记录loss和准确率\n",
    "# loss：键为(strategy,iter,epoch 三元组 值为train_loss或者val_loss\n",
    "train_loss_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "val_loss_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "val_F1_overall_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "val_F1_AGENT_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "val_F1_TARGET_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "\n",
    "test_loss_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "test_F1_overall_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "test_F1_AGENT_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "test_F1_TARGET_record = np.zeros((len(strategies) + 1, max_iters + 1, max_epochs + 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# new_folder = \"model_newoneone\"\n",
    "# base_dir = f\"./saved_models/strategy_test\"\n",
    "# full_dir = base_dir + \"/\" + new_folder\n",
    "# model.model.save_pretrained(full_dir)\n",
    "# print(f\"Model saved at {full_dir}\")\n",
    "# files = os.listdir(base_dir)\n",
    "# for file in files:\n",
    "#     if file == new_folder:\n",
    "#         continue\n",
    "#     old_model_path = os.path.join(base_dir, file)\n",
    "#     if os.path.isdir(old_model_path) and file.startswith(\"model\"):\n",
    "#         shutil.rmtree(old_model_path)\n",
    "#         print(\"delete:\",{file})\n",
    "\n",
    "# print(\"-------------test finish--------------------\")\n",
    "\n",
    "for strategy_id in range(len(strategies)):\n",
    "    current_strategy = strategies[strategy_id] \n",
    "    if print_log: print(\"strategy:\", current_strategy)\n",
    "\n",
    "    # 当前的策略，能够达到的最好的f1分数\n",
    "    max_overall_f1 = 0.0 \n",
    "    for iter in range(max_iters):\n",
    "        if print_log: print(\"iter:\", iter)\n",
    "        # 对每轮新的数据都重新训练\n",
    "        model.resetModel()\n",
    "        if print_log: model.datapool.showDetail()\n",
    "        model.model.to(device)\n",
    "\n",
    "        # 训练集每轮更新，需要重新加载\n",
    "        annotated_dataloader = model.datapool.getAnnotatedDataloader()\n",
    "\n",
    "        for epoch in range(epoch_nums[iter]):\n",
    "            # 训练\n",
    "            total_loss = 0.0\n",
    "            num_batches = 0\n",
    "\n",
    "            for i, batch in enumerate(annotated_dataloader):\n",
    "                batch = {key: value.to(device) for key, value in batch.items()}  # 将批次数据移动到GPU上\n",
    "                loss = model.training_step(batch, i)\n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                model.optimizer_step()\n",
    "            avg_loss = total_loss / num_batches\n",
    "            if print_log: print(f\"Epoch [{epoch + 1}/{max_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "            # 每一个epoch训练完成后，在验证集上计算损失 计算token级别的precision,recall,F1\n",
    "            val_loss = 0.0\n",
    "            num_val_batches = 0\n",
    "\n",
    "            test_loss = 0.0\n",
    "            num_test_batches = 0\n",
    "\n",
    "            true_labels = []\n",
    "            pred_labels = []\n",
    "\n",
    "            test_true_labels = []\n",
    "            test_pred_labels = []\n",
    "            model.model.eval()\n",
    "            # for i, val_batch in enumerate(val_dataloader):\n",
    "            #     # loss\n",
    "            #     val_batch = {key: value.to(device) for key, value in val_batch.items()} \n",
    "            #     val_loss += model.validation_step(val_batch, i).item()\n",
    "            #     num_val_batches += 1\n",
    "            # for i, val_batch in enumerate(val_dataloader):\n",
    "            #     # precision,recall,F1\n",
    "            #     val_batch = {key: value.to(device) for key, value in val_batch.items()} \n",
    "            #     input_ids = val_batch['source_ids']\n",
    "            #     attention_mask = val_batch['source_mask']\n",
    "            #     val_outs = model.model.generate(input_ids=input_ids,\n",
    "            #                                 attention_mask=attention_mask)\n",
    "            #     val_dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "            #                             clean_up_tokenization_spaces=False).strip() for ids in val_outs]\n",
    "            #     val_target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "            #                 for ids in val_batch[\"target_ids\"]]\n",
    "            #     val_texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "            #                 for ids in val_batch[\"source_ids\"]]\n",
    "            #     true_label = [generate_label(val_texts[i].strip(), val_target[i].strip()) if val_target[i].strip() != 'none' else [\n",
    "            #         \"O\"]*len(val_texts[i].strip().split()) for i in range(len(val_texts))]\n",
    "            #     pred_label = [generate_label(val_texts[i].strip(), val_dec[i].strip()) if val_dec[i].strip() != 'none' else [\n",
    "            #         \"O\"]*len(val_texts[i].strip().split()) for i in range(len(val_texts))]\n",
    "            #     true_labels.extend(true_label)\n",
    "            #     pred_labels.extend(pred_label)\n",
    "\n",
    "            # for i, test_batch in enumerate(test_dataloader):\n",
    "            #     # loss\n",
    "            #     test_batch = {key: value.to(device) for key, value in test_batch.items()} \n",
    "            #     test_loss += model.validation_step(test_batch, i).item()\n",
    "            #     num_test_batches += 1\n",
    "            #     del test_batch\n",
    "\n",
    "            for i, test_batch in enumerate(test_dataloader):\n",
    "                # precision,recall,F1\n",
    "                test_batch = {key: value.to(device) for key, value in test_batch.items()} \n",
    "                input_ids = test_batch['source_ids']\n",
    "                attention_mask = test_batch['source_mask']\n",
    "                test_outs = model.model.generate(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            max_length=500,\n",
    "                                            num_beams=2)\n",
    "                test_dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=False).strip() for ids in test_outs]\n",
    "                test_target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                            for ids in test_batch[\"target_ids\"]]\n",
    "                test_texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                            for ids in test_batch[\"source_ids\"]]\n",
    "                true_label = [generate_label(test_texts[i].strip(), test_target[i].strip()) if test_target[i].strip() != 'none' else [\n",
    "                    \"O\"]*len(test_texts[i].strip().split()) for i in range(len(test_texts))]\n",
    "                pred_label = [generate_label(test_texts[i].strip(), test_dec[i].strip()) if test_dec[i].strip() != 'none' else [\n",
    "                    \"O\"]*len(test_texts[i].strip().split()) for i in range(len(test_texts))]\n",
    "                test_true_labels.extend(true_label)\n",
    "                test_pred_labels.extend(pred_label)\n",
    "                del test_batch\n",
    "            model.model.train()\n",
    "\n",
    "            metric = load_metric(\"seqeval\")\n",
    "            # metric_result = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "            test_metric_result = metric.compute(predictions=test_pred_labels, references=test_true_labels)\n",
    "            # {'AGENT': {'precision': 0.7375415282392026, 'recall': 0.7449664429530202, 'f1': 0.7412353923205344, 'number': 1192}, \\\n",
    "            # 'TARGET': {'precision': 0.48408710217755446, 'recall': 0.4605577689243028, 'f1': 0.472029399755002, 'number': 1255}, \\\n",
    "            # 'overall_precision': 0.6113427856547122, 'overall_recall': 0.5991009399264405, 'overall_f1': 0.6051599587203303, 'overall_accuracy': 0.8741352105838087}\n",
    "\n",
    "\n",
    "            # avg_val_loss = val_loss / num_val_batches\n",
    "            # avg_test_loss = test_loss / num_test_batches\n",
    "            if print_log: \n",
    "                print(\"-----------------------------------------------------------\")\n",
    "                print(\"strategy:\", current_strategy)\n",
    "                print(\"iter:\", iter)\n",
    "                print(\"epoch:\", epoch)\n",
    "                # print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "                # print(f\"F1_overall: {metric_result['overall_f1']:.4f}\")\n",
    "                # print(f\"F1_AGENT: {metric_result['AGENT']['f1']:.4f}\")\n",
    "                # print(f\"F1_TARGET: {metric_result['TARGET']['f1']:.4f}\")\n",
    "\n",
    "                # print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "                print(f\"F1_overall: {test_metric_result['overall_f1']:.4f}\")\n",
    "                print(f\"F1_AGENT: {test_metric_result['AGENT']['f1']:.4f}\")\n",
    "                print(f\"F1_TARGET: {test_metric_result['TARGET']['f1']:.4f}\")\n",
    "                print(\"-----------------------------------------------------------\")\n",
    "\n",
    "            if test_metric_result['overall_f1'] > max_overall_f1:\n",
    "                max_overall_f1 = test_metric_result['overall_f1']\n",
    "                print(\"best_overall_f1 update:\",max_overall_f1)\n",
    "                if iter >= 0:\n",
    "                    new_folder = f\"model_F1_{max_overall_f1:.5f}_iter_{iter}_epoch_{epoch}\"\n",
    "                    base_dir = f\"./saved_models/strategy_{current_strategy}\"\n",
    "                    full_dir = base_dir + \"/\" + new_folder\n",
    "                    model.model.save_pretrained(full_dir)\n",
    "                    print(f\"Model saved at {full_dir}\")\n",
    "                    files = os.listdir(base_dir)\n",
    "                    for file in files:\n",
    "                        if file == new_folder:\n",
    "                            continue\n",
    "                        old_model_path = os.path.join(base_dir, file)\n",
    "                        if os.path.isdir(old_model_path) and file.startswith(\"model\"):\n",
    "                            shutil.rmtree(old_model_path)\n",
    "                            print(f\"delete:{old_model_path}\")\n",
    "                    \n",
    "            # train_loss_record[strategy_id][iter][epoch] = avg_loss\n",
    "\n",
    "            # val_loss_record[strategy_id][iter][epoch] = avg_val_loss\n",
    "            # val_F1_overall_record[strategy_id][iter][epoch] = metric_result['overall_f1']\n",
    "            # val_F1_AGENT_record[strategy_id][iter][epoch] = metric_result['AGENT']['f1']\n",
    "            # val_F1_TARGET_record[strategy_id][iter][epoch] = metric_result['TARGET']['f1']\n",
    "            # save_array(current_strategy+\"_train_loss_record\", train_loss_record[strategy_id])\n",
    "            # save_array(current_strategy+\"_val_loss_record\", val_loss_record[strategy_id])\n",
    "            # save_array(current_strategy+\"_val_F1_overall_record\", val_F1_overall_record[strategy_id])\n",
    "            # save_array(current_strategy+\"_va1_F1_TARGET_record\", val_F1_TARGET_record[strategy_id])\n",
    "            # save_array(current_strategy+\"_val_F1_AGENT_record\", val_F1_AGENT_record[strategy_id])\n",
    "\n",
    "            # test_loss_record[strategy_id][iter][epoch] = avg_test_loss\n",
    "            test_F1_overall_record[strategy_id][iter][epoch] = test_metric_result['overall_f1']\n",
    "            test_F1_AGENT_record[strategy_id][iter][epoch] = test_metric_result['AGENT']['f1']\n",
    "            test_F1_TARGET_record[strategy_id][iter][epoch] = test_metric_result['TARGET']['f1']\n",
    "            # save_array(current_strategy+\"_test_loss_record\", test_loss_record[strategy_id])\n",
    "            save_array(current_strategy+\"_test_F1_overall_record\", test_F1_overall_record[strategy_id])\n",
    "            save_array(current_strategy+\"_test_F1_TARGET_record\", test_F1_TARGET_record[strategy_id])\n",
    "            save_array(current_strategy+\"_test_F1_AGENT_record\", test_F1_AGENT_record[strategy_id])\n",
    "\n",
    "        # 所有epoch结束后，对未标注数据采用选择策略，选出新的标注数据，更新datapool\n",
    "        model.update_datapool(strategy=current_strategy,add_percentage=0.05)\n",
    "        if print_log: \n",
    "            print(\"update datapool!\")\n",
    "            model.datapool.showDetail()\n",
    "    model.resetDatapool()\n",
    "if write_log:\n",
    "    sys.stdout.close()\n",
    "\n",
    "# trainWithAL()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create: {'aaa'}\n",
      "delete: {'model11'}\n"
     ]
    }
   ],
   "source": [
    "# 测试更新模型文件\n",
    "import os, shutil\n",
    "# 获取目录中的所有文件\n",
    "directory = \"./saved_models/deleteme\"\n",
    "new_folder = \"aaa\"\n",
    "os.makedirs(os.path.join(directory, new_folder))\n",
    "print(\"create:\",{new_folder})\n",
    "files = os.listdir(directory)\n",
    "for file in files:\n",
    "    if file == new_folder:\n",
    "        continue\n",
    "    full_path = os.path.join(directory, file)\n",
    "    if os.path.isdir(full_path) and file.startswith(\"model\"):\n",
    "        shutil.rmtree(full_path)\n",
    "        print(\"delete:\",{file})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./saved_models/model_epoch_3_trainloss_0.05953783357124902_valloss_0.23555645180333937'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch [1/10]---------------\n",
      "Average Loss: 0.3657\n",
      "learning_rate:0.0003000000\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_9307/2162164560.py:83: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "strategy: FULL\n",
      "epoch: 0\n",
      "Test Loss: 0.2504\n",
      "F1_overall: 0.6068\n",
      "F1_AGENT: 0.7180\n",
      "F1_TARGET: 0.5009\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch [2/10]---------------\n",
      "Average Loss: 0.1882\n",
      "learning_rate:0.0003000000\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "strategy: FULL\n",
      "epoch: 1\n",
      "Test Loss: 0.2154\n",
      "F1_overall: 0.6597\n",
      "F1_AGENT: 0.7637\n",
      "F1_TARGET: 0.5639\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch [3/10]---------------\n",
      "Average Loss: 0.1375\n",
      "learning_rate:0.0003000000\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "strategy: FULL\n",
      "epoch: 2\n",
      "Test Loss: 0.2470\n",
      "F1_overall: 0.6466\n",
      "F1_AGENT: 0.7537\n",
      "F1_TARGET: 0.5444\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch [4/10]---------------\n",
      "Average Loss: 0.1095\n",
      "learning_rate:0.0003000000\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "strategy: FULL\n",
      "epoch: 3\n",
      "Test Loss: 0.2677\n",
      "F1_overall: 0.6496\n",
      "F1_AGENT: 0.7402\n",
      "F1_TARGET: 0.5576\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Epoch [5/10]---------------\n",
      "Average Loss: 0.0826\n",
      "learning_rate:0.0003000000\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "strategy: FULL\n",
      "epoch: 4\n",
      "Test Loss: 0.2914\n",
      "F1_overall: 0.6545\n",
      "F1_AGENT: 0.7601\n",
      "F1_TARGET: 0.5503\n",
      "-----------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/ORL/test_AL_copy.ipynb Cell 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m batch \u001b[39m=\u001b[39m {key: value\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()}  \u001b[39m# 将批次数据移动到GPU上\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining_step(batch, i)\n\u001b[0;32m---> <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 不使用AL的训练流程\n",
    "model = T5FineTuner(args)\n",
    "max_epochs = 10\n",
    "current_strategy = \"FULL\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_dataloader = model.train_dataloader()\n",
    "test_dataloader = model.test_dataloader()\n",
    "print_log = True\n",
    "write_log = False\n",
    "\n",
    "model.model.to(device)\n",
    "if write_log:\n",
    "    sys.stdout = open('log/'+log_file_name, 'w+')\n",
    "\n",
    "full_train_loss_record = np.zeros(max_epochs + 1)\n",
    "full_test_loss_record = np.zeros(max_epochs + 1)\n",
    "full_test_F1_overall_record= np.zeros(max_epochs + 1)\n",
    "full_test_F1_AGENT_record = np.zeros(max_epochs + 1)\n",
    "full_test_F1_TARGET_record = np.zeros(max_epochs + 1)\n",
    "for epoch in range(max_epochs):\n",
    "    # 训练\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    model.model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        # print(f\"#######################################\")\n",
    "        # print(f\"learning_rate:{model.lr_scheduler.get_last_lr()[-1]:.10f}\")\n",
    "        # print(f\"learning_rate:{model.lr_scheduler.get_lr()}\")\n",
    "        # print(f\"#######################################\")\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}  # 将批次数据移动到GPU上\n",
    "        loss = model.training_step(batch, i)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        model.optimizer_step()\n",
    "    avg_loss = total_loss / num_batches\n",
    "    if print_log: \n",
    "        print(f\"--------------Epoch [{epoch + 1}/{max_epochs}]---------------\")\n",
    "        print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "        print(f\"learning_rate:{model.lr_scheduler.get_last_lr()[-1]:.10f}\")\n",
    "        print(\"-------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # 每一个epoch训练完成后，在验证集上计算损失 计算token级别的precision,recall,F1\n",
    "    test_loss = 0.0\n",
    "    num_test_batches = 0\n",
    "\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    test_true_labels = []\n",
    "    test_pred_labels = []\n",
    "    model.model.eval()\n",
    "    for i, test_batch in enumerate(test_dataloader):\n",
    "        test_batch = {key: value.to(device) for key, value in test_batch.items()} \n",
    "        test_loss += model.validation_step(test_batch, i).item()\n",
    "        num_test_batches += 1\n",
    "        # del test_batch\n",
    "    for i, test_batch in enumerate(test_dataloader):\n",
    "        # precision,recall,F1\n",
    "        test_batch = {key: value.to(device) for key, value in test_batch.items()} \n",
    "        input_ids = test_batch['source_ids']\n",
    "        attention_mask = test_batch['source_mask']\n",
    "        test_outs = model.model.generate(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    max_length=500,\n",
    "                                    num_beams=2)\n",
    "        test_dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                                clean_up_tokenization_spaces=False).strip() for ids in test_outs]\n",
    "        test_target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                    for ids in test_batch[\"target_ids\"]]\n",
    "        test_texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                    for ids in test_batch[\"source_ids\"]]\n",
    "        true_label = [generate_label(test_texts[i].strip(), test_target[i].strip()) if test_target[i].strip() != 'none' else [\n",
    "            \"O\"]*len(test_texts[i].strip().split()) for i in range(len(test_texts))]\n",
    "        pred_label = [generate_label(test_texts[i].strip(), test_dec[i].strip()) if test_dec[i].strip() != 'none' else [\n",
    "            \"O\"]*len(test_texts[i].strip().split()) for i in range(len(test_texts))]\n",
    "        test_true_labels.extend(true_label)\n",
    "        test_pred_labels.extend(pred_label)\n",
    "        # del test_batch\n",
    "    model.model.train()\n",
    "\n",
    "    metric = load_metric(\"seqeval\")\n",
    "    # metric_result = metric.compute(predictions=pred_labels, references=true_labels)\n",
    "    test_metric_result = metric.compute(predictions=test_pred_labels, references=test_true_labels)\n",
    "    # {'AGENT': {'precision': 0.7375415282392026, 'recall': 0.7449664429530202, 'f1': 0.7412353923205344, 'number': 1192}, \\\n",
    "    # 'TARGET': {'precision': 0.48408710217755446, 'recall': 0.4605577689243028, 'f1': 0.472029399755002, 'number': 1255}, \\\n",
    "    # 'overall_precision': 0.6113427856547122, 'overall_recall': 0.5991009399264405, 'overall_f1': 0.6051599587203303, 'overall_accuracy': 0.8741352105838087}\n",
    "\n",
    "\n",
    "    # avg_val_loss = val_loss / num_val_batches\n",
    "    avg_test_loss = test_loss / num_test_batches\n",
    "    if print_log: \n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        print(\"strategy:\", current_strategy)\n",
    "        print(\"epoch:\", epoch)\n",
    "        # print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        # print(f\"F1_overall: {metric_result['overall_f1']:.4f}\")\n",
    "        # print(f\"F1_AGENT: {metric_result['AGENT']['f1']:.4f}\")\n",
    "        # print(f\"F1_TARGET: {metric_result['TARGET']['f1']:.4f}\")\n",
    "\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "        print(f\"F1_overall: {test_metric_result['overall_f1']:.4f}\")\n",
    "        print(f\"F1_AGENT: {test_metric_result['AGENT']['f1']:.4f}\")\n",
    "        print(f\"F1_TARGET: {test_metric_result['TARGET']['f1']:.4f}\")\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "\n",
    "    full_train_loss_record[epoch] = avg_loss\n",
    "\n",
    "    full_test_loss_record[epoch] = avg_test_loss\n",
    "    full_test_F1_overall_record[epoch] = test_metric_result['overall_f1']\n",
    "    full_test_F1_AGENT_record[epoch] = test_metric_result['AGENT']['f1']\n",
    "    full_test_F1_TARGET_record[epoch] = test_metric_result['TARGET']['f1']\n",
    "    save_array(current_strategy+\"_test_loss_record\", full_test_loss_record)\n",
    "    save_array(current_strategy+\"_test_F1_overall_record\", full_test_F1_overall_record)\n",
    "    save_array(current_strategy+\"_test_F1_TARGET_record\", full_test_F1_TARGET_record)\n",
    "    save_array(current_strategy+\"_test_F1_AGENT_record\", full_test_F1_AGENT_record)\n",
    "if write_log:\n",
    "    sys.stdout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不使用AL的训练流程\n",
    "model = T5FineTuner(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataloader = model.train_dataloader()\n",
    "test_dataloader = model.test_dataloader()\n",
    "for epoch in range(max_epochs):\n",
    "    # 在验证集上计算损失\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        val_batch = {key: value.to(device) for key, value in val_batch.items()}  # 将批次数据移动到GPU上\n",
    "        val_loss += model.validation_step(val_batch, i).item()\n",
    "        num_val_batches += 1\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}  # 将批次数据移动到GPU上\n",
    "        loss = model.training_step(batch, i)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # 打印每个批次的损失值\n",
    "        print(f\"Epoch [{epoch + 1}/{max_epochs}], Batch [{i + 1}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # 执行优化步骤\n",
    "        model.optimizer_step()\n",
    "    # 计算并打印平均损失值\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{max_epochs}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 在验证集上计算损失\n",
    "    model.model.eval()\n",
    "    val_loss = 0.0\n",
    "    num_val_batches = 0\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        val_batch = {key: value.to(device) for key, value in val_batch.items()}  # 将批次数据移动到GPU上\n",
    "        val_loss += model.validation_step(val_batch, i).item()\n",
    "        num_val_batches += 1\n",
    "    avg_val_loss = val_loss / num_val_batches\n",
    "    print(\"---------------------------------------\")\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(\"---------------------------------------\")\n",
    "    model.model.train()\n",
    "\n",
    "\n",
    "    save_dir = f\"./saved_models/model_epoch_{epoch + 1}_trainloss_{avg_loss}_valloss_{avg_val_loss}\"\n",
    "    model.model.save_pretrained(save_dir)\n",
    "    print(f\"Model saved at {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.keys():  odict_keys(['sequences', 'scores', 'past_key_values'])\n",
      "len(outputs.scores) 35\n",
      "outputs.scores[0].shape: torch.Size([8, 32128])\n",
      "len(outputs.scores): 35\n",
      "outputs.scores[0].shape: torch.Size([8, 32128])\n",
      "ddddddddddddddddddddddddddddddddddddd tensor(-26.1285, device='cuda:0')\n",
      "transition_scores.shape: torch.Size([8, 35])\n",
      "len(transition_scores[0]):  35\n",
      "outputs.sequences:  tensor([[    0,  2387,    10,  1045,  2371,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,     3,     7,   232,    45,  8390,   844,     3,\n",
      "             6,   902,     8,  4723,  1911,  7579,    23,     9, 21286,  1719,\n",
      "             3,     6,    11, 12848,  5784,    45,     8,   690,     3,    31,\n",
      "             7,     3, 24530,  1449,  1471,     1],\n",
      "        [    0,  3102,    10,     3, 26165,   117,  2387,    10,  3409,    13,\n",
      "             3,     2,  1038,  7778,     3,    31,    31,    12, 16363,  9830,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8,     3, 26165,    29,  2959,  8409,   117,\n",
      "          2387,    10,   838,   139,   905,     3, 26165,     3,    31,     7,\n",
      "           806,  3984,     3,    31,    31,    16,   464,    91,  1038,  2691,\n",
      "          6238,     8,  9464,    13,  9830,     1],\n",
      "        [    0,  3102,    10,     3,     9,  2959,  8409,  2493,   117,  2387,\n",
      "            10,   838,   139,   905,     3, 26165,     3,    31,     7,  3984,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3, 26165,   117,  2387,    10,  1577,  1038,\n",
      "          7778,    21,     3, 27321,  9830,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    66,  1320,  1016,   725,   117,  2387,    10,\n",
      "             8,  3298,     3,  9475,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8,   178,   117,  2387,    10, 14510,    45,\n",
      "             3, 13505,   538,  3629,    12, 16363,     8, 24578,    13, 17572,\n",
      "         24436,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]], device='cuda:0')\n",
      "len(generated_tokens[0]): 35\n",
      "generated_tokens:  tensor([[ 2387,    10,  1045,  2371,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 2387,    10,     3,     7,   232,    45,  8390,   844,     3,     6,\n",
      "           902,     8,  4723,  1911,  7579,    23,     9, 21286,  1719,     3,\n",
      "             6,    11, 12848,  5784,    45,     8,   690,     3,    31,     7,\n",
      "             3, 24530,  1449,  1471,     1],\n",
      "        [ 3102,    10,     3, 26165,   117,  2387,    10,  3409,    13,     3,\n",
      "             2,  1038,  7778,     3,    31,    31,    12, 16363,  9830,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 3102,    10,     8,     3, 26165,    29,  2959,  8409,   117,  2387,\n",
      "            10,   838,   139,   905,     3, 26165,     3,    31,     7,   806,\n",
      "          3984,     3,    31,    31,    16,   464,    91,  1038,  2691,  6238,\n",
      "             8,  9464,    13,  9830,     1],\n",
      "        [ 3102,    10,     3,     9,  2959,  8409,  2493,   117,  2387,    10,\n",
      "           838,   139,   905,     3, 26165,     3,    31,     7,  3984,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 3102,    10,     3, 26165,   117,  2387,    10,  1577,  1038,  7778,\n",
      "            21,     3, 27321,  9830,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 3102,    10,    66,  1320,  1016,   725,   117,  2387,    10,     8,\n",
      "          3298,     3,  9475,     1,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [ 3102,    10,     8,   178,   117,  2387,    10, 14510,    45,     3,\n",
      "         13505,   538,  3629,    12, 16363,     8, 24578,    13, 17572, 24436,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]], device='cuda:0')\n",
      "|  2387 | target   | -0.1016 |90.34%\n",
      "|    10 | :        | -0.0002 |99.98%\n",
      "|  1045 | private  | -0.2222 |80.08%\n",
      "|  2371 | organizations | -0.0161 |98.41%\n",
      "|     1 | </s>     | -0.4718 |62.39%\n",
      "average prob: 0.8623833537101746\n",
      "min prob: 0.6239086\n",
      "target: private organizations\n",
      "----------------------------------------------\n",
      "|  2387 | target   | -0.2311 |79.37%\n",
      "|    10 | :        | -0.0004 |99.96%\n",
      "|     3 |          | -0.0236 |97.67%\n",
      "|     7 | s        | -0.0015 |99.85%\n",
      "|   232 | and      | -0.0001 |99.99%\n",
      "|    45 | from     | -0.0108 |98.93%\n",
      "|  8390 | northern | -0.0046 |99.54%\n",
      "|   844 | areas    | -0.0010 |99.90%\n",
      "|     3 |          | -0.0706 |93.19%\n",
      "|     6 | ,        | -0.0003 |99.97%\n",
      "|   902 | especially | -0.0735 |92.91%\n",
      "|     8 | the      | -0.0057 |99.43%\n",
      "|  4723 | inner    | -0.0015 |99.85%\n",
      "|  1911 | mon      | -0.0002 |99.98%\n",
      "|  7579 | gol      | -0.0003 |99.97%\n",
      "|    23 | i        | -0.0002 |99.98%\n",
      "|     9 | a        | -0.0001 |99.99%\n",
      "| 21286 | autonomous | -0.0083 |99.17%\n",
      "|  1719 | region   | -0.0017 |99.83%\n",
      "|     3 |          | -0.1291 |87.89%\n",
      "|     6 | ,        | -0.0002 |99.98%\n",
      "|    11 | and      | -0.0012 |99.88%\n",
      "| 12848 | floating | -0.0016 |99.84%\n",
      "|  5784 | dust     | -0.0001 |99.99%\n",
      "|    45 | from     | -0.0004 |99.96%\n",
      "|     8 | the      | -0.0001 |99.99%\n",
      "|   690 | city     | -0.0001 |99.99%\n",
      "|     3 |          | -0.0028 |99.72%\n",
      "|    31 | '        | -0.0002 |99.98%\n",
      "|     7 | s        | -0.0003 |99.97%\n",
      "|     3 |          | -0.0007 |99.93%\n",
      "| 24530 | uncovered | -0.0000 |100.00%\n",
      "|  1449 | construction | -0.0000 |100.00%\n",
      "|  1471 | sites    | -0.0001 |99.99%\n",
      "|     1 | </s>     | -0.0086 |99.15%\n",
      "average prob: 0.9844972746712821\n",
      "min prob: 0.79369926\n",
      "target: sand from northern areas, especially the inner mongolia autonomous region, and floating dust from the city's uncovered construction sites\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.0114 |98.87%\n",
      "|    10 | :        | -0.0000 |100.00%\n",
      "|     3 |          | -0.0009 |99.91%\n",
      "| 26165 | russia   | -0.0001 |99.99%\n",
      "|   117 | ;        | -0.0076 |99.24%\n",
      "|  2387 | target   | -0.0006 |99.94%\n",
      "|    10 | :        | -0.0000 |100.00%\n",
      "|  3409 | creation | -0.0262 |97.41%\n",
      "|    13 | of       | -0.0007 |99.93%\n",
      "|     3 |          | -0.0060 |99.41%\n",
      "|     2 | <unk>    | -0.0001 |99.99%\n",
      "|  1038 | international | -0.0057 |99.43%\n",
      "|  7778 | instruments | -0.0001 |99.99%\n",
      "|     3 |          | -0.0578 |94.38%\n",
      "|    31 | '        | -0.0016 |99.84%\n",
      "|    31 | '        | -0.0006 |99.94%\n",
      "|    12 | to       | -0.0246 |97.57%\n",
      "| 16363 | regulate | -0.0001 |99.99%\n",
      "|  9830 | emissions | -0.0010 |99.90%\n",
      "|     1 | </s>     | -0.0009 |99.91%\n",
      "average prob: 0.9928144454956055\n",
      "min prob: 0.9438475\n",
      "agent: russia; target: creation of  international instruments '' to regulate emissions\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.0491 |95.21%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|     8 | the      | -0.0088 |99.13%\n",
      "|     3 |          | -0.0014 |99.86%\n",
      "| 26165 | russia   | -0.0000 |100.00%\n",
      "|    29 | n        | -0.0000 |100.00%\n",
      "|  2959 | foreign  | -0.0002 |99.98%\n",
      "|  8409 | ministry | -0.0001 |99.99%\n",
      "|   117 | ;        | -0.0296 |97.08%\n",
      "|  2387 | target   | -0.0003 |99.97%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|   838 | taking   | -0.7194 |48.71%\n",
      "|   139 | into     | -0.0001 |99.99%\n",
      "|   905 | account  | -0.0238 |97.64%\n",
      "|     3 |          | -0.0002 |99.98%\n",
      "| 26165 | russia   | -0.0000 |100.00%\n",
      "|     3 |          | -0.0013 |99.87%\n",
      "|    31 | '        | -0.0000 |100.00%\n",
      "|     7 | s        | -0.0004 |99.96%\n",
      "|   806 | specific | -0.0095 |99.05%\n",
      "|  3984 | interests | -0.0010 |99.90%\n",
      "|     3 |          | -0.0132 |98.69%\n",
      "|    31 | '        | -0.0001 |99.99%\n",
      "|    31 | '        | -0.0011 |99.89%\n",
      "|    16 | in       | -0.0219 |97.83%\n",
      "|   464 | working  | -0.0066 |99.34%\n",
      "|    91 | out      | -0.0000 |100.00%\n",
      "|  1038 | international | -0.0010 |99.90%\n",
      "|  2691 | documents | -0.0004 |99.96%\n",
      "|  6238 | concerning | -0.0126 |98.75%\n",
      "|     8 | the      | -0.0001 |99.99%\n",
      "|  9464 | regulation | -0.0001 |99.99%\n",
      "|    13 | of       | -0.0006 |99.94%\n",
      "|  9830 | emissions | -0.0016 |99.84%\n",
      "|     1 | </s>     | -0.0010 |99.90%\n",
      "average prob: 0.9800868008817946\n",
      "min prob: 0.4870669\n",
      "agent: the russian foreign ministry; target: taking into account russia's specific interests '' in working out international documents concerning the regulation of emissions\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.0582 |94.34%\n",
      "|    10 | :        | -0.0000 |100.00%\n",
      "|     3 |          | -0.0103 |98.97%\n",
      "|     9 | a        | -0.0003 |99.97%\n",
      "|  2959 | foreign  | -0.0022 |99.78%\n",
      "|  8409 | ministry | -0.0004 |99.96%\n",
      "|  2493 | statement | -0.0018 |99.82%\n",
      "|   117 | ;        | -0.1001 |90.47%\n",
      "|  2387 | target   | -0.0007 |99.93%\n",
      "|    10 | :        | -0.0000 |100.00%\n",
      "|   838 | taking   | -0.6259 |53.48%\n",
      "|   139 | into     | -0.0011 |99.89%\n",
      "|   905 | account  | -0.0078 |99.22%\n",
      "|     3 |          | -0.0004 |99.96%\n",
      "| 26165 | russia   | -0.0000 |100.00%\n",
      "|     3 |          | -0.0014 |99.86%\n",
      "|    31 | '        | -0.0000 |100.00%\n",
      "|     7 | s        | -0.0003 |99.97%\n",
      "|  3984 | interests | -0.0019 |99.81%\n",
      "|     1 | </s>     | -0.0504 |95.08%\n",
      "average prob: 0.9652594476938248\n",
      "min prob: 0.53475946\n",
      "agent: a foreign ministry statement; target: taking into account russia's interests\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.2543 |77.55%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|     3 |          | -0.0012 |99.88%\n",
      "| 26165 | russia   | -0.0002 |99.98%\n",
      "|   117 | ;        | -0.0725 |93.00%\n",
      "|  2387 | target   | -0.0002 |99.98%\n",
      "|    10 | :        | -0.0000 |100.00%\n",
      "|  1577 | creating | -0.0987 |90.61%\n",
      "|  1038 | international | -0.0024 |99.76%\n",
      "|  7778 | instruments | -0.0003 |99.97%\n",
      "|    21 | for      | -0.0018 |99.82%\n",
      "|     3 |          | -0.0002 |99.98%\n",
      "| 27321 | regulating | -0.0002 |99.98%\n",
      "|  9830 | emissions | -0.0018 |99.82%\n",
      "|     1 | </s>     | -0.0107 |98.94%\n",
      "average prob: 0.9728418946266174\n",
      "min prob: 0.77549034\n",
      "agent: russia; target: creating international instruments for regulating emissions\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.9577 |38.38%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|    66 | all      | -0.4111 |66.29%\n",
      "|  1320 | sign     | -0.0012 |99.88%\n",
      "|  1016 | ator     | -0.0061 |99.39%\n",
      "|   725 | ies      | -0.0000 |100.00%\n",
      "|   117 | ;        | -0.4395 |64.44%\n",
      "|  2387 | target   | -0.0009 |99.91%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|     8 | the      | -0.0251 |97.52%\n",
      "|  3298 | climate  | -0.0012 |99.88%\n",
      "|     3 |          | -0.0433 |95.77%\n",
      "|  9475 | sphere   | -0.0000 |100.00%\n",
      "|     1 | </s>     | -0.0618 |94.01%\n",
      "average prob: 0.8967667945793697\n",
      "min prob: 0.38379097\n",
      "agent: all signatories; target: the climate sphere\n",
      "----------------------------------------------\n",
      "|  3102 | agent    | -0.2109 |80.98%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "|     8 | the      | -0.0546 |94.69%\n",
      "|   178 | us       | -0.0046 |99.54%\n",
      "|   117 | ;        | -0.2044 |81.52%\n",
      "|  2387 | target   | -0.0003 |99.97%\n",
      "|    10 | :        | -0.0001 |99.99%\n",
      "| 14510 | withdraw | -0.5577 |57.25%\n",
      "|    45 | from     | -0.0022 |99.78%\n",
      "|     3 |          | -0.0009 |99.91%\n",
      "| 13505 | introducing | -0.0017 |99.84%\n",
      "|   538 | state    | -0.0011 |99.89%\n",
      "|  3629 | measures | -0.0002 |99.98%\n",
      "|    12 | to       | -0.0061 |99.39%\n",
      "| 16363 | regulate | -0.0001 |99.99%\n",
      "|     8 | the      | -0.0003 |99.97%\n",
      "| 24578 | emission | -0.0030 |99.70%\n",
      "|    13 | of       | -0.0000 |100.00%\n",
      "| 17572 | greenhouse | -0.0000 |100.00%\n",
      "| 24436 | gases    | -0.0009 |99.91%\n",
      "|     1 | </s>     | -0.0071 |99.29%\n",
      "average prob: 0.9578959090369088\n",
      "min prob: 0.5725292\n",
      "agent: the us; target: withdraw from introducing state measures to regulate the emission of greenhouse gases\n",
      "----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "val_dataloader = model.val_dataloader()\n",
    "model.model.to(device)\n",
    "cnt = 0\n",
    "for i, batch in enumerate(val_dataloader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()} \n",
    "        outputs = model.model.generate(input_ids=batch[\"source_ids\"],\\\n",
    "                attention_mask=batch[\"source_mask\"],return_dict_in_generate=True,output_scores=True,max_length=500)\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt > 1:\n",
    "            break\n",
    "        # if len(outputs.scores) == 19 :\n",
    "        #     continue\n",
    "        \n",
    "\n",
    "        print(\"outputs.keys(): \",outputs.keys())\n",
    "        print(\"len(outputs.scores)\",len(outputs.scores))\n",
    "        print(\"outputs.scores[0].shape:\",outputs.scores[0].shape)\n",
    "        print(\"len(outputs.scores):\",len(outputs.scores))\n",
    "        #output.scores是一个长度为输出的token数量的元组，元组中的每个元素为[batchsize,词表大小]的tensor\n",
    "        print(\"outputs.scores[0].shape:\",outputs.scores[0].shape) \n",
    "        print(\"ddddddddddddddddddddddddddddddddddddd\",outputs.scores[0][0][0])\n",
    "\n",
    "        transition_scores = model.model.compute_transition_scores(\n",
    "            outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        )\n",
    "        print(\"transition_scores.shape:\", transition_scores.shape)\n",
    "        # compute_transition_scores计算出来的transition_scores是一个[batchsize,输出的token数量]的tensor，相当于对于每个输出的token一个评分\n",
    "        print(\"len(transition_scores[0]): \", len(transition_scores[0]))\n",
    "        # print(\"transition_scores: \", transition_scores)\n",
    "\n",
    "        # input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "        # 由于t5是encoder-decoder架构，所以这里input_length取1\n",
    "        generated_tokens = outputs.sequences[:, 1:] \n",
    "        # generated_tokens是一个[batchsize,输出的token数量]的tensor，相当于每一个输出的token对应的id\n",
    "        print(\"outputs.sequences: \", outputs.sequences)\n",
    "        print(\"len(generated_tokens[0]):\",len(generated_tokens[0]))\n",
    "        print(\"generated_tokens: \",generated_tokens)\n",
    "\n",
    "        # 遍历batch中的每一条样本，计算对每一条样本的不确定程度\n",
    "        for i in range(generated_tokens.shape[0]):\n",
    "            total_probs = 0.0 # 可能性的总和\n",
    "            valid_num = 0 # 实际的token的数量\n",
    "            min_probs = 1.0\n",
    "            for tok, score in zip(generated_tokens[i], transition_scores[i]):\n",
    "                if tok == 0:\n",
    "                    break\n",
    "                # | token | token string | logits | probability\n",
    "                print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.cpu().numpy():.4f} |{np.exp(score.cpu().numpy()):.2%}\")\n",
    "                total_probs += np.exp(score.cpu().numpy())\n",
    "                min_probs = min(min_probs, np.exp(score.cpu().numpy()))\n",
    "                valid_num += 1\n",
    "            if valid_num > 0:\n",
    "                print(\"average prob:\", total_probs/valid_num)\n",
    "                print(\"min prob:\", min_probs)\n",
    "            \n",
    "            print(tokenizer.decode(outputs.sequences[i], skip_special_tokens=True))\n",
    "            print(\"----------------------------------------------\")\n",
    "\n",
    "        \n",
    "        # print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(outputs.scores) 19\n",
      "dict_keys(['source_ids', 'source_mask', 'target_ids', 'target_mask'])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"len(outputs.scores)\",len(outputs.scores))\n",
    "print(batch.keys())\n",
    "print(batch[\"source_ids\"].shape)\n",
    "print(batch[\"source_mask\"].shape)\n",
    "print(batch[\"target_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:49.405585Z",
     "iopub.status.busy": "2024-02-28T14:16:49.405011Z",
     "iopub.status.idle": "2024-02-28T14:16:49.410174Z",
     "shell.execute_reply": "2024-02-28T14:16:49.409675Z",
     "shell.execute_reply.started": "2024-02-28T14:16:49.405565Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback():\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:53.624799Z",
     "iopub.status.busy": "2024-02-28T14:16:53.624457Z",
     "iopub.status.idle": "2024-02-28T14:16:53.629524Z",
     "shell.execute_reply": "2024-02-28T14:16:53.629044Z",
     "shell.execute_reply.started": "2024-02-28T14:16:53.624778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=args.output_dir+\"/checkpoint.pth\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    #early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    #amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "# train_params = dict(\n",
    "#     accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "#     ## gpus=args.n_gpu,\n",
    "#     max_epochs=args.num_train_epochs,\n",
    "#     #early_stop_callback=False,\n",
    "#     precision= 16 if args.fp_16 else 32,\n",
    "#     #amp_level=args.opt_level,\n",
    "#     gradient_clip_val=args.max_grad_norm,\n",
    "#     # callbacks=[LoggingCallback()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T14:16:59.154405Z",
     "iopub.status.busy": "2024-02-28T14:16:59.153852Z",
     "iopub.status.idle": "2024-02-28T14:16:59.157263Z",
     "shell.execute_reply": "2024-02-28T14:16:59.156780Z",
     "shell.execute_reply.started": "2024-02-28T14:16:59.154380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:17:03.095693Z",
     "iopub.status.busy": "2024-02-28T14:17:03.095368Z",
     "iopub.status.idle": "2024-02-28T14:17:03.102459Z",
     "shell.execute_reply": "2024-02-28T14:17:03.102015Z",
     "shell.execute_reply.started": "2024-02-28T14:17:03.095673Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fb6a2a8c9d0>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7fb6a2a8c9d0>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T14:17:04.889255Z",
     "iopub.status.busy": "2024-02-28T14:17:04.888686Z",
     "iopub.status.idle": "2024-02-28T14:54:53.783173Z",
     "shell.execute_reply": "2024-02-28T14:54:53.782617Z",
     "shell.execute_reply.started": "2024-02-28T14:17:04.889232Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-28T15:15:30.833864Z",
     "iopub.status.busy": "2024-02-28T15:15:30.833522Z",
     "iopub.status.idle": "2024-02-28T15:15:37.034017Z",
     "shell.execute_reply": "2024-02-28T15:15:37.033415Z",
     "shell.execute_reply.started": "2024-02-28T15:15:30.833841Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: '/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    399\u001b[0m         path_or_repo_id,\n\u001b[1;32m    400\u001b[0m         filename,\n\u001b[1;32m    401\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    402\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    403\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    404\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    405\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    406\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    407\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    408\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    409\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m    410\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    112\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt'. Use `repo_type` argument if needed.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/mnt/workspace/ORL/test_AL_copy.ipynb Cell 41\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# model = model.load_from_checkpoint(\"/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt\")\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://dsw-gateway-cn-beijing.data.aliyun.com/mnt/workspace/ORL/test_AL_copy.ipynb#Y232sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39m/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/transformers/modeling_utils.py:2874\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[39mif\u001b[39;00m commit_hash \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2872\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2873\u001b[0m         \u001b[39m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 2874\u001b[0m         resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m   2875\u001b[0m             pretrained_model_name_or_path,\n\u001b[1;32m   2876\u001b[0m             CONFIG_NAME,\n\u001b[1;32m   2877\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   2878\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   2879\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   2880\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   2881\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   2882\u001b[0m             token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   2883\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   2884\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   2885\u001b[0m             _raise_exceptions_for_gated_repo\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2886\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2887\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   2888\u001b[0m         )\n\u001b[1;32m   2889\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2890\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/pai/envs/T5/lib/python3.9/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThere was a specific connection error when trying to load \u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00merr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[39mexcept\u001b[39;00m HFValidationError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIncorrect path_or_model_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "# model = model.load_from_checkpoint(\"/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt\")\n",
    "model = model.model.from_pretrained(\"/mnt/workspace/ORL/lightning_logs/version_4/checkpoints/epoch=9-step=279.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "dataloader = DataLoader(input_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
    "model.eval()\n",
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:15:39.989894Z",
     "iopub.status.busy": "2024-02-28T15:15:39.989324Z",
     "iopub.status.idle": "2024-02-28T15:15:53.553095Z",
     "shell.execute_reply": "2024-02-28T15:15:53.552563Z",
     "shell.execute_reply.started": "2024-02-28T15:15:39.989874Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: i do n't , just so i can work in the school i 'll be happy . dse:happy\n",
      "\n",
      "Actual Entities: target: work in the school; agent: i\n",
      "Predicted Entities: agent: i\n",
      "=====================================================================\n",
      "\n",
      "text: the uk foreign secretary , jack straw , said :  for months the government of zimbabwe has\n",
      "conducted a systematic campaign of violence and intimidation , designed to achieve an outcome -\n",
      "power at all costs . '' dse:said\n",
      "\n",
      "Actual Entities: agent: the uk foreign secretary , jack straw; target: the government of zimbabwe\n",
      "Predicted Entities: agent: the uk foreign secretary , jack straw; target: the government of zi\n",
      "=====================================================================\n",
      "\n",
      "text: in the citizen , columnist rosa harris-adler wrote a devastatingly sarcastic piece ,\n",
      "ridiculing mr. tobin 's assertion that he was leaving for family reasons . dse:ridiculing\n",
      "\n",
      "Actual Entities: agent: rosa harris-adler\n",
      "Predicted Entities: agent: rosa harris-adler; target: mr.\n",
      "=====================================================================\n",
      "\n",
      "text: they 're actually going to paint arrows on the floors of the cells so they 'll know to face\n",
      "north , '' he says . dse:says\n",
      "\n",
      "Actual Entities: agent: he\n",
      "Predicted Entities: agent: he\n",
      "=====================================================================\n",
      "\n",
      "text: we hope that the united states will come to realize that an active climate protection policy\n",
      "is not only necessary for environmental policy but is also an excellent opportunity to modernize\n",
      "industrial society . dse:hope\n",
      "\n",
      "Actual Entities: agent: we; target: an active climate protection policy is not only necessary for environmental policy but is also an excellent opportunity to modernize industrial society\n",
      "Predicted Entities: agent: we; target: the united states\n",
      "=====================================================================\n",
      "\n",
      "text: uh , while i was in high school a teacher of mine noticed that i had an extra bone in my foot\n",
      "and she said ,  well now sandra we can use this as get toward , on your grant to go to college .\n",
      "dse:noticed\n",
      "\n",
      "Actual Entities: agent: teacher of mine; target: i had an extra bone in my foot\n",
      "Predicted Entities: agent: teacher of mine; target: i had an extra bone in my foot\n",
      "=====================================================================\n",
      "\n",
      "text: at the time when the abortive coup had just taken place , the united states  did not show the\n",
      "slightest sympathy , '' still less  the least regret , '' for the breaking-off of venezuela 's\n",
      "democratic system . dse:sympathy\n",
      "\n",
      "Actual Entities: agent: the united states\n",
      "Predicted Entities: agent: the united states\n",
      "=====================================================================\n",
      "\n",
      "text: referring to president bush 's climate change plan for the us that he announced on 14 february\n",
      "the commissioner noted :  these proposals will not lead to a reduction of greenhouse gas emissions\n",
      "in the united states but allow a significant increase . dse:noted\n",
      "\n",
      "Actual Entities: agent: the commissioner; target: these proposals\n",
      "Predicted Entities: agent: the commissioner; target: these proposals\n",
      "=====================================================================\n",
      "\n",
      "text: we have to enforce the law . '' dse:we\n",
      "\n",
      "Actual Entities: target: the law\n",
      "Predicted Entities: target: enforce the law\n",
      "=====================================================================\n",
      "\n",
      "text: a final communique issued on saturday evening after the barcelona eu summit said ,  the\n",
      "european union will maintain its humanitarian assistance to the people of zimbabwe and will consider\n",
      "possible additional targeted measures against its government . '' dse:will consider\n",
      "\n",
      "Actual Entities: agent: the european union; target: possible additional targeted measures against its government\n",
      "Predicted Entities: agent: the european union; target: possible additional targeted measures against its government\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "targets = []\n",
    "texts = []\n",
    "cnt = 0\n",
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    texts.extend(text)\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    break\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    c = texts[i]\n",
    "    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual Entities: %s\" % target[i])\n",
    "    print(\"Predicted Entities: %s\" % outputs[i])\n",
    "    print(\"=====================================================================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(model.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs.keys():  odict_keys(['sequences', 'sequences_scores', 'scores', 'beam_indices', 'past_key_values'])\n",
      "outputs.scores[0].shape: torch.Size([5, 32128])\n",
      "len(outputs.scores): 11\n",
      "outputs.scores[0].shape: torch.Size([5, 32128])\n",
      "transition_scores.shape: torch.Size([5, 11])\n",
      "len(transition_scores[0]):  11\n",
      "outputs.sequences_scores: -1.9136189222335815\n",
      "tensor(-558.4424)\n",
      "outputs.sequences:  tensor([[    0, 32099,   100,    19,     3,     9,  1391,  7142,     5,     1]])\n",
      "len(generated_tokens[0]): 9\n",
      "generated_tokens:  tensor([[32099,   100,    19,     3,     9,  1391,  7142,     5,     1]])\n",
      "| 32099 | <extra_id_0> | -24.7070 |0.00%\n",
      "|   100 | This     | -6.1793 |0.21%\n",
      "|    19 | is       | -11.8298 |0.00%\n",
      "|     3 |          | -9.4976 |0.01%\n",
      "|     9 | a        | -15.5207 |0.00%\n",
      "|  1391 | source   | -12.2161 |0.00%\n",
      "|  7142 | sentence | -13.4654 |0.00%\n",
      "|     5 | .        | -9.9812 |0.00%\n",
      "|     1 | </s>     | -0.0221 |97.81%\n",
      "----------------------------------------------\n",
      "This is a source sentence.\n"
     ]
    }
   ],
   "source": [
    "# search:how to get the probability huggingface\n",
    "# https://discuss.huggingface.co/t/announcement-generation-get-probabilities-for-generated-output/30075\n",
    "\n",
    "# 测试model.generate()方法用\n",
    "model = T5FineTuner(args)\n",
    "input_text = \"I love the person who likes me. dse:love\"\n",
    "# input_texts = [\"I love the person who likes me. dse:love\",\"He loves his wife. dse:loves\"]\n",
    "max_length = 32\n",
    "temperature = 1.0\n",
    "num_samples = 1\n",
    "\n",
    "# 推理\n",
    "# inputs = tokenizer([input_text], return_tensors=\"pt\")\n",
    "\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    input_texts, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "source_text = \"This is a source sentence.\"\n",
    "source_ids = tokenizer(source_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     **inputs,\n",
    "#     max_new_tokens=5,\n",
    "#     num_beams=4,\n",
    "#     num_return_sequences=4,\n",
    "#     return_dict_in_generate=True,\n",
    "#     output_scores=True,\n",
    "# )\n",
    "\n",
    "# gen_outputs = model.model.generate(\n",
    "#     inputs=source_ids,\n",
    "#     num_beams=5,\n",
    "#     min_length=0,\n",
    "#     max_length=512,\n",
    "#     length_penalty=0,\n",
    "#     output_scores=True,\n",
    "#     return_dict_in_generate=True,\n",
    "# )\n",
    "\n",
    "outputs = model.model.generate(\n",
    "    inputs=source_ids,\n",
    "    num_beams=5,\n",
    "    min_length=0,\n",
    "    max_length=512,\n",
    "    length_penalty=0,\n",
    "    output_scores=True,\n",
    "    return_dict_in_generate=True)\n",
    "# print(outputs)\n",
    "print(\"outputs.keys(): \",outputs.keys())\n",
    "print(\"outputs.scores[0].shape:\",outputs.scores[0].shape)\n",
    "print(\"len(outputs.scores):\",len(outputs.scores))\n",
    "#output.scores是一个长度为输出的token数量的元组，元组中的每个元素为batchsize*词表大小的tensor\n",
    "print(\"outputs.scores[0].shape:\",outputs.scores[0].shape) \n",
    "\n",
    "# compute_transition_scores计算出来的transition_scores是一个[batchsize*num_beams,输出的token数量]的tensor，相当于对于每个输出的token一个评分\n",
    "transition_scores = model.model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, normalize_logits=True\n",
    ")\n",
    "print(\"transition_scores.shape:\", transition_scores.shape)\n",
    "print(\"len(transition_scores[0]): \", len(transition_scores[0]))\n",
    "# print(\"transition_scores: \", transition_scores)\n",
    "\n",
    "print(\"outputs.sequences_scores:\", outputs.sequences_scores.item())\n",
    "print(transition_scores.sum())\n",
    "\n",
    "# input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "# 由于t5是encoder-decoder架构，所以这里input_length取1\n",
    "generated_tokens = outputs.sequences[:, 1:] \n",
    "print(\"outputs.sequences: \", outputs.sequences)\n",
    "print(\"len(generated_tokens[0]):\",len(generated_tokens[0]))\n",
    "print(\"generated_tokens: \",generated_tokens)\n",
    "for i in range(generated_tokens.shape[0]):\n",
    "    for tok, score in zip(generated_tokens[i], transition_scores[i]):\n",
    "        # | token | token string | logits | probability\n",
    "        print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.4f} |{np.exp(score.numpy()):.2%}\")\n",
    "    print(\"----------------------------------------------\")\n",
    "\n",
    "print(tokenizer.decode(outputs.sequences[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.57641923427581787109, -1.59257841110229492188,\n",
      "        -2.77720165252685546875, -3.55330038070678710938,\n",
      "        -4.07467317581176757812, -4.47132682800292968750,\n",
      "        -5.66037988662719726562, -5.95519733428955078125,\n",
      "        -6.08485507965087890625, -6.08797788619995117188])\n",
      "tensor([-0.57641923427581787109, -1.59257841110229492188,\n",
      "        -2.77720165252685546875, -3.55330038070678710938,\n",
      "        -4.07467317581176757812, -4.47132682800292968750,\n",
      "        -5.66037988662719726562, -5.95519733428955078125,\n",
      "        -6.08485555648803710938, -6.08797788619995117188], dtype=torch.float64)\n",
      "tensor([0.56190681457519531250, 0.20340049266815185547, 0.06221235543489456177,\n",
      "        0.02862999215722084045, 0.01699776947498321533, 0.01143213640898466110,\n",
      "        0.00348119414411485195, 0.00259233219549059868, 0.00227709440514445305,\n",
      "        0.00226999446749687195])\n",
      "tensor([0.56190682520934231992, 0.20340048496015694646, 0.06221235580842737062,\n",
      "        0.02862999367750398982, 0.01699776905133964719, 0.01143213730316449024,\n",
      "        0.00348119418786893738, 0.00259233222808678181, 0.00227709321184207600,\n",
      "        0.00226999446413249546], dtype=torch.float64)\n",
      "tensor(0.89520018010186508395, dtype=torch.float64)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# generate和compute_transition_scores用法测试\n",
    "inputs = tokenizer([\"sdfadfasfasdfavzcvag\"], return_tensors=\"pt\")\n",
    "outputs = model.model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=5,\n",
    "    num_beams=10,\n",
    "    num_return_sequences=10,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    length_penalty=0\n",
    ")\n",
    "transition_scores = model.model.compute_transition_scores(\n",
    "    outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
    ")\n",
    "# If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
    "# Tip: set `normalize_logits=True` to recompute the scores from the normalized logits.\n",
    "output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
    "length_penalty = 0\n",
    "reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
    "torch.set_printoptions(precision=20)\n",
    "print(outputs.sequences_scores)\n",
    "print(reconstructed_scores)\n",
    "print(np.exp(outputs.sequences_scores))\n",
    "print(np.exp(reconstructed_scores))\n",
    "print(np.exp(reconstructed_scores).sum())\n",
    "print(np.allclose(outputs.sequences_scores, reconstructed_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  2387,    10,     8, 23997,  3368,   628,  2478,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,     3,  1258,\n",
      "           189,    63,   410,    48, 24522,     1,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8,   126, 25453,   648,     3,     6, 10381,\n",
      "         11831,    15,     7,   648,    11,  6179,  6029,   442,   117,  2387],\n",
      "        [    0,  2387,    10,     8,     3,   102,    29,   102,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,  7609,   261,    12,     8,  1075,    13,  1291,\n",
      "          4028,   127,     7,   117,  2387,    10,  4297,  3101,    84,  4840],\n",
      "        [    0,  3102,    10,   112,   117,  2387,    10, 26504,   323,    30,\n",
      "             8,   962,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,     9,  1021,  1370,   117,  2387,    10,\n",
      "          4326,  2856,   840,     3,     6,  2425,   871,    11,  8033,     3],\n",
      "        [    0,  3102,    10,  1440,   117,  2387,    10,     8,     3,  1795,\n",
      "          2420,    13,     8,     3,  3781,    32,   235, 10015,     1,     0],\n",
      "        [    0,  3102,    10,     8,     3, 26165,    29,     3, 16812,   257,\n",
      "           117,  2387,    10,     8,     3,  3781,    32,   235, 10015,     1],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,     3,    23,\n",
      "            47,   838,  1456,     7,     1,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     8, 14864,  7021,   117,  2387,    10,   487,\n",
      "          1151,  7774,  3629,   581,   165,   789,     1,     0,     0,     0],\n",
      "        [    0,  2387,    10,     8,   711,  5492,    76,  1222,  4373,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,  2781,     7,  5003,   117,  2387,    10,\n",
      "           662,   931,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   381,    13, 28480,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,     3, 23064,\n",
      "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,   112, 12914, 12028,    45,  6525,     1,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    62,   117,  2387,    10,     8,  6032,     1,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   178,     9,   469,     1,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,     8,  7648,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    79,   117,  2387,    10,     8,     3,  3781,\n",
      "            32,   235, 10015,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,  2753, 17907,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,     3,    23,   117,  2387,    10,   125,     3,\n",
      "            23,   103,     1,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,   255,     3,    31,     7,  4896,   117,  2387,\n",
      "            10,   255,     3,    31,     7,   182,  2592,     3,     6,    11],\n",
      "        [    0,  3102,    10,  4942,   117,  2387,    10,     8,     3,  1033,\n",
      "          1559, 12315,  2426,     1,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,    21,     8,     3,    76,     5,     7,     5,\n",
      "          2716,    12,  2665,     8, 22040,   136, 10302,   117,  3102,    10],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,     8,  1296,\n",
      "           779,  5217, 20576,    56, 11230,     1,     0,     0,     0,     0],\n",
      "        [    0,  2387,    10,     3,    17, 31081,   117,  3102,    10,     8,\n",
      "         12392,    13,     8,     3,    32,    17,    17,  7396, 13008, 14829],\n",
      "        [    0,  3102,    10,     3,  2781,     7,  5003,   117,  2387,    10,\n",
      "             3,    76,     5,     7,     5,  1058,    13,    20, 15354,    15],\n",
      "        [    0,  3102,    10,     3,    88,   117,  2387,    10,  4028,   112,\n",
      "          1291,    13, 17323,  9569,     1,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,    11,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    0,  3102,    10,   126,     3,   776,   138,   232,  2959, 12748,\n",
      "          6323,     3, 18118,     3,   122,  1647,   117,  2387,    10,   126],\n",
      "        [    0,  3102,    10,   452,     1,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "target: the christmas island space station\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    # dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    # target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "    #             for ids in batch[\"target_ids\"]]\n",
    "    # text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "    #             for ids in batch[\"source_ids\"]]\n",
    "    # texts.extend(text)\n",
    "    # outputs.extend(dec)\n",
    "    # targets.extend(target)\n",
    "    # cnt += 1\n",
    "    # if cnt > 10:\n",
    "    #     break\n",
    "    print(outs)\n",
    "    print(tokenizer.decode(outs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,  2387,    10,     8, 23997,  3368,   628,  2478,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "target: the christmas island space station\n",
      "tensor([    0,  3102,    10,     3,    23,   117,  2387,    10,     3,  1258,\n",
      "          189,    63,   410,    48, 24522,     1,     0,     0,     0,     0])\n",
      "agent: i; target: kathy did this intentionally\n",
      "tensor(0)\n",
      "\n",
      "tensor(3102)\n",
      "agent\n",
      "tensor(10)\n",
      ":\n",
      "tensor(3)\n",
      "\n",
      "tensor(23)\n",
      "i\n",
      "tensor(117)\n",
      ";\n",
      "tensor(2387)\n",
      "target\n",
      "tensor(10)\n",
      ":\n",
      "tensor(3)\n",
      "\n",
      "tensor(1258)\n",
      "ka\n",
      "tensor(189)\n",
      "th\n",
      "tensor(63)\n",
      "y\n",
      "tensor(410)\n",
      "did\n",
      "tensor(48)\n",
      "this\n",
      "tensor(24522)\n",
      "intentionally\n"
     ]
    }
   ],
   "source": [
    "print(outs[0])\n",
    "print(tokenizer.decode(outs[0], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "\n",
    "print(outs[1])\n",
    "print(tokenizer.decode(outs[1], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())\n",
    "for i in range(15):\n",
    "    print(outs[1][i])\n",
    "    print(tokenizer.decode(outs[1][i], skip_special_tokens=True, clean_up_tokenization_spaces=False).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:16:17.579251Z",
     "iopub.status.busy": "2024-02-28T15:16:17.578902Z",
     "iopub.status.idle": "2024-02-28T15:16:17.585178Z",
     "shell.execute_reply": "2024-02-28T15:16:17.584743Z",
     "shell.execute_reply.started": "2024-02-28T15:16:17.579228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:16:22.561581Z",
     "iopub.status.busy": "2024-02-28T15:16:22.560990Z",
     "iopub.status.idle": "2024-02-28T15:16:57.335332Z",
     "shell.execute_reply": "2024-02-28T15:16:57.334689Z",
     "shell.execute_reply.started": "2024-02-28T15:16:22.561561Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/48 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "100%|██████████| 48/48 [00:33<00:00,  1.42it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = MPQADataset(tokenizer=tokenizer, dataset=dataset, type_path='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32,\n",
    "                             num_workers=2, shuffle=True)\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "outputs = []\n",
    "targets = []\n",
    "all_text = []\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['source_ids'].to(\"cuda\")\n",
    "    attention_mask = batch['source_mask'].to(\"cuda\")\n",
    "    outs = model.generate(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask)\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "    pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    true_labels.extend(true_label)\n",
    "    pred_labels.extend(pred_label)\n",
    "    all_text.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-28T15:17:33.521899Z",
     "iopub.status.busy": "2024-02-28T15:17:33.521562Z",
     "iopub.status.idle": "2024-02-28T15:17:35.880177Z",
     "shell.execute_reply": "2024-02-28T15:17:35.879656Z",
     "shell.execute_reply.started": "2024-02-28T15:17:33.521876Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  we have been left with no alternative but to buy some weapons via our allies in the drc who have access to some of these european weapons that we have preference for . '' dse:have preference for\n",
      "Predicted Token Class:  ['B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  all this has sounded the alarm for the us executive and suits the opposition 's objectives to bring about an intervention or a blockade , which would hurt the people it claims to defend . '' dse:claims\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  4 february upset the lives of venezuelans with the ghost of a military insurgency that we had all thought had been overcome , because it belonged to past eras and because the armed forces were fully within the legitimate order , complying with institutional precepts , far from the meddling forbidden by the constitution . dse:had all thought\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  all said they were protesting an election they said had been tainted by months of political violence and intimidation , last-minute changes to electoral laws , and the disenfranchisement of tens of thousands of voters in harare -- an opposition stronghold -- who failed to vote because the government reduced the number of polling stations . dse:said\n",
      "Predicted Token Class:  ['O', 'O', 'B-AGENT', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'B-AGENT', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  taiwan 's foreign minister tien hung-mao had vowed to back the us in fighting terrorism saying taiwan had  no option '' but to do so , despite a lack of diplomatic ties between taipei and washington . dse:had vowed\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'I-AGENT', 'I-AGENT', 'I-AGENT', 'I-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  the community initiatives committee is working with our development office to put together a long-range fundraising plan that will ultimately bring more money into our centers . dse:put together\n",
      "Predicted Token Class:  ['B-AGENT', 'I-AGENT', 'I-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'I-AGENT', 'I-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  president khatami , for his part , highlighted a collective quest by opec member states to stabilize oil prices at a reasonable level . dse:highlighted\n",
      "Predicted Token Class:  ['B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  investors and western diplomats have said they might interpret mbeki 's support for mugabe or the elections as a sign that africa is not intent on revitalizing its economies through good government and expanded international trade . dse:is not intent\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  he added :  there is no reason for any country to request the agreement of others to determine its conduct and adopt its decisions . '' dse:to request\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'I-AGENT', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'I-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  in perfect agreement with the latest polls on the popularity of president hugo chavez , the most recent study by alfredo keller has confirmed the resounding and sustained fall of the approval rating the president had at the beginning of his administration . dse:confirmed the resounding and sustained fall of the approval rating\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-AGENT', 'I-AGENT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-TARGET', 'I-TARGET', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "{'AGENT': {'precision': 0.7375415282392026, 'recall': 0.7449664429530202, 'f1': 0.7412353923205344, 'number': 1192}, 'TARGET': {'precision': 0.48408710217755446, 'recall': 0.4605577689243028, 'f1': 0.472029399755002, 'number': 1255}, 'overall_precision': 0.6113427856547122, 'overall_recall': 0.5991009399264405, 'overall_f1': 0.6051599587203303, 'overall_accuracy': 0.8741352105838087}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Text:  {all_text[i]}\")\n",
    "    print(f\"Predicted Token Class:  {pred_labels[i]}\")\n",
    "    print(f\"True Token Class:  {true_labels[i]}\")\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n",
    "print(metric.compute(predictions=pred_labels, references=true_labels))\n",
    "# {'AGENT': {'precision': 0.7375415282392026, 'recall': 0.7449664429530202, 'f1': 0.7412353923205344, 'number': 1192}, 'TARGET': {'precision': 0.48408710217755446, 'recall': 0.4605577689243028, 'f1': 0.472029399755002, 'number': 1255}, 'overall_precision': 0.6113427856547122, 'overall_recall': 0.5991009399264405, 'overall_f1': 0.6051599587203303, 'overall_accuracy': 0.8741352105838087}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8h8R2bLDhjlc"
   },
   "source": [
    "# Model\n",
    "\n",
    "Majority of the code here is adapted from [here](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) which uses the pytorch-lightning framework for training neural networks. T5 has shown that it can generate state of the art on many tasks as long as it can be cast as a text-to-text problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:24:59.853431Z",
     "iopub.status.busy": "2024-02-27T12:24:59.853050Z",
     "iopub.status.idle": "2024-02-27T12:24:59.864657Z",
     "shell.execute_reply": "2024-02-27T12:24:59.863958Z",
     "shell.execute_reply.started": "2024-02-27T12:24:59.853410Z"
    },
    "id": "KL8_p4YS6H0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class T5FineTuner(pl.LightningModule):\n",
    "    def __init__(self, hparam):\n",
    "        super(T5FineTuner, self).__init__()\n",
    "        self.hparam = hparam\n",
    "\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "            hparam.model_name_or_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            hparam.model_name_or_path\n",
    "        )\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def is_logger(self):\n",
    "        return True\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids, attention_mask=None, decoder_input_ids=None, decoder_attention_mask=None, lm_labels=None\n",
    "    ):\n",
    "        return self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=lm_labels,\n",
    "        )\n",
    "\n",
    "    def _step(self, batch):\n",
    "        lm_labels = batch[\"target_ids\"]\n",
    "        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        outputs = self(\n",
    "            input_ids=batch[\"source_ids\"],\n",
    "            attention_mask=batch[\"source_mask\"],\n",
    "            lm_labels=lm_labels,\n",
    "            decoder_attention_mask=batch['target_mask']\n",
    "        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "\n",
    "        tensorboard_logs = {\"train_loss\": loss}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        return {\"val_loss\": loss}\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"val_loss\": avg_loss}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"Prepare optimizer and schedule (linear warmup and decay)\"\n",
    "\n",
    "        model = self.model\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparam.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparam.learning_rate, eps=self.hparam.adam_epsilon)\n",
    "        self.opt = optimizer\n",
    "        return [optimizer]\n",
    "\n",
    "    def optimizer_step(self,\n",
    "                       epoch=None,\n",
    "                       batch_idx=None,\n",
    "                       optimizer=None,\n",
    "                       optimizer_idx=None,\n",
    "                       optimizer_closure=None,\n",
    "                       on_tpu=None,\n",
    "                       using_native_amp=None,\n",
    "                       using_lbfgs=None\n",
    "                       ):\n",
    "\n",
    "        optimizer.step(closure=optimizer_closure)\n",
    "        optimizer.zero_grad()\n",
    "        self.lr_scheduler.step()\n",
    "\n",
    "    def get_tqdm_dict(self):\n",
    "        tqdm_dict = {\"loss\": \"{:.3f}\".format(\n",
    "            self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n",
    "\n",
    "        return tqdm_dict\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"train\", args=self.hparam)\n",
    "        dataloader = DataLoader(train_dataset, batch_size=self.hparam.train_batch_size,\n",
    "                                drop_last=True, shuffle=True, num_workers=2)\n",
    "        t_total = (\n",
    "            (len(dataloader.dataset) //\n",
    "             (self.hparam.train_batch_size * max(1, self.hparam.n_gpu)))\n",
    "            // self.hparam.gradient_accumulation_steps\n",
    "            * float(self.hparam.num_train_epochs)\n",
    "        )\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            self.opt, num_warmup_steps=self.hparam.warmup_steps, num_training_steps=t_total\n",
    "        )\n",
    "        self.lr_scheduler = scheduler\n",
    "        return dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = get_dataset(\n",
    "            tokenizer=self.tokenizer, type_path=\"validation\", args=self.hparam)\n",
    "        return DataLoader(val_dataset, batch_size=self.hparam.eval_batch_size, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:01.768401Z",
     "iopub.status.busy": "2024-02-27T12:25:01.767846Z",
     "iopub.status.idle": "2024-02-27T12:25:01.773286Z",
     "shell.execute_reply": "2024-02-27T12:25:01.772660Z",
     "shell.execute_reply.started": "2024-02-27T12:25:01.768381Z"
    },
    "id": "6VQpUMNe6Wf8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LoggingCallback(pl.Callback):\n",
    "  def on_validation_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Validation results *****\")\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "      # Log results\n",
    "      for key in sorted(metrics):\n",
    "        if key not in [\"log\", \"progress_bar\"]:\n",
    "          logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "\n",
    "  def on_test_end(self, trainer, pl_module):\n",
    "    logger.info(\"***** Test results *****\")\n",
    "\n",
    "    if pl_module.is_logger():\n",
    "      metrics = trainer.callback_metrics\n",
    "\n",
    "      # Log and save results to file\n",
    "      output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n",
    "      with open(output_test_results_file, \"w\") as writer:\n",
    "        for key in sorted(metrics):\n",
    "          if key not in [\"log\", \"progress_bar\"]:\n",
    "            logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n",
    "            writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:02.692693Z",
     "iopub.status.busy": "2024-02-27T12:25:02.692356Z",
     "iopub.status.idle": "2024-02-27T12:25:02.697059Z",
     "shell.execute_reply": "2024-02-27T12:25:02.696544Z",
     "shell.execute_reply.started": "2024-02-27T12:25:02.692672Z"
    },
    "id": "I55QMghp6YbE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir=\"wikiann\", # path for data files\n",
    "    output_dir=\"\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-small',\n",
    "    tokenizer_name_or_path='t5-small',\n",
    "    max_seq_length=256,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=8,\n",
    "    eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=True, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxV5l0Wcinfb"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "Here, I used the popular [WikiANN](https://https://huggingface.co/datasets/wikiann) dataset which is a multilingual named entity recognition dataset consisting of Wikipedia articles annotated with LOC (location), PER (person), and ORG (organisation) tags in the IOB2 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "b112c9a8184d4b479fc0e59698789b43",
      "e550224ab580420cb4973b805c2393af",
      "ebcc288979ff4cd5901e9223063751d1",
      "304c605d63584a298e8cefd4ffcf4ead",
      "7ca93c1fe9a9415e9c7d6f22a58b90d1",
      "56496f0e8cef4b5cb989f289af418f9f",
      "388d8e1e34a143c3afe9e4082be44ebb",
      "5d88a23982ac46b3b8077264a6998b9a",
      "dfdd904fb4924ad8948793222255cc3b",
      "43278de125c54c1e8b55baa0370c4cc1",
      "fa0489927bdd4bf4809c9fcc92d9d7b4"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:09.149655Z",
     "iopub.status.busy": "2024-02-27T12:25:09.149329Z",
     "iopub.status.idle": "2024-02-27T12:25:26.708433Z",
     "shell.execute_reply": "2024-02-27T12:25:26.707926Z",
     "shell.execute_reply.started": "2024-02-27T12:25:09.149635Z"
    },
    "id": "soCZS7n07Ts1",
    "outputId": "6e0d1772-838a-414e-b028-e6362faeb653",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"wikiann\", \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.709771Z",
     "iopub.status.busy": "2024-02-27T12:25:26.709381Z",
     "iopub.status.idle": "2024-02-27T12:25:26.713447Z",
     "shell.execute_reply": "2024-02-27T12:25:26.712901Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.709753Z"
    },
    "id": "7eSAir8g80rm",
    "outputId": "aaf53c53-939a-424a-e625-9ba8059fee67",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    validation: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags', 'langs', 'spans'],\n",
      "        num_rows: 20000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.714288Z",
     "iopub.status.busy": "2024-02-27T12:25:26.713990Z",
     "iopub.status.idle": "2024-02-27T12:25:26.720368Z",
     "shell.execute_reply": "2024-02-27T12:25:26.719900Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.714273Z"
    },
    "id": "cWCXP8F373Nm",
    "outputId": "bc803c3d-6947-4c16-eb68-0fde40b8289c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'R.H. Saunders ( St. Lawrence River ) ( 968 MW )'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(dataset['train'][0]['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:26.721462Z",
     "iopub.status.busy": "2024-02-27T12:25:26.721156Z",
     "iopub.status.idle": "2024-02-27T12:25:26.725372Z",
     "shell.execute_reply": "2024-02-27T12:25:26.724914Z",
     "shell.execute_reply.started": "2024-02-27T12:25:26.721445Z"
    },
    "id": "rXjs7Khn9oi6",
    "outputId": "c96fa06e-f32d-4cad-bd85-513ffde91282",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['R.H.',\n",
       "  'Saunders',\n",
       "  '(',\n",
       "  'St.',\n",
       "  'Lawrence',\n",
       "  'River',\n",
       "  ')',\n",
       "  '(',\n",
       "  '968',\n",
       "  'MW',\n",
       "  ')'],\n",
       " 'ner_tags': [3, 4, 0, 3, 4, 4, 0, 0, 0, 0, 0],\n",
       " 'langs': ['en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en', 'en'],\n",
       " 'spans': ['ORG: R.H. Saunders', 'ORG: St. Lawrence River']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5KDQPACi8FT"
   },
   "source": [
    "In this section, we create a custom dataset class where we cast the NER task as a text to text problem. This is done by concatenating the spans in the data as one line of string separated by a semi-colon (;). e.g\n",
    "\n",
    "*   **Input**: R.H. Saunders ( St. Lawrence River ) ( 968 MW )\n",
    "*   **Target**: ORG: R.H. Saunders; ORG: St. Lawrence River\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:28.595917Z",
     "iopub.status.busy": "2024-02-27T12:25:28.595563Z",
     "iopub.status.idle": "2024-02-27T12:25:28.602276Z",
     "shell.execute_reply": "2024-02-27T12:25:28.601623Z",
     "shell.execute_reply.started": "2024-02-27T12:25:28.595896Z"
    },
    "id": "LgZKb7T48Mzw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WikiAnnDataset(Dataset):\n",
    "  def __init__(self, tokenizer, dataset, type_path, max_len=512):\n",
    "\n",
    "    self.data = dataset[type_path]\n",
    "    self.max_len = max_len\n",
    "    self.tokenizer = tokenizer\n",
    "    self.tokenizer.max_length = max_len\n",
    "    self.tokenizer.model_max_length = max_len\n",
    "    self.inputs = []\n",
    "    self.targets = []\n",
    "\n",
    "    self._build()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.inputs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
    "    target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
    "\n",
    "    src_mask    = self.inputs[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "    target_mask = self.targets[index][\"attention_mask\"].squeeze()  # might need to squeeze\n",
    "\n",
    "    return {\"source_ids\": source_ids, \"source_mask\": src_mask, \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
    "\n",
    "  def _build(self):\n",
    "    for idx in range(len(self.data)):\n",
    "      input_, target = \" \".join(self.data[idx][\"tokens\"]), \"; \".join(self.data[idx][\"spans\"])\n",
    "\n",
    "      input_ = input_.lower() + ' </s>'\n",
    "      target = target.lower() + \" </s>\"\n",
    "\n",
    "       # tokenize inputs\n",
    "      tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
    "          [input_], max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "       # tokenize targets\n",
    "      tokenized_targets = self.tokenizer.batch_encode_plus(\n",
    "          [target],max_length=self.max_len, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      self.inputs.append(tokenized_inputs)\n",
    "      self.targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:33.140341Z",
     "iopub.status.busy": "2024-02-27T12:25:33.140007Z",
     "iopub.status.idle": "2024-02-27T12:25:42.716000Z",
     "shell.execute_reply": "2024-02-27T12:25:42.715500Z",
     "shell.execute_reply.started": "2024-02-27T12:25:33.140323Z"
    },
    "id": "XrlJEayI-4tS",
    "outputId": "97284917-3b24-462e-fd2c-930f0061f79f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5TokenizerFast(name_or_path='../T5-base', vocab_size=32100, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<extra_id_0>', '<extra_id_1>', '<extra_id_2>', '<extra_id_3>', '<extra_id_4>', '<extra_id_5>', '<extra_id_6>', '<extra_id_7>', '<extra_id_8>', '<extra_id_9>', '<extra_id_10>', '<extra_id_11>', '<extra_id_12>', '<extra_id_13>', '<extra_id_14>', '<extra_id_15>', '<extra_id_16>', '<extra_id_17>', '<extra_id_18>', '<extra_id_19>', '<extra_id_20>', '<extra_id_21>', '<extra_id_22>', '<extra_id_23>', '<extra_id_24>', '<extra_id_25>', '<extra_id_26>', '<extra_id_27>', '<extra_id_28>', '<extra_id_29>', '<extra_id_30>', '<extra_id_31>', '<extra_id_32>', '<extra_id_33>', '<extra_id_34>', '<extra_id_35>', '<extra_id_36>', '<extra_id_37>', '<extra_id_38>', '<extra_id_39>', '<extra_id_40>', '<extra_id_41>', '<extra_id_42>', '<extra_id_43>', '<extra_id_44>', '<extra_id_45>', '<extra_id_46>', '<extra_id_47>', '<extra_id_48>', '<extra_id_49>', '<extra_id_50>', '<extra_id_51>', '<extra_id_52>', '<extra_id_53>', '<extra_id_54>', '<extra_id_55>', '<extra_id_56>', '<extra_id_57>', '<extra_id_58>', '<extra_id_59>', '<extra_id_60>', '<extra_id_61>', '<extra_id_62>', '<extra_id_63>', '<extra_id_64>', '<extra_id_65>', '<extra_id_66>', '<extra_id_67>', '<extra_id_68>', '<extra_id_69>', '<extra_id_70>', '<extra_id_71>', '<extra_id_72>', '<extra_id_73>', '<extra_id_74>', '<extra_id_75>', '<extra_id_76>', '<extra_id_77>', '<extra_id_78>', '<extra_id_79>', '<extra_id_80>', '<extra_id_81>', '<extra_id_82>', '<extra_id_83>', '<extra_id_84>', '<extra_id_85>', '<extra_id_86>', '<extra_id_87>', '<extra_id_88>', '<extra_id_89>', '<extra_id_90>', '<extra_id_91>', '<extra_id_92>', '<extra_id_93>', '<extra_id_94>', '<extra_id_95>', '<extra_id_96>', '<extra_id_97>', '<extra_id_98>', '<extra_id_99>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32000: AddedToken(\"<extra_id_99>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<extra_id_98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<extra_id_97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<extra_id_96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<extra_id_95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<extra_id_94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<extra_id_93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<extra_id_92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<extra_id_91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<extra_id_90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<extra_id_89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32011: AddedToken(\"<extra_id_88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32012: AddedToken(\"<extra_id_87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32013: AddedToken(\"<extra_id_86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32014: AddedToken(\"<extra_id_85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32015: AddedToken(\"<extra_id_84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32016: AddedToken(\"<extra_id_83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32017: AddedToken(\"<extra_id_82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32018: AddedToken(\"<extra_id_81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32019: AddedToken(\"<extra_id_80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32020: AddedToken(\"<extra_id_79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32021: AddedToken(\"<extra_id_78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32022: AddedToken(\"<extra_id_77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32023: AddedToken(\"<extra_id_76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32024: AddedToken(\"<extra_id_75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32025: AddedToken(\"<extra_id_74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32026: AddedToken(\"<extra_id_73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32027: AddedToken(\"<extra_id_72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32028: AddedToken(\"<extra_id_71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32029: AddedToken(\"<extra_id_70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32030: AddedToken(\"<extra_id_69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32031: AddedToken(\"<extra_id_68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32032: AddedToken(\"<extra_id_67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32033: AddedToken(\"<extra_id_66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32034: AddedToken(\"<extra_id_65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32035: AddedToken(\"<extra_id_64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32036: AddedToken(\"<extra_id_63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32037: AddedToken(\"<extra_id_62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32038: AddedToken(\"<extra_id_61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32039: AddedToken(\"<extra_id_60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32040: AddedToken(\"<extra_id_59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32041: AddedToken(\"<extra_id_58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32042: AddedToken(\"<extra_id_57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32043: AddedToken(\"<extra_id_56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32044: AddedToken(\"<extra_id_55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32045: AddedToken(\"<extra_id_54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32046: AddedToken(\"<extra_id_53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32047: AddedToken(\"<extra_id_52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32048: AddedToken(\"<extra_id_51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32049: AddedToken(\"<extra_id_50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32050: AddedToken(\"<extra_id_49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32051: AddedToken(\"<extra_id_48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32052: AddedToken(\"<extra_id_47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32053: AddedToken(\"<extra_id_46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32054: AddedToken(\"<extra_id_45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32055: AddedToken(\"<extra_id_44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32056: AddedToken(\"<extra_id_43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32057: AddedToken(\"<extra_id_42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32058: AddedToken(\"<extra_id_41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32059: AddedToken(\"<extra_id_40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32060: AddedToken(\"<extra_id_39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32061: AddedToken(\"<extra_id_38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32062: AddedToken(\"<extra_id_37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32063: AddedToken(\"<extra_id_36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32064: AddedToken(\"<extra_id_35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32065: AddedToken(\"<extra_id_34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32066: AddedToken(\"<extra_id_33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32067: AddedToken(\"<extra_id_32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32068: AddedToken(\"<extra_id_31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32069: AddedToken(\"<extra_id_30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32070: AddedToken(\"<extra_id_29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32071: AddedToken(\"<extra_id_28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32072: AddedToken(\"<extra_id_27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32073: AddedToken(\"<extra_id_26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32074: AddedToken(\"<extra_id_25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32075: AddedToken(\"<extra_id_24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32076: AddedToken(\"<extra_id_23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32077: AddedToken(\"<extra_id_22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32078: AddedToken(\"<extra_id_21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32079: AddedToken(\"<extra_id_20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32080: AddedToken(\"<extra_id_19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32081: AddedToken(\"<extra_id_18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32082: AddedToken(\"<extra_id_17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32083: AddedToken(\"<extra_id_16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32084: AddedToken(\"<extra_id_15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32085: AddedToken(\"<extra_id_14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32086: AddedToken(\"<extra_id_13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32087: AddedToken(\"<extra_id_12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32088: AddedToken(\"<extra_id_11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32089: AddedToken(\"<extra_id_10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32090: AddedToken(\"<extra_id_9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32091: AddedToken(\"<extra_id_8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32092: AddedToken(\"<extra_id_7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32093: AddedToken(\"<extra_id_6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32094: AddedToken(\"<extra_id_5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32095: AddedToken(\"<extra_id_4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32096: AddedToken(\"<extra_id_3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32097: AddedToken(\"<extra_id_2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32098: AddedToken(\"<extra_id_1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32099: AddedToken(\"<extra_id_0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"../T5-base\")\n",
    "\n",
    "print(tokenizer)\n",
    "\n",
    "input_dataset = WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:42.717524Z",
     "iopub.status.busy": "2024-02-27T12:25:42.717015Z",
     "iopub.status.idle": "2024-02-27T12:25:42.871971Z",
     "shell.execute_reply": "2024-02-27T12:25:42.871475Z",
     "shell.execute_reply.started": "2024-02-27T12:25:42.717506Z"
    },
    "id": "Cyrnvv1zTKaw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(input_dataset)):\n",
    "    _ = input_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:42.873108Z",
     "iopub.status.busy": "2024-02-27T12:25:42.872825Z",
     "iopub.status.idle": "2024-02-27T12:25:42.876923Z",
     "shell.execute_reply": "2024-02-27T12:25:42.876466Z",
     "shell.execute_reply.started": "2024-02-27T12:25:42.873090Z"
    },
    "id": "OJ02SXgv_UW8",
    "outputId": "5c541c8e-9376-47ed-bc9f-7e138caee0e7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r.h. saunders ( st. lawrence river ) ( 968 mw )</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "org: r.h. saunders; org: st. lawrence river</s></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "data = input_dataset[0]\n",
    "\n",
    "print(tokenizer.decode(data[\"source_ids\"], skip_special_tokens=False))\n",
    "print(tokenizer.decode(data[\"target_ids\"], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:43.437406Z",
     "iopub.status.busy": "2024-02-27T12:25:43.437088Z",
     "iopub.status.idle": "2024-02-27T12:25:43.685932Z",
     "shell.execute_reply": "2024-02-27T12:25:43.685362Z",
     "shell.execute_reply.started": "2024-02-27T12:25:43.437387Z"
    },
    "id": "Sepl17_eCHy4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p t5_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:53.021540Z",
     "iopub.status.busy": "2024-02-27T12:25:53.021188Z",
     "iopub.status.idle": "2024-02-27T12:25:54.752236Z",
     "shell.execute_reply": "2024-02-27T12:25:54.751686Z",
     "shell.execute_reply.started": "2024-02-27T12:25:53.021519Z"
    },
    "id": "4Toa_qnXDrTw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(**args_dict)\n",
    "model = T5FineTuner(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:25:57.604520Z",
     "iopub.status.busy": "2024-02-27T12:25:57.604156Z",
     "iopub.status.idle": "2024-02-27T12:25:57.608734Z",
     "shell.execute_reply": "2024-02-27T12:25:57.608252Z",
     "shell.execute_reply.started": "2024-02-27T12:25:57.604493Z"
    },
    "id": "dIZ3LwE3DXNo",
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filename=args.output_dir+\"/checkpoint.pth\", monitor=\"val_loss\", mode=\"min\", save_top_k=5\n",
    ")\n",
    "\n",
    "train_params = dict(\n",
    "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "    gpus=args.n_gpu,\n",
    "    max_epochs=args.num_train_epochs,\n",
    "    #early_stop_callback=False,\n",
    "    precision= 16 if args.fp_16 else 32,\n",
    "    #amp_level=args.opt_level,\n",
    "    gradient_clip_val=args.max_grad_norm,\n",
    "    checkpoint_callback=checkpoint_callback,\n",
    "    callbacks=[LoggingCallback()],\n",
    ")\n",
    "\n",
    "# train_params = dict(\n",
    "#     accumulate_grad_batches=args.gradient_accumulation_steps,\n",
    "#     ## gpus=args.n_gpu,\n",
    "#     max_epochs=args.num_train_epochs,\n",
    "#     #early_stop_callback=False,\n",
    "#     precision= 16 if args.fp_16 else 32,\n",
    "#     #amp_level=args.opt_level,\n",
    "#     gradient_clip_val=args.max_grad_norm,\n",
    "#     # callbacks=[LoggingCallback()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:01.158877Z",
     "iopub.status.busy": "2024-02-27T12:26:01.158529Z",
     "iopub.status.idle": "2024-02-27T12:26:01.162217Z",
     "shell.execute_reply": "2024-02-27T12:26:01.161716Z",
     "shell.execute_reply.started": "2024-02-27T12:26:01.158856Z"
    },
    "id": "MxBEgT6MDqe3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(tokenizer, type_path, args):\n",
    "    tokenizer.max_length = args.max_seq_length\n",
    "    tokenizer.model_max_length = args.max_seq_length\n",
    "    dataset = load_dataset(args.data_dir, \"en\")\n",
    "    return WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path=type_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:01.731175Z",
     "iopub.status.busy": "2024-02-27T12:26:01.730847Z",
     "iopub.status.idle": "2024-02-27T12:26:01.738097Z",
     "shell.execute_reply": "2024-02-27T12:26:01.737626Z",
     "shell.execute_reply.started": "2024-02-27T12:26:01.731149Z"
    },
    "id": "KCKmlJ5DDaHw",
    "outputId": "e5db4e86-8699-475a-a794-cc03d8b615cc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:147: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f6e53eb2f70>)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=<pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint object at 0x7f6e53eb2f70>)`.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445,
     "referenced_widgets": [
      "d2b327e2b0074354a6b889afc169a62e",
      "84ab2eb4f0b74ac3a334eb7ecd7c4d63",
      "f8ebf6d2f2954cbbb525e67de58a766a",
      "a0a3db286a1843b293b1d3b944a3d33a",
      "d68d5a7c97bd47719eb1e19f67d64805",
      "1fac6af61b4d411a94f3916061731428",
      "127c67d7658d4bf392408e7d5600a09c",
      "25b71debb82040a2960eb8348df2a4ca",
      "9428191170994f0aa7fd400caf0d1d07",
      "ef4fa30c44df4726a36cfc1da100fb4f",
      "dffd33c1015f4bfd91fd702095c2127e",
      "ba1298c2f3db42c48b7e3e3ee83d5000",
      "0e46dd857d504def992f555a9e9c9c5f",
      "285649e2ae134f87bd2f8dcf40a7758d",
      "83ec7a9ea11e410bb3fcad13c082d664",
      "5fd2178313d34631a030d79934dcffe0",
      "c3c43f7938044239b4eca004d07874ae",
      "b3e24f0b77884455868b103ab69ddd5d",
      "79385887dd0f479d993b251748f8cb2f",
      "429fbc36c65446d09d32298cfc4b606e",
      "b449b3c3b1b944a38293f2f13d517860",
      "c4dc2a989d794b44b8000ca88cf095fa",
      "97e269fbcb954d629b2314d30c41c57b",
      "84c7612e3ed740958159eec4ee147476",
      "93c2ea7279794b88b6264f832400e3c7",
      "45340f3b0bf4413182060f27c7458334",
      "6bf0078bd11d4ed980f1b7a9b612a091",
      "4fb2b190729943cb8c1f1fd2b84d6935",
      "a5a571c48de54900b2b696f2c6817a2e",
      "966b77941ef046babfb1e9ed597a6aee",
      "f16174670d2c4182ace507dda7812328",
      "e61579f72d51414782f8fe3c28e12e86",
      "dbcd62725c684fcc897583042ca44888",
      "ba6f4922558c41fea4a4b171a021b871",
      "5587ced0255640dd9bec68478cd973fb",
      "48bb2d6d069e411a9e969187d5ddf17d",
      "4fffbd7c89314024a7b8c128ad1ba248",
      "dea06faaa22b41b6bbfd7ff6d8ee39f6",
      "347f2c97cd4f46aa8bd1d30f79041b29",
      "81211b58e69b4973a9e1b500e621656b",
      "ce97e8d1d6b54e17a16ec75e1b3abf7f",
      "5d4e33c5404f49caa4811bfd55b0bee5",
      "d37bf18a5e9346fcb59242e0e5288307",
      "005a930db3d242a4be28a9c5923eed76",
      "0573bd35717d4189ab4d74fdc489653b",
      "06e6627b73a24e4291f316a281a20aea",
      "bd7b7a9f5f80492997ec2cbd4bf22627",
      "1b08c691a8db4cb79e6f7837a08ac799",
      "b904bbebe38943728a23a0d630f86a88",
      "9cd468cf60c643db96d0cd35923c8258",
      "cddc7d98bbb740a9a81bf8e244d4595d",
      "cfcb95376da04ce884ab18c57c2d63ed",
      "f69895f136a14e629614a45f2e7d8557",
      "47144694b20f49f0aa54ee1d83119fda",
      "3898654592194daca51999f513192a24",
      "39ceb75a4fa14f6e9a8a5f4ae1324cbc",
      "49901550f7c14e439505f80faaaa8357",
      "4bba40ddade644b79cce613925a4f47e",
      "aa38ae45e85545bebec6c8b43db7abbf",
      "37a7f3f9244a4fd991fe9e8847268ec1",
      "b537afbec50e4567885220cf27a5b761",
      "ce10ff94a3454674a5deb511394387ff",
      "be10758757c1426c8779c6706a445ecd",
      "d26f30f1432e4f2ab1c53484b44989ee",
      "0305993a3c1a4f8296faeb1aba61fa38",
      "fce9ba2896164d4391fea7f32dc3d811",
      "ffe6027ef4bb4fd9964717116a39f1a3",
      "4b15ff3c6b044b40a15b21f051bf53c8",
      "3ddd6cf350fb4378ad6cf2698f67d68f",
      "ceca8a9a02fe46d5a1a3aa093fdbcde7",
      "20d7306a9b474da3b0c8e80a3d7abe09",
      "3b924323276f4f92a66a64d295ba6c78",
      "21b88aca18f048759fb25ce63540c992",
      "b58e5e54e53e46d7a4f196a93812373c",
      "b849b43f5c4649deb018fb959191edc5",
      "95d7c71216094ea0b466a79efc95341d",
      "264fdba12e854d54b3ed2510458c7a1b"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T12:26:10.156541Z",
     "iopub.status.busy": "2024-02-27T12:26:10.156209Z",
     "iopub.status.idle": "2024-02-27T12:50:26.838508Z",
     "shell.execute_reply": "2024-02-27T12:50:26.837953Z",
     "shell.execute_reply.started": "2024-02-27T12:26:10.156522Z"
    },
    "id": "sxQ-s0izFQGQ",
    "outputId": "fd7d2eef-efef-4367-dcc3-70ff9e3d18fb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:102: UserWarning: When using `Trainer(accumulate_grad_batches != 1)` and overriding `LightningModule.optimizer_{step,zero_grad}`, the hooks will not be called on every batch (rather, they are called on every optimization step).\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 60.5 M\n",
      "-----------------------------------------------------\n",
      "60.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "60.5 M    Total params\n",
      "121.013   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:110: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/3750 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/pytorch_lightning/loops/optimization/closure.py:35: LightningDeprecationWarning: One of the returned values {'log'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  67%|██████▋   | 2500/3750 [06:36<03:18,  6.30it/s, loss=0.138, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  67%|██████▋   | 2502/3750 [06:37<03:18,  6.30it/s, loss=0.138, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0:  67%|██████▋   | 2504/3750 [06:37<03:17,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2506/3750 [06:37<03:17,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2508/3750 [06:37<03:16,  6.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2510/3750 [06:37<03:16,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2512/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2514/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2516/3750 [06:37<03:15,  6.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2518/3750 [06:37<03:14,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2520/3750 [06:38<03:14,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2522/3750 [06:38<03:13,  6.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2524/3750 [06:38<03:13,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2526/3750 [06:38<03:13,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2528/3750 [06:38<03:12,  6.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  67%|██████▋   | 2530/3750 [06:38<03:12,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2532/3750 [06:38<03:11,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2534/3750 [06:38<03:11,  6.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2536/3750 [06:38<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2538/3750 [06:39<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2540/3750 [06:39<03:10,  6.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2542/3750 [06:39<03:09,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2544/3750 [06:39<03:09,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2546/3750 [06:39<03:08,  6.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2548/3750 [06:39<03:08,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2550/3750 [06:39<03:08,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2552/3750 [06:39<03:07,  6.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2554/3750 [06:39<03:07,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2556/3750 [06:40<03:06,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2558/3750 [06:40<03:06,  6.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2560/3750 [06:40<03:06,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2562/3750 [06:40<03:05,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2564/3750 [06:40<03:05,  6.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2566/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  68%|██████▊   | 2568/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2570/3750 [06:40<03:04,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2572/3750 [06:40<03:03,  6.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2574/3750 [06:41<03:03,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2576/3750 [06:41<03:02,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▊   | 2578/3750 [06:41<03:02,  6.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2580/3750 [06:41<03:02,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2582/3750 [06:41<03:01,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2584/3750 [06:41<03:01,  6.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2586/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2588/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2590/3750 [06:41<03:00,  6.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2592/3750 [06:42<02:59,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2594/3750 [06:42<02:59,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2596/3750 [06:42<02:58,  6.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2598/3750 [06:42<02:58,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2600/3750 [06:42<02:58,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2602/3750 [06:42<02:57,  6.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2604/3750 [06:42<02:57,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  69%|██████▉   | 2606/3750 [06:42<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2608/3750 [06:42<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2610/3750 [06:43<02:56,  6.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2612/3750 [06:43<02:55,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2614/3750 [06:43<02:55,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2616/3750 [06:43<02:54,  6.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2618/3750 [06:43<02:54,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2620/3750 [06:43<02:54,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2622/3750 [06:43<02:53,  6.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|██████▉   | 2624/3750 [06:43<02:53,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2626/3750 [06:43<02:52,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2628/3750 [06:44<02:52,  6.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2630/3750 [06:44<02:52,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2632/3750 [06:44<02:51,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2634/3750 [06:44<02:51,  6.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2636/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2638/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2640/3750 [06:44<02:50,  6.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  70%|███████   | 2642/3750 [06:44<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2644/3750 [06:45<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2646/3750 [06:45<02:49,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2648/3750 [06:45<02:48,  6.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2650/3750 [06:45<02:48,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2652/3750 [06:45<02:47,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2654/3750 [06:45<02:47,  6.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2656/3750 [06:45<02:47,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2658/3750 [06:45<02:46,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2660/3750 [06:45<02:46,  6.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2662/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2664/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2666/3750 [06:46<02:45,  6.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2668/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████   | 2670/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2672/3750 [06:46<02:44,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2674/3750 [06:46<02:43,  6.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2676/3750 [06:46<02:43,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2678/3750 [06:46<02:42,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  71%|███████▏  | 2680/3750 [06:47<02:42,  6.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2682/3750 [06:47<02:42,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2684/3750 [06:47<02:41,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2686/3750 [06:47<02:41,  6.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2688/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2690/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2692/3750 [06:47<02:40,  6.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2694/3750 [06:47<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2696/3750 [06:47<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2698/3750 [06:48<02:39,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2700/3750 [06:48<02:38,  6.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2702/3750 [06:48<02:38,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2704/3750 [06:48<02:37,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2706/3750 [06:48<02:37,  6.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2708/3750 [06:48<02:37,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2710/3750 [06:48<02:36,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2712/3750 [06:48<02:36,  6.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2714/3750 [06:48<02:36,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2716/3750 [06:49<02:35,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  72%|███████▏  | 2718/3750 [06:49<02:35,  6.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2720/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2722/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2724/3750 [06:49<02:34,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2726/3750 [06:49<02:33,  6.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2728/3750 [06:49<02:33,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2730/3750 [06:49<02:33,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2732/3750 [06:49<02:32,  6.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2734/3750 [06:50<02:32,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2736/3750 [06:50<02:32,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2738/3750 [06:50<02:31,  6.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2740/3750 [06:50<02:31,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2742/3750 [06:50<02:30,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2744/3750 [06:50<02:30,  6.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2746/3750 [06:50<02:30,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2748/3750 [06:50<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2750/3750 [06:50<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2752/3750 [06:51<02:29,  6.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2754/3750 [06:51<02:28,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  73%|███████▎  | 2756/3750 [06:51<02:28,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2758/3750 [06:51<02:27,  6.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2760/3750 [06:51<02:27,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2762/3750 [06:51<02:27,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▎  | 2764/3750 [06:51<02:26,  6.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2766/3750 [06:51<02:26,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2768/3750 [06:52<02:26,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2770/3750 [06:52<02:25,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2772/3750 [06:52<02:25,  6.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2774/3750 [06:52<02:25,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2776/3750 [06:52<02:24,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2778/3750 [06:52<02:24,  6.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2780/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2782/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2784/3750 [06:52<02:23,  6.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2786/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2788/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2790/3750 [06:53<02:22,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  74%|███████▍  | 2792/3750 [06:53<02:21,  6.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2794/3750 [06:53<02:21,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2796/3750 [06:53<02:21,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2798/3750 [06:53<02:20,  6.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2800/3750 [06:53<02:20,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2802/3750 [06:53<02:20,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2804/3750 [06:54<02:19,  6.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2806/3750 [06:54<02:19,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2808/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2810/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▍  | 2812/3750 [06:54<02:18,  6.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2814/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2816/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2818/3750 [06:54<02:17,  6.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2820/3750 [06:54<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2822/3750 [06:55<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2824/3750 [06:55<02:16,  6.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2826/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2828/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  75%|███████▌  | 2830/3750 [06:55<02:15,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2832/3750 [06:55<02:14,  6.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2834/3750 [06:55<02:14,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2836/3750 [06:55<02:14,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2838/3750 [06:55<02:13,  6.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2840/3750 [06:56<02:13,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2842/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2844/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2846/3750 [06:56<02:12,  6.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2848/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2850/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2852/3750 [06:56<02:11,  6.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2854/3750 [06:56<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2856/3750 [06:56<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▌  | 2858/3750 [06:57<02:10,  6.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2860/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2862/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2864/3750 [06:57<02:09,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2866/3750 [06:57<02:08,  6.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  76%|███████▋  | 2868/3750 [06:57<02:08,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2870/3750 [06:57<02:08,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2872/3750 [06:57<02:07,  6.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2874/3750 [06:57<02:07,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2876/3750 [06:58<02:07,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2878/3750 [06:58<02:06,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2880/3750 [06:58<02:06,  6.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2882/3750 [06:58<02:06,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2884/3750 [06:58<02:05,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2886/3750 [06:58<02:05,  6.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2888/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2890/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2892/3750 [06:58<02:04,  6.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2894/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2896/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2898/3750 [06:59<02:03,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2900/3750 [06:59<02:02,  6.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2902/3750 [06:59<02:02,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2904/3750 [06:59<02:02,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  77%|███████▋  | 2906/3750 [06:59<02:01,  6.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2908/3750 [06:59<02:01,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2910/3750 [07:00<02:01,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2912/3750 [07:00<02:00,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2914/3750 [07:00<02:00,  6.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2916/3750 [07:00<02:00,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2918/3750 [07:00<01:59,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2920/3750 [07:00<01:59,  6.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2922/3750 [07:00<01:59,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2924/3750 [07:00<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2926/3750 [07:00<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2928/3750 [07:01<01:58,  6.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2930/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2932/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2934/3750 [07:01<01:57,  6.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2936/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2938/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2940/3750 [07:01<01:56,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  78%|███████▊  | 2942/3750 [07:01<01:55,  6.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2944/3750 [07:01<01:55,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2946/3750 [07:02<01:55,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2948/3750 [07:02<01:54,  6.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2950/3750 [07:02<01:54,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▊  | 2952/3750 [07:02<01:54,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2954/3750 [07:02<01:53,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2956/3750 [07:02<01:53,  6.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2958/3750 [07:02<01:53,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2960/3750 [07:02<01:52,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2962/3750 [07:02<01:52,  7.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2964/3750 [07:03<01:52,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2966/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2968/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2970/3750 [07:03<01:51,  7.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2972/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2974/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2976/3750 [07:03<01:50,  7.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2978/3750 [07:03<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  79%|███████▉  | 2980/3750 [07:03<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2982/3750 [07:04<01:49,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2984/3750 [07:04<01:48,  7.03it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2986/3750 [07:04<01:48,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2988/3750 [07:04<01:48,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2990/3750 [07:04<01:47,  7.04it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2992/3750 [07:04<01:47,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2994/3750 [07:04<01:47,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2996/3750 [07:04<01:46,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|███████▉  | 2998/3750 [07:04<01:46,  7.05it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3000/3750 [07:05<01:46,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3002/3750 [07:05<01:45,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3004/3750 [07:05<01:45,  7.06it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3006/3750 [07:05<01:45,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3008/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3010/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3012/3750 [07:05<01:44,  7.07it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3014/3750 [07:05<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3016/3750 [07:05<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  80%|████████  | 3018/3750 [07:06<01:43,  7.08it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3020/3750 [07:06<01:43,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3022/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3024/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3026/3750 [07:06<01:42,  7.09it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3028/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3030/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3032/3750 [07:06<01:41,  7.10it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3034/3750 [07:06<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3036/3750 [07:07<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3038/3750 [07:07<01:40,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3040/3750 [07:07<01:39,  7.11it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3042/3750 [07:07<01:39,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3044/3750 [07:07<01:39,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████  | 3046/3750 [07:07<01:38,  7.12it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3048/3750 [07:07<01:38,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3050/3750 [07:07<01:38,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3052/3750 [07:08<01:37,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3054/3750 [07:08<01:37,  7.13it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  81%|████████▏ | 3056/3750 [07:08<01:37,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3058/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3060/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3062/3750 [07:08<01:36,  7.14it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3064/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3066/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3068/3750 [07:08<01:35,  7.15it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3070/3750 [07:09<01:35,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3072/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3074/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3076/3750 [07:09<01:34,  7.16it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3078/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3080/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3082/3750 [07:09<01:33,  7.17it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3084/3750 [07:09<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3086/3750 [07:09<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3088/3750 [07:10<01:32,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3090/3750 [07:10<01:31,  7.18it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  82%|████████▏ | 3092/3750 [07:10<01:31,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3094/3750 [07:10<01:31,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3096/3750 [07:10<01:30,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3098/3750 [07:10<01:30,  7.19it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3100/3750 [07:10<01:30,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3102/3750 [07:10<01:30,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3104/3750 [07:10<01:29,  7.20it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3106/3750 [07:11<01:29,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3108/3750 [07:11<01:29,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3110/3750 [07:11<01:28,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3112/3750 [07:11<01:28,  7.21it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3114/3750 [07:11<01:28,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3116/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3118/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3120/3750 [07:11<01:27,  7.22it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3122/3750 [07:11<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3124/3750 [07:12<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3126/3750 [07:12<01:26,  7.23it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3128/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  83%|████████▎ | 3130/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3132/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3134/3750 [07:12<01:25,  7.24it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3136/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3138/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▎ | 3140/3750 [07:12<01:24,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3142/3750 [07:13<01:23,  7.25it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3144/3750 [07:13<01:23,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3146/3750 [07:13<01:23,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3148/3750 [07:13<01:22,  7.26it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3150/3750 [07:13<01:22,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3152/3750 [07:13<01:22,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3154/3750 [07:13<01:21,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3156/3750 [07:13<01:21,  7.27it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3158/3750 [07:13<01:21,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3160/3750 [07:14<01:21,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3162/3750 [07:14<01:20,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3164/3750 [07:14<01:20,  7.28it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3166/3750 [07:14<01:20,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  84%|████████▍ | 3168/3750 [07:14<01:19,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3170/3750 [07:14<01:19,  7.29it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3172/3750 [07:14<01:19,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3174/3750 [07:14<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3176/3750 [07:15<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3178/3750 [07:15<01:18,  7.30it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3180/3750 [07:15<01:18,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3182/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3184/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▍ | 3186/3750 [07:15<01:17,  7.31it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3188/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3190/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3192/3750 [07:15<01:16,  7.32it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3194/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3196/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3198/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3200/3750 [07:16<01:15,  7.33it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3202/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3204/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  85%|████████▌ | 3206/3750 [07:16<01:14,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3208/3750 [07:16<01:13,  7.34it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3210/3750 [07:16<01:13,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3212/3750 [07:17<01:13,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3214/3750 [07:17<01:12,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3216/3750 [07:17<01:12,  7.35it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3218/3750 [07:17<01:12,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3220/3750 [07:17<01:12,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3222/3750 [07:17<01:11,  7.36it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3224/3750 [07:17<01:11,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3226/3750 [07:17<01:11,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3228/3750 [07:17<01:10,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3230/3750 [07:18<01:10,  7.37it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3232/3750 [07:18<01:10,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▌ | 3234/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3236/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3238/3750 [07:18<01:09,  7.38it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3240/3750 [07:18<01:09,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  86%|████████▋ | 3242/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3244/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3246/3750 [07:18<01:08,  7.39it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3248/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3250/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3252/3750 [07:19<01:07,  7.40it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3254/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3256/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3258/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3260/3750 [07:19<01:06,  7.41it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3262/3750 [07:19<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3264/3750 [07:19<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3266/3750 [07:20<01:05,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3268/3750 [07:20<01:04,  7.42it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3270/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3272/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3274/3750 [07:20<01:04,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3276/3750 [07:20<01:03,  7.43it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3278/3750 [07:20<01:03,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  87%|████████▋ | 3280/3750 [07:20<01:03,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3282/3750 [07:20<01:02,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3284/3750 [07:21<01:02,  7.44it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3286/3750 [07:21<01:02,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3288/3750 [07:21<01:02,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3290/3750 [07:21<01:01,  7.45it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3292/3750 [07:21<01:01,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3294/3750 [07:21<01:01,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3296/3750 [07:21<01:00,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3298/3750 [07:21<01:00,  7.46it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3300/3750 [07:22<01:00,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3302/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3304/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3306/3750 [07:22<00:59,  7.47it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3308/3750 [07:22<00:59,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3310/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3312/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3314/3750 [07:22<00:58,  7.48it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3316/3750 [07:22<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  88%|████████▊ | 3318/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3320/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3322/3750 [07:23<00:57,  7.49it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3324/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3326/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▊ | 3328/3750 [07:23<00:56,  7.50it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3330/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3332/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3334/3750 [07:23<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3336/3750 [07:24<00:55,  7.51it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3338/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3340/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3342/3750 [07:24<00:54,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3344/3750 [07:24<00:53,  7.52it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3346/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3348/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3350/3750 [07:24<00:53,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3352/3750 [07:24<00:52,  7.53it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3354/3750 [07:25<00:52,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  89%|████████▉ | 3356/3750 [07:25<00:52,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3358/3750 [07:25<00:51,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3360/3750 [07:25<00:51,  7.54it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3362/3750 [07:25<00:51,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3364/3750 [07:25<00:51,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3366/3750 [07:25<00:50,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3368/3750 [07:25<00:50,  7.55it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3370/3750 [07:25<00:50,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3372/3750 [07:26<00:50,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|████████▉ | 3374/3750 [07:26<00:49,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3376/3750 [07:26<00:49,  7.56it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3378/3750 [07:26<00:49,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3380/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3382/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3384/3750 [07:26<00:48,  7.57it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3386/3750 [07:26<00:48,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3388/3750 [07:26<00:47,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3390/3750 [07:27<00:47,  7.58it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  90%|█████████ | 3392/3750 [07:27<00:47,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3394/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3396/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3398/3750 [07:27<00:46,  7.59it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3400/3750 [07:27<00:46,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3402/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3404/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3406/3750 [07:27<00:45,  7.60it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3408/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3410/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3412/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3414/3750 [07:28<00:44,  7.61it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3416/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3418/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████ | 3420/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3422/3750 [07:28<00:43,  7.62it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3424/3750 [07:28<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3426/3750 [07:29<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3428/3750 [07:29<00:42,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  91%|█████████▏| 3430/3750 [07:29<00:41,  7.63it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3432/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3434/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3436/3750 [07:29<00:41,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3438/3750 [07:29<00:40,  7.64it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3440/3750 [07:29<00:40,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3442/3750 [07:30<00:40,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3444/3750 [07:30<00:39,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3446/3750 [07:30<00:39,  7.65it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3448/3750 [07:30<00:39,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3450/3750 [07:30<00:39,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3452/3750 [07:30<00:38,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3454/3750 [07:30<00:38,  7.66it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3456/3750 [07:30<00:38,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3458/3750 [07:30<00:38,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3460/3750 [07:31<00:37,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3462/3750 [07:31<00:37,  7.67it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3464/3750 [07:31<00:37,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3466/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  92%|█████████▏| 3468/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3470/3750 [07:31<00:36,  7.68it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3472/3750 [07:31<00:36,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3474/3750 [07:31<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3476/3750 [07:31<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3478/3750 [07:32<00:35,  7.69it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3480/3750 [07:32<00:35,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3482/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3484/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3486/3750 [07:32<00:34,  7.70it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3488/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3490/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3492/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3494/3750 [07:32<00:33,  7.71it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3496/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3498/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3500/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3502/3750 [07:33<00:32,  7.72it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3504/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  93%|█████████▎| 3506/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3508/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3510/3750 [07:33<00:31,  7.73it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3512/3750 [07:33<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▎| 3514/3750 [07:34<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3516/3750 [07:34<00:30,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3518/3750 [07:34<00:29,  7.74it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3520/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3522/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3524/3750 [07:34<00:29,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3526/3750 [07:34<00:28,  7.75it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3528/3750 [07:34<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3530/3750 [07:34<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3532/3750 [07:35<00:28,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3534/3750 [07:35<00:27,  7.76it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3536/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3538/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3540/3750 [07:35<00:27,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  94%|█████████▍| 3542/3750 [07:35<00:26,  7.77it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3544/3750 [07:35<00:26,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3546/3750 [07:35<00:26,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3548/3750 [07:35<00:25,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3550/3750 [07:36<00:25,  7.78it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3552/3750 [07:36<00:25,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3554/3750 [07:36<00:25,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3556/3750 [07:36<00:24,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3558/3750 [07:36<00:24,  7.79it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3560/3750 [07:36<00:24,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▍| 3562/3750 [07:36<00:24,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3564/3750 [07:36<00:23,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3566/3750 [07:37<00:23,  7.80it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3568/3750 [07:37<00:23,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3570/3750 [07:37<00:23,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3572/3750 [07:37<00:22,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3574/3750 [07:37<00:22,  7.81it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3576/3750 [07:37<00:22,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3578/3750 [07:37<00:22,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  95%|█████████▌| 3580/3750 [07:37<00:21,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3582/3750 [07:37<00:21,  7.82it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3584/3750 [07:38<00:21,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3586/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3588/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3590/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3592/3750 [07:38<00:20,  7.83it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3594/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3596/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3598/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3600/3750 [07:38<00:19,  7.84it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3602/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3604/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3606/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▌| 3608/3750 [07:39<00:18,  7.85it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3610/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3612/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3614/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3616/3750 [07:39<00:17,  7.86it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  96%|█████████▋| 3618/3750 [07:39<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3620/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3622/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3624/3750 [07:40<00:16,  7.87it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3626/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3628/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3630/3750 [07:40<00:15,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3632/3750 [07:40<00:14,  7.88it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3634/3750 [07:40<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3636/3750 [07:40<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3638/3750 [07:41<00:14,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3640/3750 [07:41<00:13,  7.89it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3642/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3644/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3646/3750 [07:41<00:13,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3648/3750 [07:41<00:12,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3650/3750 [07:41<00:12,  7.90it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3652/3750 [07:41<00:12,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3654/3750 [07:41<00:12,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  97%|█████████▋| 3656/3750 [07:42<00:11,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3658/3750 [07:42<00:11,  7.91it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3660/3750 [07:42<00:11,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3662/3750 [07:42<00:11,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3664/3750 [07:42<00:10,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3666/3750 [07:42<00:10,  7.92it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3668/3750 [07:42<00:10,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3670/3750 [07:42<00:10,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3672/3750 [07:42<00:09,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3674/3750 [07:43<00:09,  7.93it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3676/3750 [07:43<00:09,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3678/3750 [07:43<00:09,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3680/3750 [07:43<00:08,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3682/3750 [07:43<00:08,  7.94it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3684/3750 [07:43<00:08,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3686/3750 [07:43<00:08,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3688/3750 [07:43<00:07,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3690/3750 [07:43<00:07,  7.95it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  98%|█████████▊| 3692/3750 [07:44<00:07,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3694/3750 [07:44<00:07,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3696/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3698/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3700/3750 [07:44<00:06,  7.96it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▊| 3702/3750 [07:44<00:06,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3704/3750 [07:44<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3706/3750 [07:44<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3708/3750 [07:45<00:05,  7.97it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3710/3750 [07:45<00:05,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3712/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3714/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3716/3750 [07:45<00:04,  7.98it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3718/3750 [07:45<00:04,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3720/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3722/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3724/3750 [07:45<00:03,  7.99it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3726/3750 [07:46<00:03,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3728/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0:  99%|█████████▉| 3730/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3732/3750 [07:46<00:02,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3734/3750 [07:46<00:01,  8.00it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3736/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3738/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3740/3750 [07:46<00:01,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3742/3750 [07:46<00:00,  8.01it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3744/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3746/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|█████████▉| 3748/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 0: 100%|██████████| 3750/3750 [07:47<00:00,  8.02it/s, loss=0.138, v_num=0]\n",
      "Epoch 1:   0%|          | 0/3750 [00:00<?, ?it/s, loss=0.138, v_num=0]           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  67%|██████▋   | 2500/3750 [06:38<03:19,  6.28it/s, loss=0.125, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:  67%|██████▋   | 2502/3750 [06:38<03:18,  6.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2504/3750 [06:38<03:18,  6.28it/s, loss=0.125, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:  67%|██████▋   | 2506/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2508/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2510/3750 [06:38<03:17,  6.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2512/3750 [06:38<03:16,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2514/3750 [06:39<03:16,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2516/3750 [06:39<03:15,  6.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2518/3750 [06:39<03:15,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2520/3750 [06:39<03:14,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2522/3750 [06:39<03:14,  6.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2524/3750 [06:39<03:14,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2526/3750 [06:39<03:13,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2528/3750 [06:39<03:13,  6.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  67%|██████▋   | 2530/3750 [06:39<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2532/3750 [06:40<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2534/3750 [06:40<03:12,  6.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2536/3750 [06:40<03:11,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2538/3750 [06:40<03:11,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2540/3750 [06:40<03:10,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2542/3750 [06:40<03:10,  6.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2544/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2546/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2548/3750 [06:40<03:09,  6.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2550/3750 [06:41<03:08,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2552/3750 [06:41<03:08,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2554/3750 [06:41<03:07,  6.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2556/3750 [06:41<03:07,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2558/3750 [06:41<03:07,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2560/3750 [06:41<03:06,  6.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2562/3750 [06:41<03:06,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2564/3750 [06:41<03:05,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2566/3750 [06:42<03:05,  6.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  68%|██████▊   | 2568/3750 [06:42<03:05,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2570/3750 [06:42<03:04,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2572/3750 [06:42<03:04,  6.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2574/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2576/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▊   | 2578/3750 [06:42<03:03,  6.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2580/3750 [06:42<03:02,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2582/3750 [06:42<03:02,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2584/3750 [06:43<03:01,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2586/3750 [06:43<03:01,  6.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2588/3750 [06:43<03:01,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2590/3750 [06:43<03:00,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2592/3750 [06:43<03:00,  6.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2594/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2596/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2598/3750 [06:43<02:59,  6.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2600/3750 [06:43<02:58,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2602/3750 [06:44<02:58,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2604/3750 [06:44<02:57,  6.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  69%|██████▉   | 2606/3750 [06:44<02:57,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2608/3750 [06:44<02:57,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2610/3750 [06:44<02:56,  6.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2612/3750 [06:44<02:56,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2614/3750 [06:44<02:55,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2616/3750 [06:44<02:55,  6.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2618/3750 [06:44<02:55,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2620/3750 [06:45<02:54,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2622/3750 [06:45<02:54,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|██████▉   | 2624/3750 [06:45<02:53,  6.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2626/3750 [06:45<02:53,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2628/3750 [06:45<02:53,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2630/3750 [06:45<02:52,  6.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2632/3750 [06:45<02:52,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2634/3750 [06:45<02:51,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2636/3750 [06:45<02:51,  6.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2638/3750 [06:46<02:51,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2640/3750 [06:46<02:50,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  70%|███████   | 2642/3750 [06:46<02:50,  6.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2644/3750 [06:46<02:50,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2646/3750 [06:46<02:49,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2648/3750 [06:46<02:49,  6.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2650/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2652/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2654/3750 [06:46<02:48,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2656/3750 [06:47<02:47,  6.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2658/3750 [06:47<02:47,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2660/3750 [06:47<02:46,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2662/3750 [06:47<02:46,  6.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2664/3750 [06:47<02:46,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2666/3750 [06:47<02:45,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2668/3750 [06:47<02:45,  6.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████   | 2670/3750 [06:47<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2672/3750 [06:47<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2674/3750 [06:48<02:44,  6.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2676/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2678/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  71%|███████▏  | 2680/3750 [06:48<02:43,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2682/3750 [06:48<02:42,  6.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2684/3750 [06:48<02:42,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2686/3750 [06:48<02:41,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2688/3750 [06:48<02:41,  6.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2690/3750 [06:49<02:41,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2692/3750 [06:49<02:40,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2694/3750 [06:49<02:40,  6.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2696/3750 [06:49<02:40,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2698/3750 [06:49<02:39,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2700/3750 [06:49<02:39,  6.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2702/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2704/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2706/3750 [06:49<02:38,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2708/3750 [06:50<02:37,  6.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2710/3750 [06:50<02:37,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2712/3750 [06:50<02:37,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2714/3750 [06:50<02:36,  6.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2716/3750 [06:50<02:36,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  72%|███████▏  | 2718/3750 [06:50<02:35,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2720/3750 [06:50<02:35,  6.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2722/3750 [06:50<02:35,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2724/3750 [06:50<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2726/3750 [06:51<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2728/3750 [06:51<02:34,  6.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2730/3750 [06:51<02:33,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2732/3750 [06:51<02:33,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2734/3750 [06:51<02:32,  6.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2736/3750 [06:51<02:32,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2738/3750 [06:51<02:32,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2740/3750 [06:51<02:31,  6.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2742/3750 [06:51<02:31,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2744/3750 [06:52<02:31,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2746/3750 [06:52<02:30,  6.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2748/3750 [06:52<02:30,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2750/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2752/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2754/3750 [06:52<02:29,  6.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  73%|███████▎  | 2756/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2758/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2760/3750 [06:52<02:28,  6.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2762/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▎  | 2764/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2766/3750 [06:53<02:27,  6.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2768/3750 [06:53<02:26,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2770/3750 [06:53<02:26,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2772/3750 [06:53<02:25,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2774/3750 [06:53<02:25,  6.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2776/3750 [06:53<02:25,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2778/3750 [06:53<02:24,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2780/3750 [06:54<02:24,  6.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2782/3750 [06:54<02:24,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2784/3750 [06:54<02:23,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2786/3750 [06:54<02:23,  6.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2788/3750 [06:54<02:23,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2790/3750 [06:54<02:22,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  74%|███████▍  | 2792/3750 [06:54<02:22,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2794/3750 [06:54<02:21,  6.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2796/3750 [06:55<02:21,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2798/3750 [06:55<02:21,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2800/3750 [06:55<02:20,  6.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2802/3750 [06:55<02:20,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2804/3750 [06:55<02:20,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2806/3750 [06:55<02:19,  6.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2808/3750 [06:55<02:19,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2810/3750 [06:55<02:19,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▍  | 2812/3750 [06:55<02:18,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2814/3750 [06:56<02:18,  6.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2816/3750 [06:56<02:18,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2818/3750 [06:56<02:17,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2820/3750 [06:56<02:17,  6.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2822/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2824/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2826/3750 [06:56<02:16,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2828/3750 [06:56<02:15,  6.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  75%|███████▌  | 2830/3750 [06:56<02:15,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2832/3750 [06:57<02:15,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2834/3750 [06:57<02:14,  6.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2836/3750 [06:57<02:14,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2838/3750 [06:57<02:14,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2840/3750 [06:57<02:13,  6.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2842/3750 [06:57<02:13,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2844/3750 [06:57<02:13,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2846/3750 [06:57<02:12,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2848/3750 [06:57<02:12,  6.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2850/3750 [06:58<02:12,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2852/3750 [06:58<02:11,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2854/3750 [06:58<02:11,  6.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2856/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▌  | 2858/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2860/3750 [06:58<02:10,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2862/3750 [06:58<02:09,  6.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2864/3750 [06:58<02:09,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2866/3750 [06:58<02:09,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  76%|███████▋  | 2868/3750 [06:59<02:08,  6.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2870/3750 [06:59<02:08,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2872/3750 [06:59<02:08,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2874/3750 [06:59<02:07,  6.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2876/3750 [06:59<02:07,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2878/3750 [06:59<02:07,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2880/3750 [06:59<02:06,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2882/3750 [06:59<02:06,  6.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2884/3750 [06:59<02:06,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2886/3750 [07:00<02:05,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2888/3750 [07:00<02:05,  6.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2890/3750 [07:00<02:05,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2892/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2894/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2896/3750 [07:00<02:04,  6.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2898/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2900/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2902/3750 [07:00<02:03,  6.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2904/3750 [07:01<02:02,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  77%|███████▋  | 2906/3750 [07:01<02:02,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2908/3750 [07:01<02:01,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2910/3750 [07:01<02:01,  6.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2912/3750 [07:01<02:01,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2914/3750 [07:01<02:00,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2916/3750 [07:01<02:00,  6.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2918/3750 [07:01<02:00,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2920/3750 [07:02<01:59,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2922/3750 [07:02<01:59,  6.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2924/3750 [07:02<01:59,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2926/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2928/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2930/3750 [07:02<01:58,  6.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2932/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2934/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2936/3750 [07:02<01:57,  6.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2938/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2940/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  78%|███████▊  | 2942/3750 [07:03<01:56,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2944/3750 [07:03<01:55,  6.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2946/3750 [07:03<01:55,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2948/3750 [07:03<01:55,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2950/3750 [07:03<01:54,  6.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▊  | 2952/3750 [07:03<01:54,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2954/3750 [07:03<01:54,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2956/3750 [07:04<01:53,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2958/3750 [07:04<01:53,  6.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2960/3750 [07:04<01:53,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2962/3750 [07:04<01:52,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2964/3750 [07:04<01:52,  6.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2966/3750 [07:04<01:52,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2968/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2970/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2972/3750 [07:04<01:51,  6.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2974/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2976/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2978/3750 [07:05<01:50,  7.00it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  79%|███████▉  | 2980/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2982/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2984/3750 [07:05<01:49,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2986/3750 [07:05<01:48,  7.01it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2988/3750 [07:05<01:48,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2990/3750 [07:05<01:48,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2992/3750 [07:06<01:47,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2994/3750 [07:06<01:47,  7.02it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2996/3750 [07:06<01:47,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|███████▉  | 2998/3750 [07:06<01:46,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3000/3750 [07:06<01:46,  7.03it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3002/3750 [07:06<01:46,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3004/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3006/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3008/3750 [07:06<01:45,  7.04it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3010/3750 [07:07<01:45,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3012/3750 [07:07<01:44,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3014/3750 [07:07<01:44,  7.05it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3016/3750 [07:07<01:44,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  80%|████████  | 3018/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3020/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3022/3750 [07:07<01:43,  7.06it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3024/3750 [07:07<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3026/3750 [07:08<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3028/3750 [07:08<01:42,  7.07it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3030/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3032/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3034/3750 [07:08<01:41,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3036/3750 [07:08<01:40,  7.08it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3038/3750 [07:08<01:40,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3040/3750 [07:08<01:40,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3042/3750 [07:08<01:39,  7.09it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3044/3750 [07:09<01:39,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████  | 3046/3750 [07:09<01:39,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3048/3750 [07:09<01:38,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3050/3750 [07:09<01:38,  7.10it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3052/3750 [07:09<01:38,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3054/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  81%|████████▏ | 3056/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3058/3750 [07:09<01:37,  7.11it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3060/3750 [07:09<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3062/3750 [07:10<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3064/3750 [07:10<01:36,  7.12it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3066/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3068/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3070/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3072/3750 [07:10<01:35,  7.13it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3074/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3076/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3078/3750 [07:10<01:34,  7.14it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3080/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3082/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3084/3750 [07:11<01:33,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3086/3750 [07:11<01:32,  7.15it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3088/3750 [07:11<01:32,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3090/3750 [07:11<01:32,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  82%|████████▏ | 3092/3750 [07:11<01:31,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3094/3750 [07:11<01:31,  7.16it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3096/3750 [07:11<01:31,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3098/3750 [07:12<01:30,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3100/3750 [07:12<01:30,  7.17it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3102/3750 [07:12<01:30,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3104/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3106/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3108/3750 [07:12<01:29,  7.18it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3110/3750 [07:12<01:29,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3112/3750 [07:12<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3114/3750 [07:12<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3116/3750 [07:13<01:28,  7.19it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3118/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3120/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3122/3750 [07:13<01:27,  7.20it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3124/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3126/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3128/3750 [07:13<01:26,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  83%|████████▎ | 3130/3750 [07:13<01:25,  7.21it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3132/3750 [07:13<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3134/3750 [07:14<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3136/3750 [07:14<01:25,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3138/3750 [07:14<01:24,  7.22it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▎ | 3140/3750 [07:14<01:24,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3142/3750 [07:14<01:24,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3144/3750 [07:14<01:23,  7.23it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3146/3750 [07:14<01:23,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3148/3750 [07:14<01:23,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3150/3750 [07:15<01:22,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3152/3750 [07:15<01:22,  7.24it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3154/3750 [07:15<01:22,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3156/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3158/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3160/3750 [07:15<01:21,  7.25it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3162/3750 [07:15<01:21,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3164/3750 [07:15<01:20,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3166/3750 [07:15<01:20,  7.26it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  84%|████████▍ | 3168/3750 [07:16<01:20,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3170/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3172/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3174/3750 [07:16<01:19,  7.27it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3176/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3178/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3180/3750 [07:16<01:18,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3182/3750 [07:16<01:17,  7.28it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3184/3750 [07:16<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▍ | 3186/3750 [07:17<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3188/3750 [07:17<01:17,  7.29it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3190/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3192/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3194/3750 [07:17<01:16,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3196/3750 [07:17<01:15,  7.30it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3198/3750 [07:17<01:15,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3200/3750 [07:17<01:15,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3202/3750 [07:17<01:14,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3204/3750 [07:18<01:14,  7.31it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  85%|████████▌ | 3206/3750 [07:18<01:14,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3208/3750 [07:18<01:14,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3210/3750 [07:18<01:13,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3212/3750 [07:18<01:13,  7.32it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3214/3750 [07:18<01:13,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3216/3750 [07:18<01:12,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3218/3750 [07:18<01:12,  7.33it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3220/3750 [07:18<01:12,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3222/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3224/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3226/3750 [07:19<01:11,  7.34it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3228/3750 [07:19<01:11,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3230/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3232/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▌ | 3234/3750 [07:19<01:10,  7.35it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3236/3750 [07:19<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3238/3750 [07:19<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3240/3750 [07:20<01:09,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  86%|████████▋ | 3242/3750 [07:20<01:08,  7.36it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3244/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3246/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3248/3750 [07:20<01:08,  7.37it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3250/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3252/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3254/3750 [07:20<01:07,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3256/3750 [07:21<01:06,  7.38it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3258/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3260/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3262/3750 [07:21<01:06,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3264/3750 [07:21<01:05,  7.39it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3266/3750 [07:21<01:05,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3268/3750 [07:21<01:05,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3270/3750 [07:21<01:04,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3272/3750 [07:21<01:04,  7.40it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3274/3750 [07:22<01:04,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3276/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3278/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  87%|████████▋ | 3280/3750 [07:22<01:03,  7.41it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3282/3750 [07:22<01:03,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3284/3750 [07:22<01:02,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3286/3750 [07:22<01:02,  7.42it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3288/3750 [07:22<01:02,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3290/3750 [07:22<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3292/3750 [07:23<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3294/3750 [07:23<01:01,  7.43it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3296/3750 [07:23<01:01,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3298/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3300/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3302/3750 [07:23<01:00,  7.44it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3304/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3306/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3308/3750 [07:23<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3310/3750 [07:24<00:59,  7.45it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3312/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3314/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3316/3750 [07:24<00:58,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  88%|████████▊ | 3318/3750 [07:24<00:57,  7.46it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3320/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3322/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3324/3750 [07:24<00:57,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3326/3750 [07:24<00:56,  7.47it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▊ | 3328/3750 [07:25<00:56,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3330/3750 [07:25<00:56,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3332/3750 [07:25<00:55,  7.48it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3334/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3336/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3338/3750 [07:25<00:55,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3340/3750 [07:25<00:54,  7.49it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3342/3750 [07:25<00:54,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3344/3750 [07:25<00:54,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3346/3750 [07:26<00:53,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3348/3750 [07:26<00:53,  7.50it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3350/3750 [07:26<00:53,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3352/3750 [07:26<00:53,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3354/3750 [07:26<00:52,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  89%|████████▉ | 3356/3750 [07:26<00:52,  7.51it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3358/3750 [07:26<00:52,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3360/3750 [07:26<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3362/3750 [07:26<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3364/3750 [07:27<00:51,  7.52it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3366/3750 [07:27<00:51,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3368/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3370/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3372/3750 [07:27<00:50,  7.53it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|████████▉ | 3374/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3376/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3378/3750 [07:27<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3380/3750 [07:28<00:49,  7.54it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3382/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3384/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3386/3750 [07:28<00:48,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3388/3750 [07:28<00:47,  7.55it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3390/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  90%|█████████ | 3392/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3394/3750 [07:28<00:47,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3396/3750 [07:28<00:46,  7.56it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3398/3750 [07:29<00:46,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3400/3750 [07:29<00:46,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3402/3750 [07:29<00:45,  7.57it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3404/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3406/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3408/3750 [07:29<00:45,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3410/3750 [07:29<00:44,  7.58it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3412/3750 [07:29<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3414/3750 [07:29<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3416/3750 [07:30<00:44,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3418/3750 [07:30<00:43,  7.59it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████ | 3420/3750 [07:30<00:43,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3422/3750 [07:30<00:43,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3424/3750 [07:30<00:42,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3426/3750 [07:30<00:42,  7.60it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3428/3750 [07:30<00:42,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  91%|█████████▏| 3430/3750 [07:30<00:42,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3432/3750 [07:30<00:41,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3434/3750 [07:31<00:41,  7.61it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3436/3750 [07:31<00:41,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3438/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3440/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3442/3750 [07:31<00:40,  7.62it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3444/3750 [07:31<00:40,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3446/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3448/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3450/3750 [07:31<00:39,  7.63it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3452/3750 [07:32<00:39,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3454/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3456/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3458/3750 [07:32<00:38,  7.64it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3460/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3462/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3464/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3466/3750 [07:32<00:37,  7.65it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  92%|█████████▏| 3468/3750 [07:32<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3470/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3472/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3474/3750 [07:33<00:36,  7.66it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3476/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3478/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3480/3750 [07:33<00:35,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3482/3750 [07:33<00:34,  7.67it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3484/3750 [07:33<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3486/3750 [07:33<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3488/3750 [07:34<00:34,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3490/3750 [07:34<00:33,  7.68it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3492/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3494/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3496/3750 [07:34<00:33,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3498/3750 [07:34<00:32,  7.69it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3500/3750 [07:34<00:32,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3502/3750 [07:34<00:32,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3504/3750 [07:35<00:31,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  93%|█████████▎| 3506/3750 [07:35<00:31,  7.70it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3508/3750 [07:35<00:31,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3510/3750 [07:35<00:31,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3512/3750 [07:35<00:30,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▎| 3514/3750 [07:35<00:30,  7.71it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3516/3750 [07:35<00:30,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3518/3750 [07:35<00:30,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3520/3750 [07:35<00:29,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3522/3750 [07:36<00:29,  7.72it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3524/3750 [07:36<00:29,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3526/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3528/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3530/3750 [07:36<00:28,  7.73it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3532/3750 [07:36<00:28,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3534/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3536/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3538/3750 [07:36<00:27,  7.74it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3540/3750 [07:37<00:27,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  94%|█████████▍| 3542/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3544/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3546/3750 [07:37<00:26,  7.75it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3548/3750 [07:37<00:26,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3550/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3552/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3554/3750 [07:37<00:25,  7.76it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3556/3750 [07:37<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3558/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3560/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 3562/3750 [07:38<00:24,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3564/3750 [07:38<00:23,  7.77it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3566/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3568/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3570/3750 [07:38<00:23,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3572/3750 [07:38<00:22,  7.78it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3574/3750 [07:38<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3576/3750 [07:39<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3578/3750 [07:39<00:22,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  95%|█████████▌| 3580/3750 [07:39<00:21,  7.79it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3582/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3584/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3586/3750 [07:39<00:21,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3588/3750 [07:39<00:20,  7.80it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3590/3750 [07:39<00:20,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3592/3750 [07:39<00:20,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3594/3750 [07:40<00:19,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3596/3750 [07:40<00:19,  7.81it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3598/3750 [07:40<00:19,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3600/3750 [07:40<00:19,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3602/3750 [07:40<00:18,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3604/3750 [07:40<00:18,  7.82it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3606/3750 [07:40<00:18,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▌| 3608/3750 [07:40<00:18,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3610/3750 [07:40<00:17,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3612/3750 [07:41<00:17,  7.83it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3614/3750 [07:41<00:17,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3616/3750 [07:41<00:17,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  96%|█████████▋| 3618/3750 [07:41<00:16,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3620/3750 [07:41<00:16,  7.84it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3622/3750 [07:41<00:16,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3624/3750 [07:41<00:16,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3626/3750 [07:41<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3628/3750 [07:42<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3630/3750 [07:42<00:15,  7.85it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3632/3750 [07:42<00:15,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3634/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3636/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3638/3750 [07:42<00:14,  7.86it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3640/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3642/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3644/3750 [07:42<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3646/3750 [07:43<00:13,  7.87it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3648/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3650/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3652/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3654/3750 [07:43<00:12,  7.88it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  97%|█████████▋| 3656/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3658/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3660/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3662/3750 [07:43<00:11,  7.89it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3664/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3666/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3668/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3670/3750 [07:44<00:10,  7.90it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3672/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3674/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3676/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3678/3750 [07:44<00:09,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3680/3750 [07:44<00:08,  7.91it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3682/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3684/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3686/3750 [07:45<00:08,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3688/3750 [07:45<00:07,  7.92it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3690/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  98%|█████████▊| 3692/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3694/3750 [07:45<00:07,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3696/3750 [07:45<00:06,  7.93it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3698/3750 [07:45<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3700/3750 [07:46<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▊| 3702/3750 [07:46<00:06,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3704/3750 [07:46<00:05,  7.94it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3706/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3708/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3710/3750 [07:46<00:05,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3712/3750 [07:46<00:04,  7.95it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3714/3750 [07:46<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3716/3750 [07:46<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3718/3750 [07:47<00:04,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3720/3750 [07:47<00:03,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3722/3750 [07:47<00:03,  7.96it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3724/3750 [07:47<00:03,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3726/3750 [07:47<00:03,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3728/3750 [07:47<00:02,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1:  99%|█████████▉| 3730/3750 [07:47<00:02,  7.97it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3732/3750 [07:47<00:02,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3734/3750 [07:48<00:02,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3736/3750 [07:48<00:01,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3738/3750 [07:48<00:01,  7.98it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3740/3750 [07:48<00:01,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3742/3750 [07:48<00:01,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3744/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3746/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|█████████▉| 3748/3750 [07:48<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 1: 100%|██████████| 3750/3750 [07:49<00:00,  7.99it/s, loss=0.125, v_num=0]\n",
      "Epoch 2:   0%|          | 0/3750 [00:00<?, ?it/s, loss=0.125, v_num=0]           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 2500/3750 [06:38<03:19,  6.28it/s, loss=0.0849, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:  67%|██████▋   | 2502/3750 [06:38<03:18,  6.28it/s, loss=0.0849, v_num=0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  67%|██████▋   | 2504/3750 [06:38<03:18,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2506/3750 [06:38<03:17,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2508/3750 [06:38<03:17,  6.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2510/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2512/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2514/3750 [06:38<03:16,  6.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2516/3750 [06:39<03:15,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2518/3750 [06:39<03:15,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2520/3750 [06:39<03:14,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2522/3750 [06:39<03:14,  6.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2524/3750 [06:39<03:14,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2526/3750 [06:39<03:13,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2528/3750 [06:39<03:13,  6.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  67%|██████▋   | 2530/3750 [06:39<03:12,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2532/3750 [06:39<03:12,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2534/3750 [06:40<03:11,  6.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2536/3750 [06:40<03:11,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2538/3750 [06:40<03:11,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2540/3750 [06:40<03:10,  6.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2542/3750 [06:40<03:10,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2544/3750 [06:40<03:09,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2546/3750 [06:40<03:09,  6.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2548/3750 [06:40<03:09,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2550/3750 [06:40<03:08,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2552/3750 [06:41<03:08,  6.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2554/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2556/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2558/3750 [06:41<03:07,  6.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2560/3750 [06:41<03:06,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2562/3750 [06:41<03:06,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2564/3750 [06:41<03:05,  6.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2566/3750 [06:41<03:05,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  68%|██████▊   | 2568/3750 [06:41<03:05,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2570/3750 [06:42<03:04,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2572/3750 [06:42<03:04,  6.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2574/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2576/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▊   | 2578/3750 [06:42<03:03,  6.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2580/3750 [06:42<03:02,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2582/3750 [06:42<03:02,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2584/3750 [06:42<03:01,  6.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2586/3750 [06:42<03:01,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2588/3750 [06:43<03:00,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2590/3750 [06:43<03:00,  6.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2592/3750 [06:43<03:00,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2594/3750 [06:43<02:59,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2596/3750 [06:43<02:59,  6.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2598/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2600/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2602/3750 [06:43<02:58,  6.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2604/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  69%|██████▉   | 2606/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2608/3750 [06:44<02:57,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2610/3750 [06:44<02:56,  6.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2612/3750 [06:44<02:56,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2614/3750 [06:44<02:55,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2616/3750 [06:44<02:55,  6.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2618/3750 [06:44<02:55,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2620/3750 [06:44<02:54,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2622/3750 [06:45<02:54,  6.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|██████▉   | 2624/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2626/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2628/3750 [06:45<02:53,  6.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2630/3750 [06:45<02:52,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2632/3750 [06:45<02:52,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2634/3750 [06:45<02:51,  6.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2636/3750 [06:45<02:51,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2638/3750 [06:45<02:51,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2640/3750 [06:46<02:50,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  70%|███████   | 2642/3750 [06:46<02:50,  6.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2644/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2646/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2648/3750 [06:46<02:49,  6.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2650/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2652/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2654/3750 [06:46<02:48,  6.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2656/3750 [06:46<02:47,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2658/3750 [06:47<02:47,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2660/3750 [06:47<02:46,  6.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2662/3750 [06:47<02:46,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2664/3750 [06:47<02:46,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2666/3750 [06:47<02:45,  6.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2668/3750 [06:47<02:45,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████   | 2670/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2672/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2674/3750 [06:47<02:44,  6.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2676/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2678/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  71%|███████▏  | 2680/3750 [06:48<02:43,  6.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2682/3750 [06:48<02:42,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2684/3750 [06:48<02:42,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2686/3750 [06:48<02:41,  6.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2688/3750 [06:48<02:41,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2690/3750 [06:48<02:41,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2692/3750 [06:48<02:40,  6.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2694/3750 [06:49<02:40,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2696/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2698/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2700/3750 [06:49<02:39,  6.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2702/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2704/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2706/3750 [06:49<02:38,  6.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2708/3750 [06:49<02:37,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2710/3750 [06:50<02:37,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2712/3750 [06:50<02:36,  6.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2714/3750 [06:50<02:36,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2716/3750 [06:50<02:36,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  72%|███████▏  | 2718/3750 [06:50<02:35,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2720/3750 [06:50<02:35,  6.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2722/3750 [06:50<02:35,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2724/3750 [06:50<02:34,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2726/3750 [06:50<02:34,  6.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2728/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2730/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2732/3750 [06:51<02:33,  6.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2734/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2736/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2738/3750 [06:51<02:32,  6.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2740/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2742/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2744/3750 [06:51<02:31,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2746/3750 [06:52<02:30,  6.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2748/3750 [06:52<02:30,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2750/3750 [06:52<02:29,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2752/3750 [06:52<02:29,  6.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2754/3750 [06:52<02:29,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  73%|███████▎  | 2756/3750 [06:52<02:28,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2758/3750 [06:52<02:28,  6.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2760/3750 [06:52<02:28,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2762/3750 [06:52<02:27,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▎  | 2764/3750 [06:53<02:27,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2766/3750 [06:53<02:26,  6.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2768/3750 [06:53<02:26,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2770/3750 [06:53<02:26,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2772/3750 [06:53<02:25,  6.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2774/3750 [06:53<02:25,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2776/3750 [06:53<02:25,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2778/3750 [06:53<02:24,  6.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2780/3750 [06:53<02:24,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2782/3750 [06:54<02:24,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2784/3750 [06:54<02:23,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2786/3750 [06:54<02:23,  6.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2788/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2790/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  74%|███████▍  | 2792/3750 [06:54<02:22,  6.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2794/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2796/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2798/3750 [06:54<02:21,  6.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2800/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2802/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2804/3750 [06:55<02:20,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2806/3750 [06:55<02:19,  6.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2808/3750 [06:55<02:19,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2810/3750 [06:55<02:19,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▍  | 2812/3750 [06:55<02:18,  6.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2814/3750 [06:55<02:18,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2816/3750 [06:55<02:17,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2818/3750 [06:56<02:17,  6.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2820/3750 [06:56<02:17,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2822/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2824/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2826/3750 [06:56<02:16,  6.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2828/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  75%|███████▌  | 2830/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2832/3750 [06:56<02:15,  6.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2834/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2836/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2838/3750 [06:57<02:14,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2840/3750 [06:57<02:13,  6.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2842/3750 [06:57<02:13,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2844/3750 [06:57<02:13,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2846/3750 [06:57<02:12,  6.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2848/3750 [06:57<02:12,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2850/3750 [06:57<02:11,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2852/3750 [06:58<02:11,  6.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2854/3750 [06:58<02:11,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2856/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▌  | 2858/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2860/3750 [06:58<02:10,  6.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2862/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2864/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2866/3750 [06:58<02:09,  6.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  76%|███████▋  | 2868/3750 [06:58<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2870/3750 [06:59<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2872/3750 [06:59<02:08,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2874/3750 [06:59<02:07,  6.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2876/3750 [06:59<02:07,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2878/3750 [06:59<02:07,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2880/3750 [06:59<02:06,  6.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2882/3750 [06:59<02:06,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2884/3750 [06:59<02:06,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2886/3750 [06:59<02:05,  6.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2888/3750 [07:00<02:05,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2890/3750 [07:00<02:05,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2892/3750 [07:00<02:04,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2894/3750 [07:00<02:04,  6.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2896/3750 [07:00<02:04,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2898/3750 [07:00<02:03,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2900/3750 [07:00<02:03,  6.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2902/3750 [07:00<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2904/3750 [07:00<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  77%|███████▋  | 2906/3750 [07:01<02:02,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2908/3750 [07:01<02:01,  6.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2910/3750 [07:01<02:01,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2912/3750 [07:01<02:01,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2914/3750 [07:01<02:00,  6.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2916/3750 [07:01<02:00,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2918/3750 [07:01<02:00,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2920/3750 [07:01<01:59,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2922/3750 [07:01<01:59,  6.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2924/3750 [07:02<01:59,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2926/3750 [07:02<01:58,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2928/3750 [07:02<01:58,  6.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2930/3750 [07:02<01:58,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2932/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2934/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2936/3750 [07:02<01:57,  6.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2938/3750 [07:02<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2940/3750 [07:02<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  78%|███████▊  | 2942/3750 [07:03<01:56,  6.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2944/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2946/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2948/3750 [07:03<01:55,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2950/3750 [07:03<01:54,  6.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▊  | 2952/3750 [07:03<01:54,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2954/3750 [07:03<01:54,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2956/3750 [07:03<01:53,  6.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2958/3750 [07:04<01:53,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2960/3750 [07:04<01:53,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2962/3750 [07:04<01:52,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2964/3750 [07:04<01:52,  6.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2966/3750 [07:04<01:52,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2968/3750 [07:04<01:51,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2970/3750 [07:04<01:51,  6.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2972/3750 [07:04<01:51,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2974/3750 [07:04<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2976/3750 [07:05<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2978/3750 [07:05<01:50,  7.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  79%|███████▉  | 2980/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2982/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2984/3750 [07:05<01:49,  7.01it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2986/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2988/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2990/3750 [07:05<01:48,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2992/3750 [07:05<01:47,  7.02it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2994/3750 [07:06<01:47,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2996/3750 [07:06<01:47,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|███████▉  | 2998/3750 [07:06<01:46,  7.03it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3000/3750 [07:06<01:46,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3002/3750 [07:06<01:46,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3004/3750 [07:06<01:45,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3006/3750 [07:06<01:45,  7.04it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3008/3750 [07:06<01:45,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3010/3750 [07:06<01:44,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3012/3750 [07:07<01:44,  7.05it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3014/3750 [07:07<01:44,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3016/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  80%|████████  | 3018/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3020/3750 [07:07<01:43,  7.06it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3022/3750 [07:07<01:43,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3024/3750 [07:07<01:42,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3026/3750 [07:07<01:42,  7.07it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3028/3750 [07:07<01:42,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3030/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3032/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3034/3750 [07:08<01:41,  7.08it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3036/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3038/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3040/3750 [07:08<01:40,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3042/3750 [07:08<01:39,  7.09it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3044/3750 [07:08<01:39,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████  | 3046/3750 [07:08<01:39,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3048/3750 [07:09<01:38,  7.10it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3050/3750 [07:09<01:38,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3052/3750 [07:09<01:38,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3054/3750 [07:09<01:37,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  81%|████████▏ | 3056/3750 [07:09<01:37,  7.11it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3058/3750 [07:09<01:37,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3060/3750 [07:09<01:36,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3062/3750 [07:09<01:36,  7.12it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3064/3750 [07:10<01:36,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3066/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3068/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3070/3750 [07:10<01:35,  7.13it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3072/3750 [07:10<01:35,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3074/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3076/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3078/3750 [07:10<01:34,  7.14it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3080/3750 [07:10<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3082/3750 [07:11<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3084/3750 [07:11<01:33,  7.15it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3086/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3088/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3090/3750 [07:11<01:32,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  82%|████████▏ | 3092/3750 [07:11<01:31,  7.16it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3094/3750 [07:11<01:31,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3096/3750 [07:11<01:31,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3098/3750 [07:11<01:30,  7.17it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3100/3750 [07:12<01:30,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3102/3750 [07:12<01:30,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3104/3750 [07:12<01:29,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3106/3750 [07:12<01:29,  7.18it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3108/3750 [07:12<01:29,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3110/3750 [07:12<01:29,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3112/3750 [07:12<01:28,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3114/3750 [07:12<01:28,  7.19it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3116/3750 [07:12<01:28,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3118/3750 [07:13<01:27,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3120/3750 [07:13<01:27,  7.20it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3122/3750 [07:13<01:27,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3124/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3126/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3128/3750 [07:13<01:26,  7.21it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  83%|████████▎ | 3130/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3132/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3134/3750 [07:13<01:25,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3136/3750 [07:14<01:24,  7.22it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3138/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▎ | 3140/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3142/3750 [07:14<01:24,  7.23it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3144/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3146/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3148/3750 [07:14<01:23,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3150/3750 [07:14<01:22,  7.24it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3152/3750 [07:14<01:22,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3154/3750 [07:15<01:22,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3156/3750 [07:15<01:21,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3158/3750 [07:15<01:21,  7.25it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3160/3750 [07:15<01:21,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3162/3750 [07:15<01:20,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3164/3750 [07:15<01:20,  7.26it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3166/3750 [07:15<01:20,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  84%|████████▍ | 3168/3750 [07:15<01:20,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3170/3750 [07:15<01:19,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3172/3750 [07:16<01:19,  7.27it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3174/3750 [07:16<01:19,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3176/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3178/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3180/3750 [07:16<01:18,  7.28it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3182/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3184/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▍ | 3186/3750 [07:16<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3188/3750 [07:17<01:17,  7.29it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3190/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3192/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3194/3750 [07:17<01:16,  7.30it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3196/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3198/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3200/3750 [07:17<01:15,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3202/3750 [07:17<01:14,  7.31it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3204/3750 [07:17<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  85%|████████▌ | 3206/3750 [07:18<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3208/3750 [07:18<01:14,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3210/3750 [07:18<01:13,  7.32it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3212/3750 [07:18<01:13,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3214/3750 [07:18<01:13,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3216/3750 [07:18<01:12,  7.33it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3218/3750 [07:18<01:12,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3220/3750 [07:18<01:12,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3222/3750 [07:18<01:11,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3224/3750 [07:19<01:11,  7.34it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3226/3750 [07:19<01:11,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3228/3750 [07:19<01:11,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3230/3750 [07:19<01:10,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3232/3750 [07:19<01:10,  7.35it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▌ | 3234/3750 [07:19<01:10,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3236/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3238/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3240/3750 [07:19<01:09,  7.36it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  86%|████████▋ | 3242/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3244/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3246/3750 [07:20<01:08,  7.37it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3248/3750 [07:20<01:08,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3250/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3252/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3254/3750 [07:20<01:07,  7.38it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3256/3750 [07:20<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3258/3750 [07:20<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3260/3750 [07:21<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3262/3750 [07:21<01:06,  7.39it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3264/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3266/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3268/3750 [07:21<01:05,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3270/3750 [07:21<01:04,  7.40it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3272/3750 [07:21<01:04,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3274/3750 [07:21<01:04,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3276/3750 [07:21<01:03,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3278/3750 [07:22<01:03,  7.41it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  87%|████████▋ | 3280/3750 [07:22<01:03,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3282/3750 [07:22<01:03,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3284/3750 [07:22<01:02,  7.42it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3286/3750 [07:22<01:02,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3288/3750 [07:22<01:02,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3290/3750 [07:22<01:01,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3292/3750 [07:22<01:01,  7.43it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3294/3750 [07:22<01:01,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3296/3750 [07:23<01:01,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3298/3750 [07:23<01:00,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3300/3750 [07:23<01:00,  7.44it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3302/3750 [07:23<01:00,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3304/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3306/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3308/3750 [07:23<00:59,  7.45it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3310/3750 [07:23<00:59,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3312/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3314/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3316/3750 [07:24<00:58,  7.46it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  88%|████████▊ | 3318/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3320/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3322/3750 [07:24<00:57,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3324/3750 [07:24<00:56,  7.47it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3326/3750 [07:24<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▊ | 3328/3750 [07:24<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3330/3750 [07:25<00:56,  7.48it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3332/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3334/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3336/3750 [07:25<00:55,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3338/3750 [07:25<00:54,  7.49it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3340/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3342/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3344/3750 [07:25<00:54,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3346/3750 [07:25<00:53,  7.50it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3348/3750 [07:26<00:53,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3350/3750 [07:26<00:53,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3352/3750 [07:26<00:52,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3354/3750 [07:26<00:52,  7.51it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  89%|████████▉ | 3356/3750 [07:26<00:52,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3358/3750 [07:26<00:52,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3360/3750 [07:26<00:51,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3362/3750 [07:26<00:51,  7.52it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3364/3750 [07:26<00:51,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3366/3750 [07:27<00:51,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3368/3750 [07:27<00:50,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3370/3750 [07:27<00:50,  7.53it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3372/3750 [07:27<00:50,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|████████▉ | 3374/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3376/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3378/3750 [07:27<00:49,  7.54it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3380/3750 [07:27<00:49,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3382/3750 [07:27<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3384/3750 [07:28<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3386/3750 [07:28<00:48,  7.55it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3388/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3390/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  90%|█████████ | 3392/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3394/3750 [07:28<00:47,  7.56it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3396/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3398/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3400/3750 [07:28<00:46,  7.57it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3402/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3404/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3406/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3408/3750 [07:29<00:45,  7.58it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3410/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3412/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3414/3750 [07:29<00:44,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3416/3750 [07:29<00:43,  7.59it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3418/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████ | 3420/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3422/3750 [07:30<00:43,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3424/3750 [07:30<00:42,  7.60it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3426/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3428/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  91%|█████████▏| 3430/3750 [07:30<00:42,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3432/3750 [07:30<00:41,  7.61it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3434/3750 [07:30<00:41,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3436/3750 [07:31<00:41,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3438/3750 [07:31<00:40,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3440/3750 [07:31<00:40,  7.62it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3442/3750 [07:31<00:40,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3444/3750 [07:31<00:40,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3446/3750 [07:31<00:39,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3448/3750 [07:31<00:39,  7.63it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3450/3750 [07:31<00:39,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3452/3750 [07:31<00:39,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3454/3750 [07:32<00:38,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3456/3750 [07:32<00:38,  7.64it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3458/3750 [07:32<00:38,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3460/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3462/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3464/3750 [07:32<00:37,  7.65it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3466/3750 [07:32<00:37,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  92%|█████████▏| 3468/3750 [07:32<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3470/3750 [07:32<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3472/3750 [07:33<00:36,  7.66it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3474/3750 [07:33<00:36,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3476/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3478/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3480/3750 [07:33<00:35,  7.67it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3482/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3484/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3486/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3488/3750 [07:33<00:34,  7.68it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3490/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3492/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3494/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3496/3750 [07:34<00:33,  7.69it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3498/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3500/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3502/3750 [07:34<00:32,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3504/3750 [07:34<00:31,  7.70it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  93%|█████████▎| 3506/3750 [07:34<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3508/3750 [07:35<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3510/3750 [07:35<00:31,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3512/3750 [07:35<00:30,  7.71it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▎| 3514/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3516/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3518/3750 [07:35<00:30,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3520/3750 [07:35<00:29,  7.72it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3522/3750 [07:35<00:29,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3524/3750 [07:35<00:29,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3526/3750 [07:36<00:28,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3528/3750 [07:36<00:28,  7.73it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3530/3750 [07:36<00:28,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3532/3750 [07:36<00:28,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3534/3750 [07:36<00:27,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3536/3750 [07:36<00:27,  7.74it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3538/3750 [07:36<00:27,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3540/3750 [07:36<00:27,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  94%|█████████▍| 3542/3750 [07:37<00:26,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3544/3750 [07:37<00:26,  7.75it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3546/3750 [07:37<00:26,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3548/3750 [07:37<00:26,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3550/3750 [07:37<00:25,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3552/3750 [07:37<00:25,  7.76it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3554/3750 [07:37<00:25,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3556/3750 [07:37<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3558/3750 [07:37<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3560/3750 [07:38<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▍| 3562/3750 [07:38<00:24,  7.77it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3564/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3566/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3568/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3570/3750 [07:38<00:23,  7.78it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3572/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3574/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3576/3750 [07:38<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3578/3750 [07:39<00:22,  7.79it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  95%|█████████▌| 3580/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3582/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3584/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3586/3750 [07:39<00:21,  7.80it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3588/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3590/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3592/3750 [07:39<00:20,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3594/3750 [07:39<00:19,  7.81it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3596/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3598/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3600/3750 [07:40<00:19,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3602/3750 [07:40<00:18,  7.82it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3604/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3606/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▌| 3608/3750 [07:40<00:18,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3610/3750 [07:40<00:17,  7.83it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3612/3750 [07:40<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3614/3750 [07:41<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3616/3750 [07:41<00:17,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  96%|█████████▋| 3618/3750 [07:41<00:16,  7.84it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3620/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3622/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3624/3750 [07:41<00:16,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3626/3750 [07:41<00:15,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3628/3750 [07:41<00:15,  7.85it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3630/3750 [07:41<00:15,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3632/3750 [07:42<00:15,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3634/3750 [07:42<00:14,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3636/3750 [07:42<00:14,  7.86it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3638/3750 [07:42<00:14,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3640/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3642/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3644/3750 [07:42<00:13,  7.87it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3646/3750 [07:42<00:13,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3648/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3650/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3652/3750 [07:43<00:12,  7.88it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3654/3750 [07:43<00:12,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  97%|█████████▋| 3656/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3658/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3660/3750 [07:43<00:11,  7.89it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3662/3750 [07:43<00:11,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3664/3750 [07:43<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3666/3750 [07:44<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3668/3750 [07:44<00:10,  7.90it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3670/3750 [07:44<00:10,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3672/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3674/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3676/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3678/3750 [07:44<00:09,  7.91it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3680/3750 [07:44<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3682/3750 [07:44<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3684/3750 [07:45<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3686/3750 [07:45<00:08,  7.92it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3688/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3690/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  98%|█████████▊| 3692/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3694/3750 [07:45<00:07,  7.93it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3696/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3698/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3700/3750 [07:45<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▊| 3702/3750 [07:46<00:06,  7.94it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3704/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3706/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3708/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3710/3750 [07:46<00:05,  7.95it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3712/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3714/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3716/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3718/3750 [07:46<00:04,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3720/3750 [07:47<00:03,  7.96it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3722/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3724/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3726/3750 [07:47<00:03,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3728/3750 [07:47<00:02,  7.97it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2:  99%|█████████▉| 3730/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3732/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3734/3750 [07:47<00:02,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3736/3750 [07:47<00:01,  7.98it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3738/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3740/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3742/3750 [07:48<00:01,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3744/3750 [07:48<00:00,  7.99it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3746/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|█████████▉| 3748/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|██████████| 3750/3750 [07:48<00:00,  8.00it/s, loss=0.0849, v_num=0]\n",
      "Epoch 2: 100%|██████████| 3750/3750 [07:50<00:00,  7.98it/s, loss=0.0849, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nan0hu-nj5Zm"
   },
   "source": [
    "## Load the Stored Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:03:06.550253Z",
     "iopub.status.busy": "2024-02-27T13:03:06.549917Z",
     "iopub.status.idle": "2024-02-27T13:03:08.694709Z",
     "shell.execute_reply": "2024-02-27T13:03:08.694174Z",
     "shell.execute_reply.started": "2024-02-27T13:03:06.550225Z"
    },
    "id": "27yNiUp_W1pn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(\"/mnt/workspace/ORL/lightning_logs/version_0/checkpoints/epoch=2-step=470.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:03:16.977947Z",
     "iopub.status.busy": "2024-02-27T13:03:16.977623Z",
     "iopub.status.idle": "2024-02-27T13:03:24.844373Z",
     "shell.execute_reply": "2024-02-27T13:03:24.843720Z",
     "shell.execute_reply.started": "2024-02-27T13:03:16.977926Z"
    },
    "id": "-yVoc97oPged",
    "outputId": "06ddd83e-dae5-43f4-b585-fd1056192d07",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/pai/envs/T5/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: calhoun city , mississippi\n",
      "\n",
      "Actual Entities: loc: calhoun city , mississippi\n",
      "Predicted Entities: loc: calhoun city , mississippi\n",
      "=====================================================================\n",
      "\n",
      "text: honorary fellow of the american institute of architects ( 1993 ) and of the royal institute of\n",
      "british architects ( 1995 ) .\n",
      "\n",
      "Actual Entities: org: american institute of architects; org: royal institute of british architects\n",
      "Predicted Entities: org: american institute of architects; org: royal institute of british architects\n",
      "=====================================================================\n",
      "\n",
      "text: museo di storia naturale di venezia , venice\n",
      "\n",
      "Actual Entities: org: museo di storia naturale di venezia; loc: venice\n",
      "Predicted Entities: loc: museo di storia naturale di venezia\n",
      "=====================================================================\n",
      "\n",
      "text: edgar allan poe in popular culture\n",
      "\n",
      "Actual Entities: org: edgar allan poe in popular culture\n",
      "Predicted Entities: per: edgar allan poe in popular culture\n",
      "=====================================================================\n",
      "\n",
      "text: '' shine my shoes ''\n",
      "\n",
      "Actual Entities: org: shine my shoes\n",
      "Predicted Entities: org: shine my shoes\n",
      "=====================================================================\n",
      "\n",
      "text: värmlands län , seat no .\n",
      "\n",
      "Actual Entities: org: värmlands län\n",
      "Predicted Entities: loc: värmlands län\n",
      "=====================================================================\n",
      "\n",
      "text: their first retail store was set up at shaw house and centre on scotts road .\n",
      "\n",
      "Actual Entities: org: shaw house and centre; loc: scotts road\n",
      "Predicted Entities: org: shaw house and centre; loc: scotts road\n",
      "=====================================================================\n",
      "\n",
      "text: on 28 july 2006 , it was sold to air greenland .\n",
      "\n",
      "Actual Entities: org: air greenland\n",
      "Predicted Entities: org: air greenland\n",
      "=====================================================================\n",
      "\n",
      "text: buchanan 's birthplace state park ( franklin county )\n",
      "\n",
      "Actual Entities: org: buchanan 's birthplace state park; loc: franklin county\n",
      "Predicted Entities: loc: buchanan 's birthplace state park; loc: frankli\n",
      "=====================================================================\n",
      "\n",
      "text: *american singer-guitarist glen campbell covered the song on his 12th album wichita lineman ''\n",
      ", released in 1968 by capitol records .\n",
      "\n",
      "Actual Entities: per: glen campbell; org: wichita lineman; org: capitol records\n",
      "Predicted Entities: per: glen campbell; org: wichita lineman;\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "dataloader = DataLoader(input_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
    "model.model.eval()\n",
    "model = model.to(\"cpu\")\n",
    "outputs = []\n",
    "targets = []\n",
    "texts = []\n",
    "for batch in dataloader:\n",
    "\n",
    "    outs = model.model.generate(input_ids=batch['source_ids'],\n",
    "                                attention_mask=batch['source_mask'])\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    text = [tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    texts.extend(text)\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    break\n",
    "\n",
    "for i in range(10):\n",
    "    c = texts[i]\n",
    "    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n",
    "    print(\"\\n\".join(lines))\n",
    "    print(\"\\nActual Entities: %s\" % target[i])\n",
    "    print(\"Predicted Entities: %s\" % outputs[i])\n",
    "    print(\"=====================================================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-27T13:04:07.805434Z",
     "iopub.status.busy": "2024-02-27T13:04:07.805085Z",
     "iopub.status.idle": "2024-02-27T13:04:07.811701Z",
     "shell.execute_reply": "2024-02-27T13:04:07.811208Z",
     "shell.execute_reply.started": "2024-02-27T13:04:07.805408Z"
    },
    "id": "Q358Ph_JUeSA",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_sub_list(sl, l):\n",
    "    results = []\n",
    "    sll = len(sl)\n",
    "    for ind in (i for i, e in enumerate(l) if e == sl[0]):\n",
    "        if l[ind:ind+sll] == sl:\n",
    "            results.append((ind, ind+sll-1))\n",
    "    return results\n",
    "\n",
    "def generate_label(input: str, target: str):\n",
    "    mapper = {'O': 0, 'B-DATE': 1, 'I-DATE': 2, 'B-PER': 3,\n",
    "              'I-PER': 4, 'B-ORG': 5, 'I-ORG': 6, 'B-LOC': 7, 'I-LOC': 8}\n",
    "    inv_mapper = {v: k for k, v in mapper.items()}\n",
    "\n",
    "    input = input.split(\" \")\n",
    "    target = target.split(\"; \")\n",
    "\n",
    "    init_target_label = [mapper['O']]*len(input)\n",
    "\n",
    "    for ent in target:\n",
    "        ent = ent.split(\": \")\n",
    "        try:\n",
    "            sent_end = ent[1].split(\" \")\n",
    "            index = find_sub_list(sent_end, input)\n",
    "        except:\n",
    "            continue\n",
    "        # print(index)\n",
    "        try:\n",
    "            init_target_label[index[0][0]] = mapper[f\"B-{ent[0].upper()}\"]\n",
    "            for i in range(index[0][0]+1, index[0][1]+1):\n",
    "                init_target_label[i] = mapper[f\"I-{ent[0].upper()}\"]\n",
    "        except:\n",
    "            continue\n",
    "    init_target_label = [inv_mapper[j] for j in init_target_label]\n",
    "    return init_target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:04:11.030985Z",
     "iopub.status.busy": "2024-02-27T13:04:11.030664Z",
     "iopub.status.idle": "2024-02-27T13:05:34.504615Z",
     "shell.execute_reply": "2024-02-27T13:05:34.504054Z",
     "shell.execute_reply.started": "2024-02-27T13:04:11.030962Z"
    },
    "id": "s8b7-Y07T5qV",
    "outputId": "6ff28c77-103b-4a3a-f50b-0b3c1dbee16c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/313 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 313/313 [01:18<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_dataset = WikiAnnDataset(tokenizer=tokenizer, dataset=dataset, type_path='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32,\n",
    "                             num_workers=2, shuffle=True)\n",
    "model.model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "outputs = []\n",
    "targets = []\n",
    "all_text = []\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "for batch in tqdm(test_loader):\n",
    "    input_ids = batch['source_ids'].to(\"cuda\")\n",
    "    attention_mask = batch['source_mask'].to(\"cuda\")\n",
    "    outs = model.model.generate(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask)\n",
    "    dec = [tokenizer.decode(ids, skip_special_tokens=True,\n",
    "                            clean_up_tokenization_spaces=False).strip() for ids in outs]\n",
    "    target = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"target_ids\"]]\n",
    "    texts = [tokenizer.decode(ids, skip_special_tokens=True,  clean_up_tokenization_spaces=False).strip()\n",
    "                for ids in batch[\"source_ids\"]]\n",
    "    true_label = [generate_label(texts[i].strip(), target[i].strip()) if target[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "    pred_label = [generate_label(texts[i].strip(), dec[i].strip()) if dec[i].strip() != 'none' else [\n",
    "        \"O\"]*len(texts[i].strip().split()) for i in range(len(texts))]\n",
    "\n",
    "    outputs.extend(dec)\n",
    "    targets.extend(target)\n",
    "    true_labels.extend(true_label)\n",
    "    pred_labels.extend(pred_label)\n",
    "    all_text.extend(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:05:43.154403Z",
     "iopub.status.busy": "2024-02-27T13:05:43.154049Z",
     "iopub.status.idle": "2024-02-27T13:05:43.159246Z",
     "shell.execute_reply": "2024-02-27T13:05:43.158785Z",
     "shell.execute_reply.started": "2024-02-27T13:05:43.154377Z"
    },
    "id": "MTDdTHwdadFx",
    "outputId": "845d8b90-cd00-490e-f0a9-96d71eff167e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"orguss '' - additional voices\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-02-27T13:06:23.049881Z",
     "iopub.status.busy": "2024-02-27T13:06:23.049554Z",
     "iopub.status.idle": "2024-02-27T13:06:28.187486Z",
     "shell.execute_reply": "2024-02-27T13:06:28.186898Z",
     "shell.execute_reply.started": "2024-02-27T13:06:23.049861Z"
    },
    "id": "ZrFA-BSHerhf",
    "outputId": "3df4f246-e26e-46ff-dd92-cf5a6da5cc4f",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pai/envs/T5/lib/python3.9/site-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  reginald beckwith as kenniston .\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'O', 'B-PER', 'O']\n",
      "True Token Class:  ['B-PER', 'I-PER', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  orguss '' - additional voices\n",
      "Predicted Token Class:  ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  dihedral symmetry groups with even-orders have a number of subgroups .\n",
      "Predicted Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  council of southern africa football associations\n",
      "Predicted Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG', 'I-ORG']\n",
      "=====================================================================\n",
      "\n",
      "Text:  he died in 1924 and was buried in the hôtel des invalides .\n",
      "Predicted Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']\n",
      "True Token Class:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG', 'I-ORG', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  paul warren rieger .\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O']\n",
      "True Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  diggy liggy lo '' ( j. d. miller ) – 2:16\n",
      "Predicted Token Class:  ['B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "True Token Class:  ['B-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  '' pereza '' '\n",
      "Predicted Token Class:  ['O', 'B-LOC', 'O', 'O']\n",
      "True Token Class:  ['O', 'B-ORG', 'O', 'O']\n",
      "=====================================================================\n",
      "\n",
      "Text:  atkinson , new hampshire\n",
      "Predicted Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "True Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "=====================================================================\n",
      "\n",
      "Text:  san diego county , california\n",
      "Predicted Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "True Token Class:  ['B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC']\n",
      "=====================================================================\n",
      "\n",
      "{'LOC': {'precision': 0.7034047919293821, 'recall': 0.6028966709900562, 'f1': 0.6492841345594227, 'number': 4626}, 'ORG': {'precision': 0.6855995410212278, 'recall': 0.5156418554476807, 'f1': 0.5885974633665805, 'number': 4635}, 'PER': {'precision': 0.7557732680195941, 'recall': 0.7139709122961657, 'f1': 0.7342776203966006, 'number': 4538}, 'overall_precision': 0.7172431419321861, 'overall_recall': 0.6101166751213856, 'overall_f1': 0.6593570113952305, 'overall_accuracy': 0.8310771188661119}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Text:  {all_text[i]}\")\n",
    "    print(f\"Predicted Token Class:  {pred_labels[i]}\")\n",
    "    print(f\"True Token Class:  {true_labels[i]}\")\n",
    "    print(\"=====================================================================\\n\")\n",
    "\n",
    "print(metric.compute(predictions=pred_labels, references=true_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sF7loiUFVbQM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "T5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "005a930db3d242a4be28a9c5923eed76": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0305993a3c1a4f8296faeb1aba61fa38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0573bd35717d4189ab4d74fdc489653b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bd7b7a9f5f80492997ec2cbd4bf22627",
       "IPY_MODEL_1b08c691a8db4cb79e6f7837a08ac799",
       "IPY_MODEL_b904bbebe38943728a23a0d630f86a88"
      ],
      "layout": "IPY_MODEL_06e6627b73a24e4291f316a281a20aea"
     }
    },
    "06e6627b73a24e4291f316a281a20aea": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "0e46dd857d504def992f555a9e9c9c5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "127c67d7658d4bf392408e7d5600a09c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b08c691a8db4cb79e6f7837a08ac799": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f69895f136a14e629614a45f2e7d8557",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfcb95376da04ce884ab18c57c2d63ed",
      "value": 1250
     }
    },
    "1fac6af61b4d411a94f3916061731428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "20d7306a9b474da3b0c8e80a3d7abe09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_264fdba12e854d54b3ed2510458c7a1b",
      "placeholder": "​",
      "style": "IPY_MODEL_95d7c71216094ea0b466a79efc95341d",
      "value": " 1250/1250 [02:30&lt;00:00,  8.20it/s]"
     }
    },
    "21b88aca18f048759fb25ce63540c992": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25b71debb82040a2960eb8348df2a4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "264fdba12e854d54b3ed2510458c7a1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "285649e2ae134f87bd2f8dcf40a7758d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3e24f0b77884455868b103ab69ddd5d",
      "placeholder": "​",
      "style": "IPY_MODEL_c3c43f7938044239b4eca004d07874ae",
      "value": "100%"
     }
    },
    "304c605d63584a298e8cefd4ffcf4ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dfdd904fb4924ad8948793222255cc3b",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5d88a23982ac46b3b8077264a6998b9a",
      "value": 3
     }
    },
    "347f2c97cd4f46aa8bd1d30f79041b29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "37a7f3f9244a4fd991fe9e8847268ec1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fce9ba2896164d4391fea7f32dc3d811",
      "placeholder": "​",
      "style": "IPY_MODEL_0305993a3c1a4f8296faeb1aba61fa38",
      "value": " 1250/1250 [02:30&lt;00:00,  8.20it/s]"
     }
    },
    "388d8e1e34a143c3afe9e4082be44ebb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3898654592194daca51999f513192a24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39ceb75a4fa14f6e9a8a5f4ae1324cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4bba40ddade644b79cce613925a4f47e",
       "IPY_MODEL_aa38ae45e85545bebec6c8b43db7abbf",
       "IPY_MODEL_37a7f3f9244a4fd991fe9e8847268ec1"
      ],
      "layout": "IPY_MODEL_49901550f7c14e439505f80faaaa8357"
     }
    },
    "3b924323276f4f92a66a64d295ba6c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ddd6cf350fb4378ad6cf2698f67d68f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21b88aca18f048759fb25ce63540c992",
      "placeholder": "​",
      "style": "IPY_MODEL_3b924323276f4f92a66a64d295ba6c78",
      "value": "Validating: 100%"
     }
    },
    "429fbc36c65446d09d32298cfc4b606e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43278de125c54c1e8b55baa0370c4cc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45340f3b0bf4413182060f27c7458334": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f16174670d2c4182ace507dda7812328",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_966b77941ef046babfb1e9ed597a6aee",
      "value": 3
     }
    },
    "47144694b20f49f0aa54ee1d83119fda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48bb2d6d069e411a9e969187d5ddf17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81211b58e69b4973a9e1b500e621656b",
      "placeholder": "​",
      "style": "IPY_MODEL_347f2c97cd4f46aa8bd1d30f79041b29",
      "value": "Epoch 2: 100%"
     }
    },
    "49901550f7c14e439505f80faaaa8357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4b15ff3c6b044b40a15b21f051bf53c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "4bba40ddade644b79cce613925a4f47e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce10ff94a3454674a5deb511394387ff",
      "placeholder": "​",
      "style": "IPY_MODEL_b537afbec50e4567885220cf27a5b761",
      "value": "Validating: 100%"
     }
    },
    "4fb2b190729943cb8c1f1fd2b84d6935": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4fffbd7c89314024a7b8c128ad1ba248": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d4e33c5404f49caa4811bfd55b0bee5",
      "max": 3750,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce97e8d1d6b54e17a16ec75e1b3abf7f",
      "value": 3750
     }
    },
    "5587ced0255640dd9bec68478cd973fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "56496f0e8cef4b5cb989f289af418f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5d4e33c5404f49caa4811bfd55b0bee5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d88a23982ac46b3b8077264a6998b9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5fd2178313d34631a030d79934dcffe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4dc2a989d794b44b8000ca88cf095fa",
      "placeholder": "​",
      "style": "IPY_MODEL_b449b3c3b1b944a38293f2f13d517860",
      "value": " 3/3 [00:00&lt;00:00, 64.65it/s]"
     }
    },
    "6bf0078bd11d4ed980f1b7a9b612a091": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbcd62725c684fcc897583042ca44888",
      "placeholder": "​",
      "style": "IPY_MODEL_e61579f72d51414782f8fe3c28e12e86",
      "value": " 3/3 [00:00&lt;00:00, 73.09it/s]"
     }
    },
    "79385887dd0f479d993b251748f8cb2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ca93c1fe9a9415e9c7d6f22a58b90d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa0489927bdd4bf4809c9fcc92d9d7b4",
      "placeholder": "​",
      "style": "IPY_MODEL_43278de125c54c1e8b55baa0370c4cc1",
      "value": " 3/3 [00:00&lt;00:00, 48.52it/s]"
     }
    },
    "81211b58e69b4973a9e1b500e621656b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83ec7a9ea11e410bb3fcad13c082d664": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_429fbc36c65446d09d32298cfc4b606e",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79385887dd0f479d993b251748f8cb2f",
      "value": 3
     }
    },
    "84ab2eb4f0b74ac3a334eb7ecd7c4d63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "inline-flex",
      "flex": null,
      "flex_flow": "row wrap",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "100%"
     }
    },
    "84c7612e3ed740958159eec4ee147476": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93c2ea7279794b88b6264f832400e3c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5a571c48de54900b2b696f2c6817a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_4fb2b190729943cb8c1f1fd2b84d6935",
      "value": "100%"
     }
    },
    "9428191170994f0aa7fd400caf0d1d07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95d7c71216094ea0b466a79efc95341d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966b77941ef046babfb1e9ed597a6aee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "97e269fbcb954d629b2314d30c41c57b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_93c2ea7279794b88b6264f832400e3c7",
       "IPY_MODEL_45340f3b0bf4413182060f27c7458334",
       "IPY_MODEL_6bf0078bd11d4ed980f1b7a9b612a091"
      ],
      "layout": "IPY_MODEL_84c7612e3ed740958159eec4ee147476"
     }
    },
    "9cd468cf60c643db96d0cd35923c8258": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a0a3db286a1843b293b1d3b944a3d33a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9428191170994f0aa7fd400caf0d1d07",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_25b71debb82040a2960eb8348df2a4ca",
      "value": 2
     }
    },
    "a5a571c48de54900b2b696f2c6817a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aa38ae45e85545bebec6c8b43db7abbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d26f30f1432e4f2ab1c53484b44989ee",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be10758757c1426c8779c6706a445ecd",
      "value": 1250
     }
    },
    "b112c9a8184d4b479fc0e59698789b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ebcc288979ff4cd5901e9223063751d1",
       "IPY_MODEL_304c605d63584a298e8cefd4ffcf4ead",
       "IPY_MODEL_7ca93c1fe9a9415e9c7d6f22a58b90d1"
      ],
      "layout": "IPY_MODEL_e550224ab580420cb4973b805c2393af"
     }
    },
    "b3e24f0b77884455868b103ab69ddd5d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b449b3c3b1b944a38293f2f13d517860": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b537afbec50e4567885220cf27a5b761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b58e5e54e53e46d7a4f196a93812373c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b849b43f5c4649deb018fb959191edc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b904bbebe38943728a23a0d630f86a88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3898654592194daca51999f513192a24",
      "placeholder": "​",
      "style": "IPY_MODEL_47144694b20f49f0aa54ee1d83119fda",
      "value": " 1250/1250 [02:30&lt;00:00,  8.22it/s]"
     }
    },
    "ba1298c2f3db42c48b7e3e3ee83d5000": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_285649e2ae134f87bd2f8dcf40a7758d",
       "IPY_MODEL_83ec7a9ea11e410bb3fcad13c082d664",
       "IPY_MODEL_5fd2178313d34631a030d79934dcffe0"
      ],
      "layout": "IPY_MODEL_0e46dd857d504def992f555a9e9c9c5f"
     }
    },
    "ba6f4922558c41fea4a4b171a021b871": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_48bb2d6d069e411a9e969187d5ddf17d",
       "IPY_MODEL_4fffbd7c89314024a7b8c128ad1ba248",
       "IPY_MODEL_dea06faaa22b41b6bbfd7ff6d8ee39f6"
      ],
      "layout": "IPY_MODEL_5587ced0255640dd9bec68478cd973fb"
     }
    },
    "bd7b7a9f5f80492997ec2cbd4bf22627": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cddc7d98bbb740a9a81bf8e244d4595d",
      "placeholder": "​",
      "style": "IPY_MODEL_9cd468cf60c643db96d0cd35923c8258",
      "value": "Validating: 100%"
     }
    },
    "be10758757c1426c8779c6706a445ecd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3c43f7938044239b4eca004d07874ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4dc2a989d794b44b8000ca88cf095fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cddc7d98bbb740a9a81bf8e244d4595d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce10ff94a3454674a5deb511394387ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce97e8d1d6b54e17a16ec75e1b3abf7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ceca8a9a02fe46d5a1a3aa093fdbcde7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b849b43f5c4649deb018fb959191edc5",
      "max": 1250,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b58e5e54e53e46d7a4f196a93812373c",
      "value": 1250
     }
    },
    "cfcb95376da04ce884ab18c57c2d63ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d26f30f1432e4f2ab1c53484b44989ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2b327e2b0074354a6b889afc169a62e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8ebf6d2f2954cbbb525e67de58a766a",
       "IPY_MODEL_a0a3db286a1843b293b1d3b944a3d33a",
       "IPY_MODEL_d68d5a7c97bd47719eb1e19f67d64805"
      ],
      "layout": "IPY_MODEL_84ab2eb4f0b74ac3a334eb7ecd7c4d63"
     }
    },
    "d37bf18a5e9346fcb59242e0e5288307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d68d5a7c97bd47719eb1e19f67d64805": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dffd33c1015f4bfd91fd702095c2127e",
      "placeholder": "​",
      "style": "IPY_MODEL_ef4fa30c44df4726a36cfc1da100fb4f",
      "value": " 2/2 [00:00&lt;00:00,  3.48it/s]"
     }
    },
    "dbcd62725c684fcc897583042ca44888": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dea06faaa22b41b6bbfd7ff6d8ee39f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_005a930db3d242a4be28a9c5923eed76",
      "placeholder": "​",
      "style": "IPY_MODEL_d37bf18a5e9346fcb59242e0e5288307",
      "value": " 3750/3750 [15:58&lt;00:00,  3.91it/s, loss=0.103, v_num=0]"
     }
    },
    "dfdd904fb4924ad8948793222255cc3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dffd33c1015f4bfd91fd702095c2127e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e550224ab580420cb4973b805c2393af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e61579f72d51414782f8fe3c28e12e86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebcc288979ff4cd5901e9223063751d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_388d8e1e34a143c3afe9e4082be44ebb",
      "placeholder": "​",
      "style": "IPY_MODEL_56496f0e8cef4b5cb989f289af418f9f",
      "value": "100%"
     }
    },
    "ef4fa30c44df4726a36cfc1da100fb4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f16174670d2c4182ace507dda7812328": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f69895f136a14e629614a45f2e7d8557": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": "2",
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8ebf6d2f2954cbbb525e67de58a766a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_127c67d7658d4bf392408e7d5600a09c",
      "placeholder": "​",
      "style": "IPY_MODEL_1fac6af61b4d411a94f3916061731428",
      "value": "Validation sanity check: 100%"
     }
    },
    "fa0489927bdd4bf4809c9fcc92d9d7b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fce9ba2896164d4391fea7f32dc3d811": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffe6027ef4bb4fd9964717116a39f1a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ddd6cf350fb4378ad6cf2698f67d68f",
       "IPY_MODEL_ceca8a9a02fe46d5a1a3aa093fdbcde7",
       "IPY_MODEL_20d7306a9b474da3b0c8e80a3d7abe09"
      ],
      "layout": "IPY_MODEL_4b15ff3c6b044b40a15b21f051bf53c8"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
